{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e9adfc-25e1-4b9d-93c9-6b127369029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6f78fc-4788-40a6-9d9b-6a433f378d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Horovod cannot be imported; multi-GPU training is unsupported\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from grover.util.parsing import parse_args, get_newest_train_args\n",
    "from grover.util.utils import create_logger\n",
    "from task.cross_validate import cross_validate, randomsearch, gridsearch, make_confusion_matrix\n",
    "from task.fingerprint import generate_fingerprints\n",
    "from task.predict import make_predictions, write_prediction\n",
    "from task.pretrain import pretrain_model\n",
    "from grover.data.torchvocab import MolVocab\n",
    "\n",
    "from grover.topology.mol_tree import *\n",
    "from argparse import ArgumentParser, Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d363ab-3a1b-43aa-8fac-d95500a5d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(seed):\n",
    "    # frozen random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4561be-fbe1-4084-bcf6-a55748930529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup random seed\n",
    "setup(seed=42)\n",
    "# Avoid the pylint warning.\n",
    "a = MolVocab\n",
    "# supress rdkit logger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# Initialize MolVocab\n",
    "mol_vocab = MolVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e4bad-a530-463f-805e-b377a55d7bef",
   "metadata": {},
   "source": [
    "## args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ac4ad1-186e-4bca-b343-4ae75514f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.parsing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dffcbfe-3065-4a13-89e5-7e8fcf6e6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args() -> Namespace:\n",
    "    \"\"\"\n",
    "    Parses arguments for training and testing (includes modifying/validating arguments).\n",
    "\n",
    "    :return: A Namespace containing the parsed, modified, and validated args.\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "                                      dest=\"parser_name\",\n",
    "                                      help=\"Subcommands for fintune, prediction, and fingerprint.\")\n",
    "    parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "    add_pretrain_args(parser_pretrain)\n",
    "\n",
    "    args = parser.parse_args(['pretrain','--data_path','data/mgssl','--save_dir','model/mgssl','--atom_vocab_path','data/mgssl/mgssl_atom_vocab.pkl','--bond_vocab_path','data/mgssl/mgssl_bond_vocab.pkl',\n",
    "                              '--batch_size','100','--dropout','0.1','--depth','3','--num_attn_head','4','--hidden_size','1200','--epochs','20','--activation','PReLU','--backbone','gtrans','--embedding_output_type','both',\n",
    "                              '--save_interval','5','--init_lr', '0.0002', '--max_lr', '0.0004', '--final_lr', '0.0001', '--weight_decay', '0.0000001', \n",
    "                              '--topology','--motif_vocab_path','data/mgssl/clique.txt','--motif_hidden_size','1200','--motif_latent_size','56','--motif_order','dfs'])\n",
    "    \n",
    "    if args.parser_name == 'finetune' or args.parser_name == 'eval':\n",
    "        modify_train_args(args)\n",
    "    elif args.parser_name == \"pretrain\":\n",
    "        modify_pretrain_args(args)\n",
    "    elif args.parser_name == 'predict':\n",
    "        modify_predict_args(args)\n",
    "    elif args.parser_name == 'fingerprint':\n",
    "        modify_fingerprint_args(args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d48325-495c-4af4-bfab-907a86ed4d58",
   "metadata": {},
   "source": [
    "    args = parser.parse_args(['pretrain','--data_path','data/zinc10M_0','--save_dir','model/zinc10M_0','--atom_vocab_path','data/zinc10M/zinc10M_atom_vocab.pkl','--bond_vocab_path','data/zinc10M/zinc10M_bond_vocab.pkl',\n",
    "                              '--batch_size','100','--dropout','0.1','--depth','3','--num_attn_head','4','--hidden_size','1200','--epochs','20','--activation','PReLU','--backbone','gtrans','--embedding_output_type','both',\n",
    "                              '--save_interval','5','--init_lr', '0.0002', '--max_lr', '0.0004', '--final_lr', '0.0001', '--weight_decay', '0.0000001', \n",
    "                              '--topology','--motif_vocab_path','data/zinc10M/clique.txt','--motif_hidden_size','1200','--motif_latent_size','56','--motif_order','dfs',\n",
    "                             '--wandb','--wandb_name', 'jupyter_zinc10M'])\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd6b21-f102-48b0-99af-88d427dec63b",
   "metadata": {},
   "source": [
    "    args = parser.parse_args(['pretrain','--data_path','data/mgssl','--save_dir','model/mgssl','--atom_vocab_path','data/mgssl/mgssl_atom_vocab.pkl','--bond_vocab_path','data/mgssl/mgssl_bond_vocab.pkl',\n",
    "                              '--batch_size','100','--dropout','0.1','--depth','3','--num_attn_head','4','--hidden_size','1200','--epochs','20','--activation','PReLU','--backbone','gtrans','--embedding_output_type','both',\n",
    "                              '--save_interval','5','--init_lr', '0.0002', '--max_lr', '0.0004', '--final_lr', '0.0001', '--weight_decay', '0.0000001', \n",
    "                              '--topology','--motif_vocab_path','data/mgssl/clique.txt','--motif_hidden_size','1200','--motif_latent_size','56','--motif_order','dfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c668498-919f-47ae-8272-d99984401c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(activation='PReLU', atom_vocab_path='data/mgssl/mgssl_atom_vocab.pkl', backbone='gtrans', batch_size=100, bias=False, bond_drop_rate=0, bond_vocab_path='data/mgssl/mgssl_bond_vocab.pkl', cuda=True, data_path='data/mgssl', dense=False, depth=3, dist_coff=0.1, dropout=0.1, embedding_output_type='both', enable_multi_gpu=False, epochs=20, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=1200, init_lr=0.0002, max_lr=0.0004, motif_hidden_size=1200, motif_latent_size=56, motif_order='dfs', motif_vocab_path='data/mgssl/clique.txt', no_cache=True, num_attn_head=4, num_mt_block=1, parser_name='pretrain', save_dir='model/mgssl', save_interval=5, topology=True, undirected=False, wandb=False, wandb_name='pretrain', warmup_epochs=2.0, weight_decay=1e-07)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parse_args()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aea4093-1a23-4446-a898-496a76f4d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger(name='pretrain', save_dir=args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6e3642-40be-4be4-9e2c-565871e82fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grovermotiftrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909f31c4-2444-455f-8c0a-5559254ed88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from logging import Logger\n",
    "from typing import List, Tuple\n",
    "from collections.abc import Callable\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from grover.model.models import GroverTask, GroverMotifTask\n",
    "from grover.util.multi_gpu_wrapper import MultiGpuWrapper as mgw\n",
    "\n",
    "#add for Topology predict\n",
    "from grover.topology.chemutils import group_node_rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9888a6-b17f-47ec-aab1-d8d056a46a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GROVERMotifTrainer:\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 embedding_model: Module,\n",
    "                 topology_model: Module,\n",
    "                 atom_vocab_size: int,  # atom vocab size\n",
    "                 bond_vocab_size: int,\n",
    "                 fg_size: int,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 test_dataloader: DataLoader,\n",
    "                 optimizer_builder: Callable,\n",
    "                 scheduler_builder: Callable,\n",
    "                 logger: Logger = None,\n",
    "                 with_cuda: bool = False,\n",
    "                 enable_multi_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        The init function of GROVERTrainer\n",
    "        :param args: the input arguments.\n",
    "        :param embedding_model: the model to generate atom/bond embeddings.\n",
    "        :param topology_model : the model to predict topology of molecule from embeddings\n",
    "        :param atom_vocab_size: the vocabulary size of atoms.\n",
    "        :param bond_vocab_size: the vocabulary size of bonds.\n",
    "        :param fg_size: the size of semantic motifs (functional groups)\n",
    "        :param train_dataloader: the data loader of train data.\n",
    "        :param test_dataloader: the data loader of validation data.\n",
    "        :param optimizer_builder: the function of building the optimizer.\n",
    "        :param scheduler_builder: the function of building the scheduler.\n",
    "        :param logger: the logger\n",
    "        :param with_cuda: enable gpu training.\n",
    "        :param enable_multi_gpu: enable multi_gpu traning.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.with_cuda = with_cuda\n",
    "        self.grover = embedding_model\n",
    "        self.model = GroverMotifTask(args, embedding_model, atom_vocab_size, bond_vocab_size, fg_size)\n",
    "        self.motif_model = topology_model\n",
    "        self.loss_func = self.model.get_loss_func(args)\n",
    "        self.enable_multi_gpu = enable_multi_gpu\n",
    "\n",
    "        self.atom_vocab_size = atom_vocab_size\n",
    "        self.bond_vocab_size = bond_vocab_size\n",
    "        self.debug = logger.debug if logger is not None else print\n",
    "\n",
    "        if self.with_cuda:\n",
    "            # print(\"Using %d GPUs for training.\" % (torch.cuda.device_count()))\n",
    "            self.model = self.model.cuda()\n",
    "            self.motif_model = self.motif_model.cuda()\n",
    "\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        self.optimizer = optimizer_builder(self.model, self.args)\n",
    "        self.motif_optimizer = torch.optim.Adam(self.motif_model.parameters(), lr=args.init_lr, weight_decay=args.weight_decay)\n",
    "        self.scheduler = scheduler_builder(self.optimizer, self.args)\n",
    "        if self.enable_multi_gpu:\n",
    "            self.optimizer = mgw.DistributedOptimizer(self.optimizer,\n",
    "                                                      named_parameters=self.model.named_parameters())\n",
    "        self.args = args\n",
    "        self.n_iter = 0\n",
    "\n",
    "    def broadcast_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Broadcast parameters before training.\n",
    "        :return: no return.\n",
    "        \"\"\"\n",
    "        if self.enable_multi_gpu:\n",
    "            # broadcast parameters & optimizer state.\n",
    "            mgw.broadcast_parameters(self.model.state_dict(), root_rank=0)\n",
    "            mgw.broadcast_optimizer_state(self.optimizer, root_rank=0)\n",
    "\n",
    "    def train(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The training iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return: the loss terms of current epoch.\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.train_data, train=True)\n",
    "        return self.iter(epoch, self.train_data, train=True)\n",
    "\n",
    "    def test(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The test/validaiion iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return:  the loss terms as a list\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.test_data, train=False)\n",
    "        return self.iter(epoch, self.test_data, train=False)\n",
    "\n",
    "    def mock_iter(self, epoch: int, data_loader: DataLoader, train: bool = True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a mock iteration. For test only.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        for _, _ in enumerate(data_loader):\n",
    "            self.scheduler.step()\n",
    "        cum_loss_sum = 0.0\n",
    "        self.n_iter += self.args.batch_size\n",
    "        return self.n_iter, cum_loss_sum, (0, 0, 0, 0, 0, 0)\n",
    "\n",
    "    def iter(self, epoch, data_loader, train=True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a training / validation iteration.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            self.motif_model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            self.motif_model.eval()\n",
    "\n",
    "        loss_sum, iter_count = 0, 0\n",
    "        cum_loss_sum, cum_iter_count = 0, 0\n",
    "        av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum, bv_dist_loss_sum, fg_dist_loss_sum, node_loss_sum, topo_loss_sum = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "        topo_acc_avg, node_acc_avg = 0, 0\n",
    "        # loss_func = self.model.get_loss_func(self.args)\n",
    "\n",
    "        for _, item in enumerate(data_loader):\n",
    "            batch_graph = item[\"graph_input\"]\n",
    "            targets = item[\"targets\"]\n",
    "            \n",
    "            # add this for motif generation\n",
    "            moltree = item[\"moltree\"]\n",
    "\n",
    "            if next(self.model.parameters()).is_cuda:\n",
    "                targets[\"av_task\"] = targets[\"av_task\"].cuda()\n",
    "                targets[\"bv_task\"] = targets[\"bv_task\"].cuda()\n",
    "                targets[\"fg_task\"] = targets[\"fg_task\"].cuda()\n",
    "            \n",
    "            preds = self.model(batch_graph)\n",
    "            emb_vector = preds['emb_vec']\n",
    "\n",
    "            # add this for motif generation\n",
    "            if self.args.embedding_output_type == 'atom':\n",
    "                emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "                emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "                \n",
    "                node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = self.motif_model(moltree, emb_afa_grouped)\n",
    "                node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = self.motif_model(moltree, emb_afb_grouped)\n",
    "                \n",
    "                node_loss = node_afa_loss + node_afb_loss\n",
    "                topo_loss = topo_afa_loss + topo_afb_loss\n",
    "                node_acc = (node_afa_acc + node_afb_acc)/2\n",
    "                topo_acc = (topo_afa_acc + topo_afb_acc)/2\n",
    "                \n",
    "            elif self.args.embedding_output_type == 'bond':\n",
    "                emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "                emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "                \n",
    "                node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = self.motif_model(moltree, emb_bfa_grouped)\n",
    "                node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = self.motif_model(moltree, emb_bfb_grouped)\n",
    "                \n",
    "                node_loss = node_bfa_loss + node_bfb_loss\n",
    "                topo_loss = topo_bfa_loss + topo_bfb_loss\n",
    "                node_acc = (node_bfa_acc + node_bfb_acc)/2\n",
    "                topo_acc = (topo_bfa_acc + topo_bfb_acc)/2\n",
    "                \n",
    "            elif self.args.embedding_output_type == \"both\":\n",
    "                emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "                emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "                emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "                emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "                \n",
    "                node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = self.motif_model(moltree, emb_afa_grouped)\n",
    "                node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = self.motif_model(moltree, emb_afb_grouped)\n",
    "                node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = self.motif_model(moltree, emb_bfa_grouped)\n",
    "                node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = self.motif_model(moltree, emb_bfb_grouped)\n",
    "                \n",
    "                node_loss = node_afa_loss + node_afb_loss + node_bfa_loss + node_bfb_loss\n",
    "                topo_loss = topo_afa_loss + topo_afb_loss + topo_bfa_loss + topo_bfb_loss\n",
    "                node_acc = (node_afa_acc + node_afb_acc + node_bfa_acc + node_bfb_acc)/4\n",
    "                topo_acc = (topo_afa_acc + topo_afb_acc + topo_bfa_acc + topo_bfb_acc)/4\n",
    "\n",
    "            # # ad-hoc code, for visualizing a model, comment this block when it is not needed\n",
    "            # import dglt.contrib.grover.vis_model as vis_model\n",
    "            # for task in ['av_task', 'bv_task', 'fg_task']:\n",
    "            #     vis_graph = vis_model.make_dot(self.model(batch_graph)[task],\n",
    "            #                                    params=dict(self.model.named_parameters()))\n",
    "            #     # vis_graph.view()\n",
    "            #     vis_graph.render(f\"{self.args.backbone}_model_{task}_vis.png\", format=\"png\")\n",
    "            # exit()\n",
    "\n",
    "            loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss = self.loss_func(preds, targets)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            iter_count += self.args.batch_size\n",
    "            \n",
    "            # add for topology loss\n",
    "            loss += topo_loss\n",
    "            loss += node_loss\n",
    "            topo_loss_sum += topo_loss.item()\n",
    "            node_loss_sum += node_loss.item()\n",
    "\n",
    "            if train:\n",
    "                cum_loss_sum += loss.item()\n",
    "                # Run model\n",
    "                self.model.zero_grad()\n",
    "                self.motif_model.zero_grad()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.motif_optimizer.step()\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                # For eval model, only consider the loss of three task.\n",
    "                cum_loss_sum += av_loss.item()\n",
    "                cum_loss_sum += bv_loss.item()\n",
    "                cum_loss_sum += fg_loss.item()\n",
    "\n",
    "            av_loss_sum += av_loss.item()\n",
    "            bv_loss_sum += bv_loss.item()\n",
    "            fg_loss_sum += fg_loss.item()\n",
    "            av_dist_loss_sum += av_dist_loss.item() if type(av_dist_loss) != float else av_dist_loss\n",
    "            bv_dist_loss_sum += bv_dist_loss.item() if type(bv_dist_loss) != float else bv_dist_loss\n",
    "            fg_dist_loss_sum += fg_dist_loss.item() if type(fg_dist_loss) != float else fg_dist_loss\n",
    "\n",
    "            cum_iter_count += 1\n",
    "            self.n_iter += self.args.batch_size\n",
    "\n",
    "            # Debug only.\n",
    "            # if i % 50 == 0:\n",
    "            #     print(f\"epoch: {epoch}, batch_id: {i}, av_loss: {av_loss}, bv_loss: {bv_loss}, \"\n",
    "            #           f\"fg_loss: {fg_loss}, av_dist_loss: {av_dist_loss}, bv_dist_loss: {bv_dist_loss}, \"\n",
    "            #           f\"fg_dist_loss: {fg_dist_loss}\")\n",
    "\n",
    "        cum_loss_sum /= cum_iter_count\n",
    "        av_loss_sum /= cum_iter_count\n",
    "        bv_loss_sum /= cum_iter_count\n",
    "        fg_loss_sum /= cum_iter_count\n",
    "        av_dist_loss_sum /= cum_iter_count\n",
    "        bv_dist_loss_sum /= cum_iter_count\n",
    "        fg_dist_loss_sum /= cum_iter_count\n",
    "        \n",
    "        topo_loss_sum /= cum_iter_count\n",
    "        node_loss_sum /= cum_iter_count\n",
    "\n",
    "        return self.n_iter, cum_loss_sum, (av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum,\n",
    "                                           bv_dist_loss_sum, fg_dist_loss_sum, topo_loss_sum, node_loss_sum, topo_acc, node_acc)\n",
    "\n",
    "    def save(self, epoch, file_path, name=None) -> str:\n",
    "        \"\"\"\n",
    "        Save the intermediate models during training.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to save the model.\n",
    "        :return: the output path.\n",
    "        \"\"\"\n",
    "        # add specific time in model fine name, in order to distinguish different saved models\n",
    "        now = time.localtime()\n",
    "        if name is None:\n",
    "            name = \"_%04d_%02d_%02d_%02d_%02d_%02d\" % (\n",
    "                now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "        output_path = file_path + name + \".ep%d\" % epoch\n",
    "        scaler = None\n",
    "        features_scaler = None\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch,\n",
    "            'data_scaler': {\n",
    "                'means': scaler.means,\n",
    "                'stds': scaler.stds\n",
    "            } if scaler is not None else None,\n",
    "            'features_scaler': {\n",
    "                'means': features_scaler.means,\n",
    "                'stds': features_scaler.stds\n",
    "            } if features_scaler is not None else None\n",
    "        }\n",
    "        torch.save(state, output_path)\n",
    "\n",
    "        # Is this necessary?\n",
    "        # if self.with_cuda:\n",
    "        #    self.model = self.model.cuda()\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "\n",
    "    def save_tmp(self, epoch, file_path, rank=0):\n",
    "        \"\"\"\n",
    "        Save the models for auto-restore during training.\n",
    "        The model are stored in file_path/tmp folder and will replaced on each epoch.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        store_path = os.path.join(file_path, \"tmp\")\n",
    "        if not os.path.exists(store_path):\n",
    "            os.makedirs(store_path, exist_ok=True)\n",
    "        store_path = os.path.join(store_path, \"model.%d\" % rank)\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        torch.save(state, store_path)\n",
    "\n",
    "    def restore(self, file_path, rank=0) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Restore the training state saved by save_tmp.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return: the restored epoch number and the scheduler_step in scheduler.\n",
    "        \"\"\"\n",
    "        cpt_path = os.path.join(file_path, \"tmp\", \"model.%d\" % rank)\n",
    "        if not os.path.exists(cpt_path):\n",
    "            print(\"No checkpoint found %d\")\n",
    "            return 0, 0\n",
    "        cpt = torch.load(cpt_path)\n",
    "        self.model.load_state_dict(cpt[\"state_dict\"])\n",
    "        self.optimizer.load_state_dict(cpt[\"optimizer\"])\n",
    "        epoch = cpt[\"epoch\"]\n",
    "        scheduler_step = cpt[\"scheduler_step\"]\n",
    "        self.scheduler.current_step = scheduler_step\n",
    "        print(\"Restore checkpoint, current epoch: %d\" % (epoch))\n",
    "        return epoch, scheduler_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa567d-5ab2-40b6-b2a2-0c59e5710b64",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run_motif_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac507d5a-1734-4e38-a5a3-bf84c0047667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "from grover.data.dist_sampler import DistributedSampler\n",
    "from grover.data.groverdataset import get_data, split_data, GroverCollator, BatchMolDataset, get_motif_data, split_data_motif, GroverMotifCollator, BatchMolDataset_motif\n",
    "from grover.data.torchvocab import MolVocab\n",
    "from grover.model.models import GROVEREmbedding\n",
    "from grover.util.multi_gpu_wrapper import MultiGpuWrapper as mgw\n",
    "from grover.util.nn_utils import param_count\n",
    "from grover.util.utils import build_optimizer, build_lr_scheduler\n",
    "from task.grovertrainer import GROVERTrainer, GROVERMotifTrainer\n",
    "\n",
    "from grover.topology.mol_tree import Motif_Vocab\n",
    "from grover.topology.motif_generation import Motif_Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "848fe2c1-2007-4e74-b062-78b1ffcf79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_motif(data,\n",
    "               split_type='random',\n",
    "               sizes=(0.8, 0.1, 0.1),\n",
    "               seed=0,\n",
    "               logger=None):\n",
    "    \"\"\"\n",
    "    Split data with given train/validation/test ratio.\n",
    "    :param data:\n",
    "    :param split_type:\n",
    "    :param sizes:\n",
    "    :param seed:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(sizes) == 3 and sum(sizes) == 1\n",
    "\n",
    "    if split_type == \"random\":\n",
    "        data.shuffle(seed=seed)\n",
    "        data = data.data\n",
    "\n",
    "        train_size = int(sizes[0] * len(data))\n",
    "        train_val_size = int((sizes[0] + sizes[1]) * len(data))\n",
    "\n",
    "        train = data[:train_size]\n",
    "        val = data[train_size:train_val_size]\n",
    "        test = data[train_val_size:]\n",
    "\n",
    "        return BatchMolDataset_motif(train), BatchMolDataset_motif(val), BatchMolDataset_motif(test)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Do not support %s splits\" % split_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a27791-9346-4b28-9a12-468aa1de6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_load_data(dataset: BatchMolDataset, rank: int, num_replicas: int, sample_per_file: int = None, epoch: int = 0):\n",
    "\n",
    "    mock_sampler = DistributedSampler(dataset, num_replicas=num_replicas, rank=rank, shuffle=False,\n",
    "                                      sample_per_file=sample_per_file)\n",
    "    mock_sampler.set_epoch(epoch)\n",
    "    pre_indices = mock_sampler.get_indices()\n",
    "    for i in pre_indices:\n",
    "        dataset.load_data(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a279da9-13dc-4e47-b0cb-0306b906ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_motif_training(args, logger):\n",
    "    \"\"\"\n",
    "    Run the pretrain task with topology predict.\n",
    "    :param args:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # initalize the logger.\n",
    "    if logger is not None:\n",
    "        debug, _ = logger.debug, logger.info\n",
    "    else:\n",
    "        debug = print\n",
    "\n",
    "    # initialize the horovod library\n",
    "    if args.enable_multi_gpu:\n",
    "        mgw.init()\n",
    "\n",
    "    # binding training to GPUs.\n",
    "    master_worker = (mgw.rank() == 0) if args.enable_multi_gpu else True\n",
    "    # pin GPU to local rank. By default, we use gpu:0 for training.\n",
    "    local_gpu_idx = mgw.local_rank() if args.enable_multi_gpu else 0\n",
    "    with_cuda = args.cuda\n",
    "    if with_cuda:\n",
    "        torch.cuda.set_device(local_gpu_idx)\n",
    "\n",
    "    # get rank an  number of workers\n",
    "    rank = mgw.rank() if args.enable_multi_gpu else 0\n",
    "    num_replicas = mgw.size() if args.enable_multi_gpu else 1\n",
    "    # print(\"Rank: %d Rep: %d\" % (rank, num_replicas))\n",
    "\n",
    "    # load file paths of the data.\n",
    "    if master_worker:\n",
    "        print(args)\n",
    "        if args.enable_multi_gpu:\n",
    "            debug(\"Total workers: %d\" % (mgw.size()))\n",
    "        debug('Loading data')\n",
    "    data, sample_per_file = get_motif_data(data_path=args.data_path)\n",
    "\n",
    "    # data splitting\n",
    "    if master_worker:\n",
    "        debug(f'Splitting data with seed 0.')\n",
    "    train_data, test_data, _ = split_data_motif(data=data, sizes=(0.9, 0.1, 0.0), seed=0, logger=logger)\n",
    "\n",
    "    # Here the true train data size is the train_data divided by #GPUs\n",
    "    if args.enable_multi_gpu:\n",
    "        args.train_data_size = len(train_data) // mgw.size()\n",
    "    else:\n",
    "        args.train_data_size = len(train_data)\n",
    "    if master_worker:\n",
    "        debug(f'Total size = {len(data):,} | '\n",
    "              f'train size = {len(train_data):,} | val size = {len(test_data):,}')\n",
    "\n",
    "    # load atom and bond vocabulary and the semantic motif labels.\n",
    "    atom_vocab = MolVocab.load_vocab(args.atom_vocab_path)\n",
    "    bond_vocab = MolVocab.load_vocab(args.bond_vocab_path)\n",
    "    atom_vocab_size, bond_vocab_size = len(atom_vocab), len(bond_vocab)\n",
    "\n",
    "    # Load motif vocabulary for pretrain\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    motif_vocab = [x.strip(\"\\r\\n \") for x in open(args.motif_vocab_path)]\n",
    "    motif_vocab = Motif_Vocab(motif_vocab)\n",
    "\n",
    "    # Hard coding here, since we haven't load any data yet!\n",
    "    fg_size = 85\n",
    "    shared_dict = {}\n",
    "    motif_collator = GroverMotifCollator(shared_dict=shared_dict, atom_vocab=atom_vocab, bond_vocab=bond_vocab, args=args)\n",
    "    if master_worker:\n",
    "        debug(\"atom vocab size: %d, bond vocab size: %d, Number of FG tasks: %d\" % (atom_vocab_size,\n",
    "                                                                                    bond_vocab_size, fg_size))\n",
    "\n",
    "    # Define the distributed sampler. If using the single card, the sampler will be None.\n",
    "    train_sampler = None\n",
    "    test_sampler = None\n",
    "    shuffle = True\n",
    "    if args.enable_multi_gpu:\n",
    "        # If not shuffle, the performance may decayed.\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=True, sample_per_file=sample_per_file)\n",
    "        # Here sample_per_file in test_sampler is None, indicating the test sampler would not divide the test samples by\n",
    "        # rank. (TODO: bad design here.)\n",
    "        test_sampler = DistributedSampler(\n",
    "            test_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=False)\n",
    "        train_sampler.set_epoch(args.epochs)\n",
    "        test_sampler.set_epoch(1)\n",
    "        # if we enables multi_gpu training. shuffle should be disabled.\n",
    "        shuffle = False\n",
    "\n",
    "    # Pre load data. (Maybe unnecessary. )\n",
    "    pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
    "    pre_load_data(test_data, rank, num_replicas)\n",
    "    if master_worker:\n",
    "        # print(\"Pre-loaded training data: %d\" % train_data.count_loaded_datapoints())\n",
    "        print(\"Pre-loaded test data: %d\" % test_data.count_loaded_datapoints())\n",
    "\n",
    "    # Build dataloader\n",
    "    train_data_dl = DataLoader(train_data,\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=shuffle,\n",
    "                               num_workers=12,\n",
    "                               sampler=train_sampler,\n",
    "                               collate_fn=motif_collator)\n",
    "    test_data_dl = DataLoader(test_data,\n",
    "                              batch_size=args.batch_size,\n",
    "                              shuffle=shuffle,\n",
    "                              num_workers=10,\n",
    "                              sampler=test_sampler,\n",
    "                              collate_fn=motif_collator)\n",
    "\n",
    "    # Build the embedding model.\n",
    "    grover_model = GROVEREmbedding(args)\n",
    "    \n",
    "    # build the topology predict model.\n",
    "    motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order)\n",
    "\n",
    "    #  Build the trainer.\n",
    "    trainer = GROVERMotifTrainer(args=args,\n",
    "                            embedding_model=grover_model,\n",
    "                            topology_model=motif_model,\n",
    "                            atom_vocab_size=atom_vocab_size,\n",
    "                            bond_vocab_size=bond_vocab_size,\n",
    "                            fg_size=fg_size,\n",
    "                            train_dataloader=train_data_dl,\n",
    "                            test_dataloader=test_data_dl,\n",
    "                            optimizer_builder=build_optimizer,\n",
    "                            scheduler_builder=build_lr_scheduler,\n",
    "                            logger=logger,\n",
    "                            with_cuda=with_cuda,\n",
    "                            enable_multi_gpu=args.enable_multi_gpu)\n",
    "\n",
    "    # Restore the interrupted training.\n",
    "    model_dir = os.path.join(args.save_dir, \"model\")\n",
    "    resume_from_epoch = 0\n",
    "    resume_scheduler_step = 0\n",
    "    if master_worker:\n",
    "        resume_from_epoch, resume_scheduler_step = trainer.restore(model_dir)\n",
    "    if args.enable_multi_gpu:\n",
    "        resume_from_epoch = mgw.broadcast(torch.tensor(resume_from_epoch), root_rank=0, name=\"resume_from_epoch\").item()\n",
    "        resume_scheduler_step = mgw.broadcast(torch.tensor(resume_scheduler_step),\n",
    "                                              root_rank=0, name=\"resume_scheduler_step\").item()\n",
    "        trainer.scheduler.current_step = resume_scheduler_step\n",
    "        print(\"Restored epoch: %d Restored scheduler step: %d\" % (resume_from_epoch, trainer.scheduler.current_step))\n",
    "    trainer.broadcast_parameters()\n",
    "\n",
    "    # Print model details.\n",
    "    if master_worker:\n",
    "        # Change order here.\n",
    "        print(grover_model)\n",
    "        print(\"Total parameters: %d\" % param_count(trainer.grover))\n",
    "\n",
    "    #wandb\n",
    "    if args.wandb :\n",
    "        wandb.init(project=args.wandb_name)\n",
    "        wandb.config = args\n",
    "        wandb.watch(grover_model)\n",
    "        \n",
    "    # Perform training.\n",
    "    best_val_loss = 0\n",
    "    best_val_epoch = 0\n",
    "    best_model_dir = os.path.join(args.save_dir, \"model_best\")\n",
    "    for epoch in range(resume_from_epoch + 1, args.epochs):\n",
    "        s_time = time.time()\n",
    "\n",
    "        # Data pre-loading.\n",
    "        if args.enable_multi_gpu:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "            train_data.clean_cache()\n",
    "            idxs = train_sampler.get_indices()\n",
    "            for local_gpu_idx in idxs:\n",
    "                train_data.load_data(local_gpu_idx)\n",
    "        d_time = time.time() - s_time\n",
    "\n",
    "        # perform training and validation.\n",
    "        s_time = time.time()\n",
    "        _, train_loss, _ = trainer.train(epoch)\n",
    "        t_time = time.time() - s_time\n",
    "        s_time = time.time()\n",
    "        _, val_loss, detailed_loss_val = trainer.test(epoch)\n",
    "        val_av_loss, val_bv_loss, val_fg_loss, _, _, _, val_topo_loss, val_node_loss, topo_acc, node_acc = detailed_loss_val\n",
    "        v_time = time.time() - s_time\n",
    "        \n",
    "        if best_val_loss > val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_epoch = epoch\n",
    "            trainer.save(epoch, best_model_dir)\n",
    "\n",
    "        if args.wandb :         \n",
    "            wandb.log({\"train_loss\" : train_loss, \"val_loss\" : val_loss, \"topo_loss\" : val_topo_loss})\n",
    "        \n",
    "        # print information.\n",
    "        if master_worker:\n",
    "            print('Epoch: {:04d}'.format(epoch),\n",
    "                  'loss_train: {:.6f}'.format(train_loss),\n",
    "                  'loss_val: {:.6f}'.format(val_loss),\n",
    "                  'loss_val_av: {:.6f}'.format(val_av_loss),\n",
    "                  'loss_val_bv: {:.6f}'.format(val_bv_loss),\n",
    "                  'loss_val_fg: {:.6f}'.format(val_fg_loss),\n",
    "                  'loss_val_topo: {:.6f}'.format(val_topo_loss),\n",
    "                  'loss_val_node: {:.6f}'.format(val_node_loss),\n",
    "                  'acc_topo: {:.6f}'.format(topo_acc),\n",
    "                  'acc_node: {:.6f}'.format(node_acc),\n",
    "                  'cur_lr: {:.5f}'.format(trainer.scheduler.get_lr()[0]),\n",
    "                  't_time: {:.4f}s'.format(t_time),\n",
    "                  'v_time: {:.4f}s'.format(v_time),\n",
    "                  'd_time: {:.4f}s'.format(d_time), flush=True)\n",
    "            \n",
    "        \n",
    "            if epoch % args.save_interval == 0:\n",
    "                trainer.save(epoch, model_dir)\n",
    "\n",
    "\n",
    "            trainer.save_tmp(epoch, model_dir, rank)\n",
    "\n",
    "    # Only save final version.\n",
    "    if master_worker:\n",
    "        trainer.save(args.epochs, model_dir, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23d49d-8669-42e7-bed6-6c9b7722c639",
   "metadata": {
    "tags": []
   },
   "source": [
    "# dataset 관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8eb4868-bff7-4de5-b7b5-266d3b0003cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from argparse import Namespace\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from grover.data.molfeaturegenerator import get_features_generator\n",
    "from grover.data.scaler import StandardScaler\n",
    "\n",
    "import grover.util.utils as feautils\n",
    "from grover.data import mol2graph\n",
    "from grover.data.moldataset import MoleculeDatapoint\n",
    "from grover.data.task_labels import atom_to_vocab, bond_to_vocab\n",
    "\n",
    "from grover.topology.mol_tree import MolTree, MolTree_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb5b661d-320c-4462-9872-91eb63abdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이건 pretrain.py로\n",
    "def pre_load_data_motif(dataset: BatchMolDataset, rank: int, num_replicas: int, sample_per_file: int = None, epoch: int = 0):\n",
    "    \"\"\"\n",
    "    Pre-load data at the beginning of each epoch.\n",
    "    :param dataset: the training dataset.\n",
    "    :param rank: the rank of the current worker.\n",
    "    :param num_replicas: the replicas.\n",
    "    :param sample_per_file: the number of the data points in each file. When sample_per_file is None, all data will be\n",
    "    loaded. It implies the testing phase. (TODO: bad design here.)\n",
    "    :param epoch: the epoch number.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mock_sampler = DistributedSampler(dataset, num_replicas=num_replicas, rank=rank, shuffle=False,\n",
    "                                      sample_per_file=sample_per_file)\n",
    "    mock_sampler.set_epoch(epoch)\n",
    "    pre_indices = mock_sampler.get_indices()\n",
    "    for i in pre_indices:\n",
    "        dataset.load_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00fbb3be-0b46-4439-9f26-00d122d36a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDatapoint_motif:\n",
    "    \"\"\"A MoleculeDatapoint contains a single molecule and its associated features and targets.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 line: List[str],\n",
    "                 args: Namespace = None,\n",
    "                 features: np.ndarray = None,\n",
    "                 moltrees: object = None,\n",
    "                 use_compound_names: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes a MoleculeDatapoint, which contains a single molecule.\n",
    "\n",
    "        :param line: A list of strings generated by separating a line in a data CSV file by comma.\n",
    "        :param args: Arguments.\n",
    "        :param features: A numpy array containing additional features (ex. Morgan fingerprint).\n",
    "        :param use_compound_names: Whether the data CSV includes the compound name on each line.\n",
    "        \"\"\"\n",
    "        self.features_generator = None\n",
    "        self.args = None\n",
    "        if args is not None:\n",
    "            if hasattr(args, \"features_generator\"):\n",
    "                self.features_generator = args.features_generator\n",
    "            self.args = args\n",
    "\n",
    "        if features is not None and self.features_generator is not None:\n",
    "            raise ValueError('Currently cannot provide both loaded features and a features generator.')\n",
    "\n",
    "        self.features = features\n",
    "        self.moltree_path = moltrees[0]\n",
    "        self.moltree_index = moltrees[1]\n",
    "        self.moltrees = moltrees\n",
    "\n",
    "        if use_compound_names:\n",
    "            self.compound_name = line[0]  # str\n",
    "            line = line[1:]\n",
    "        else:\n",
    "            self.compound_name = None\n",
    "\n",
    "        self.smiles = line[0]  # str\n",
    "\n",
    "\n",
    "        # Generate additional features if given a generator\n",
    "        if self.features_generator is not None:\n",
    "            self.features = []\n",
    "            mol = Chem.MolFromSmiles(self.smiles)\n",
    "            for fg in self.features_generator:\n",
    "                features_generator = get_features_generator(fg)\n",
    "                if mol is not None and mol.GetNumHeavyAtoms() > 0:\n",
    "                    if fg in ['morgan', 'morgan_count']:\n",
    "                        self.features.extend(features_generator(mol, num_bits=args.num_bits))\n",
    "                    else:\n",
    "                        self.features.extend(features_generator(mol))\n",
    "\n",
    "            self.features = np.array(self.features)\n",
    "\n",
    "        # Fix nans in features\n",
    "        if self.features is not None:\n",
    "            replace_token = 0\n",
    "            self.features = np.where(np.isnan(self.features), replace_token, self.features)\n",
    "\n",
    "        # Create targets\n",
    "        self.targets = [float(x) if x != '' else None for x in line[1:]]\n",
    "\n",
    "    def set_features(self, features: np.ndarray):\n",
    "        \"\"\"\n",
    "        Sets the features of the molecule.\n",
    "\n",
    "        :param features: A 1-D numpy array of features for the molecule.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        \n",
    "    def set_moltrees(self, moltrees: list):\n",
    "        \"\"\"\n",
    "        Sets the moltree of the molecule.\n",
    "\n",
    "        :param moltree: moltree object\n",
    "        \"\"\"\n",
    "        self.moltrees = moltrees\n",
    "        \n",
    "    def load_moltree(self):\n",
    "        \"\"\"\n",
    "        load moltree of the molecule.\n",
    "        \"\"\"\n",
    "        with open(self.moltree_path, 'rb') as f:\n",
    "            self.moltrees = pickle.load(f)[self.moltree_index]\n",
    "        f.close()\n",
    "        \n",
    "    def clean_moltree(self):\n",
    "        \"\"\"\n",
    "        clean moltree for memory\n",
    "        \"\"\"\n",
    "        self.moltrees = None\n",
    "\n",
    "    def num_tasks(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of prediction tasks.\n",
    "\n",
    "        :return: The number of tasks.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def set_targets(self, targets: List[float]):\n",
    "        \"\"\"\n",
    "        Sets the targets of a molecule.\n",
    "\n",
    "        :param targets: A list of floats containing the targets.\n",
    "        \"\"\"\n",
    "        self.targets = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95193341-07ea-450b-9523-81affebd9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDatapoint_motif:\n",
    "    def __init__(self,\n",
    "                 smiles_file,\n",
    "                 feature_file,\n",
    "                 moltree_file,\n",
    "                 n_samples,\n",
    "                 ):\n",
    "        self.smiles_file = smiles_file\n",
    "        self.feature_file = feature_file\n",
    "        self.moltree_file = moltree_file\n",
    "        # deal with the last batch graph numbers.\n",
    "        self.n_samples = n_samples\n",
    "        self.datapoints = None\n",
    "\n",
    "    def load_datapoints(self):\n",
    "        features = self.load_feature()\n",
    "        #moltrees = self.load_moltree()\n",
    "        moltrees = self.moltree_file\n",
    "        self.datapoints = []\n",
    "\n",
    "        with open(self.smiles_file) as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for i, line in enumerate(reader):\n",
    "                # line = line[0]\n",
    "#                d = MoleculeDatapoint_motif(line=line,\n",
    "#                                      features=features[i],\n",
    "#                                      moltrees=moltrees[i])\n",
    "                d = MoleculeDatapoint_motif(line=line,\n",
    "                                      features=features[i],\n",
    "                                      moltrees=[str(self.moltree_file),i])\n",
    "                self.datapoints.append(d)\n",
    "        f.close()\n",
    "\n",
    "        assert len(self.datapoints) == self.n_samples\n",
    "\n",
    "    def load_feature(self):\n",
    "        return feautils.load_features(self.feature_file)\n",
    "    \n",
    "    def load_moltree(self):\n",
    "        self.datapoints.load_moltree()\n",
    "\n",
    "    def shuffle(self):\n",
    "        pass\n",
    "\n",
    "    def clean_cache(self):\n",
    "        del self.datapoints\n",
    "        self.datapoints = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert self.datapoints is not None\n",
    "        return self.datapoints[idx]\n",
    "\n",
    "    def is_loaded(self):\n",
    "        return self.datapoints is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2188d4-ed70-43ad-ad74-a1c5e2b91a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMolDataset_motif(Dataset):\n",
    "    def __init__(self, data: List[BatchDatapoint_motif],\n",
    "                 graph_per_file=None):\n",
    "        self.data = data\n",
    "\n",
    "        self.len = 0\n",
    "        for d in self.data:\n",
    "            self.len += len(d)\n",
    "        if graph_per_file is not None:\n",
    "            self.sample_per_file = graph_per_file\n",
    "        else:\n",
    "            self.sample_per_file = len(self.data[0]) if len(self.data) != 0 else None\n",
    "\n",
    "    def shuffle(self, seed: int = None):\n",
    "        pass\n",
    "\n",
    "    def clean_cache(self):\n",
    "        for d in self.data:\n",
    "            d.clean_cache()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx) -> Union[MoleculeDatapoint_motif, List[MoleculeDatapoint_motif]]:\n",
    "        # print(idx)\n",
    "        dp_idx = int(idx / self.sample_per_file)\n",
    "        real_idx = idx % self.sample_per_file\n",
    "        return self.data[dp_idx][real_idx]\n",
    "\n",
    "    def load_data(self, idx):\n",
    "        dp_idx = int(idx / self.sample_per_file)\n",
    "        if not self.data[dp_idx].is_loaded():\n",
    "            self.data[dp_idx].load_datapoints()\n",
    "\n",
    "    def count_loaded_datapoints(self):\n",
    "        res = 0\n",
    "        for d in self.data:\n",
    "            if d.is_loaded():\n",
    "                res += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a30fdb13-8acf-4f1a-8eaf-c341c091c77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GroverMotifCollator(object):\n",
    "    def __init__(self, shared_dict, atom_vocab, bond_vocab, args):\n",
    "        self.args = args\n",
    "        self.shared_dict = shared_dict\n",
    "        self.atom_vocab = atom_vocab\n",
    "        self.bond_vocab = bond_vocab\n",
    "\n",
    "    def atom_random_mask(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Perform the random mask operation on atoms.\n",
    "        :param smiles_batch:\n",
    "        :return: The corresponding atom labels.\n",
    "        \"\"\"\n",
    "        # There is a zero padding.\n",
    "        vocab_label = [0]\n",
    "        percent = 0.15\n",
    "        for smi in smiles_batch:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            mlabel = [0] * mol.GetNumAtoms()\n",
    "            n_mask = math.ceil(mol.GetNumAtoms() * percent)\n",
    "            perm = np.random.permutation(mol.GetNumAtoms())[:n_mask]\n",
    "            for p in perm:\n",
    "                atom = mol.GetAtomWithIdx(int(p))\n",
    "                mlabel[p] = self.atom_vocab.stoi.get(atom_to_vocab(mol, atom), self.atom_vocab.other_index)\n",
    "\n",
    "            vocab_label.extend(mlabel)\n",
    "        return vocab_label\n",
    "\n",
    "    def bond_random_mask(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Perform the random mask operaiion on bonds.\n",
    "        :param smiles_batch:\n",
    "        :return: The corresponding bond labels.\n",
    "        \"\"\"\n",
    "        # There is a zero padding.\n",
    "        vocab_label = [0]\n",
    "        percent = 0.15\n",
    "        for smi in smiles_batch:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            nm_atoms = mol.GetNumAtoms()\n",
    "            nm_bonds = mol.GetNumBonds()\n",
    "            mlabel = []\n",
    "            n_mask = math.ceil(nm_bonds * percent)\n",
    "            perm = np.random.permutation(nm_bonds)[:n_mask]\n",
    "            virtual_bond_id = 0\n",
    "            for a1 in range(nm_atoms):\n",
    "                for a2 in range(a1 + 1, nm_atoms):\n",
    "                    bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                    if bond is None:\n",
    "                        continue\n",
    "                    if virtual_bond_id in perm:\n",
    "                        label = self.bond_vocab.stoi.get(bond_to_vocab(mol, bond), self.bond_vocab.other_index)\n",
    "                        mlabel.extend([label])\n",
    "                    else:\n",
    "                        mlabel.extend([0])\n",
    "\n",
    "                    virtual_bond_id += 1\n",
    "            # todo: might need to consider bond_drop_rate\n",
    "            # todo: double check reverse bond\n",
    "            vocab_label.extend(mlabel)\n",
    "        return vocab_label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles_batch = [d.smiles for d in batch] # 여기서 말하는 batch는 batchmoldataset_motif다 그리고 d는 batchdatapoint_motif고\n",
    "        batchgraph = mol2graph(smiles_batch, self.shared_dict, self.args).get_components()\n",
    "\n",
    "        atom_vocab_label = torch.Tensor(self.atom_random_mask(smiles_batch)).long()\n",
    "        bond_vocab_label = torch.Tensor(self.bond_random_mask(smiles_batch)).long()\n",
    "        fgroup_label = torch.Tensor(np.array([d.features for d in batch])).float()\n",
    "        #[d.load_moltree() for d in batch]\n",
    "        moltree_batch = [d.moltrees for d in batch]\n",
    "        \n",
    "        # may be some mask here\n",
    "        res = {\"graph_input\": batchgraph,\n",
    "               \"targets\": {\"av_task\": atom_vocab_label,\n",
    "                           \"bv_task\": bond_vocab_label,\n",
    "                           \"fg_task\": fgroup_label},\n",
    "               \"moltree\" : moltree_batch\n",
    "               }\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cc103-3584-417b-9421-e319b0ef7b7d",
   "metadata": {},
   "source": [
    "## 혹시 이거 불러들여서?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14fab91e-57b9-4650-b8bd-5e5330b9b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae7448c3-3007-4ba7-bf04-573e21239442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_data(data_path, logger=None):\n",
    "    \"\"\"\n",
    "    Load data from the data_path.\n",
    "    :param data_path: the data_path.\n",
    "    :param logger: the logger.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    debug = logger.debug if logger is not None else print\n",
    "    summary_path = os.path.join(data_path, \"summary.txt\")\n",
    "    smiles_path = os.path.join(data_path, \"graph\")\n",
    "    feature_path = os.path.join(data_path, \"feature\")\n",
    "    moltree_path = os.path.join(data_path, \"moltrees\")\n",
    "\n",
    "    fin = open(summary_path)\n",
    "    n_files = int(fin.readline().strip().split(\":\")[-1])\n",
    "    n_samples = int(fin.readline().strip().split(\":\")[-1])\n",
    "    sample_per_file = int(fin.readline().strip().split(\":\")[-1])\n",
    "    debug(\"Loading data:\")\n",
    "    debug(\"Number of files: %d\" % n_files)\n",
    "    debug(\"Number of samples: %d\" % n_samples)\n",
    "    debug(\"Samples/file: %d\" % sample_per_file)\n",
    "\n",
    "    datapoints = []\n",
    "    for i in range(n_files):\n",
    "        smiles_path_i = os.path.join(smiles_path, str(i) + \".csv\")\n",
    "        feature_path_i = os.path.join(feature_path, str(i) + \".npz\")\n",
    "        moltree_path_i = os.path.join(moltree_path, str(i) + \".p\")\n",
    "        n_samples_i = sample_per_file if i != (n_files - 1) else n_samples % sample_per_file\n",
    "        datapoints.append(BatchDatapoint_motif(smiles_path_i, feature_path_i, moltree_path_i, n_samples_i))\n",
    "    return BatchMolDataset_motif(datapoints), sample_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eb6c208-48e9-44bd-898e-148495c2ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_motif(data,\n",
    "               split_type='random',\n",
    "               sizes=(0.8, 0.1, 0.1),\n",
    "               seed=0,\n",
    "               logger=None):\n",
    "    \"\"\"\n",
    "    Split data with given train/validation/test ratio.\n",
    "    :param data:\n",
    "    :param split_type:\n",
    "    :param sizes:\n",
    "    :param seed:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(sizes) == 3 and sum(sizes) == 1\n",
    "\n",
    "    if split_type == \"random\":\n",
    "        data.shuffle(seed=seed)\n",
    "        data = data.data\n",
    "\n",
    "        train_size = int(sizes[0] * len(data))\n",
    "        train_val_size = int((sizes[0] + sizes[1]) * len(data))\n",
    "\n",
    "        train = data[:train_size]\n",
    "        val = data[train_size:train_val_size]\n",
    "        test = data[train_val_size:]\n",
    "\n",
    "        return BatchMolDataset_motif(train), BatchMolDataset_motif(val), BatchMolDataset_motif(test)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Do not support %s splits\" % split_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d50aac7-9f74-485f-90a2-0682e61b9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GROVERMotifTrainer:\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 embedding_model: Module,\n",
    "                 topology_model: Module,\n",
    "                 atom_vocab_size: int,  # atom vocab size\n",
    "                 bond_vocab_size: int,\n",
    "                 fg_size: int,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 test_dataloader: DataLoader,\n",
    "                 optimizer_builder: Callable,\n",
    "                 scheduler_builder: Callable,\n",
    "                 logger: Logger = None,\n",
    "                 with_cuda: bool = False,\n",
    "                 enable_multi_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        The init function of GROVERTrainer\n",
    "        :param args: the input arguments.\n",
    "        :param embedding_model: the model to generate atom/bond embeddings.\n",
    "        :param topology_model : the model to predict topology of molecule from embeddings\n",
    "        :param atom_vocab_size: the vocabulary size of atoms.\n",
    "        :param bond_vocab_size: the vocabulary size of bonds.\n",
    "        :param fg_size: the size of semantic motifs (functional groups)\n",
    "        :param train_dataloader: the data loader of train data.\n",
    "        :param test_dataloader: the data loader of validation data.\n",
    "        :param optimizer_builder: the function of building the optimizer.\n",
    "        :param scheduler_builder: the function of building the scheduler.\n",
    "        :param logger: the logger\n",
    "        :param with_cuda: enable gpu training.\n",
    "        :param enable_multi_gpu: enable multi_gpu traning.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.with_cuda = with_cuda\n",
    "        self.grover = embedding_model\n",
    "        self.model = GroverMotifTask(args, embedding_model, atom_vocab_size, bond_vocab_size, fg_size)\n",
    "        self.motif_model = topology_model\n",
    "        self.loss_func = self.model.get_loss_func(args)\n",
    "        self.enable_multi_gpu = enable_multi_gpu\n",
    "\n",
    "        self.atom_vocab_size = atom_vocab_size\n",
    "        self.bond_vocab_size = bond_vocab_size\n",
    "        self.debug = logger.debug if logger is not None else print\n",
    "\n",
    "        if self.with_cuda:\n",
    "            # print(\"Using %d GPUs for training.\" % (torch.cuda.device_count()))\n",
    "            self.model = self.model.cuda()\n",
    "            self.motif_model = self.motif_model.cuda()\n",
    "\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        self.optimizer = optimizer_builder(self.model, self.args)\n",
    "        self.motif_optimizer = torch.optim.Adam(self.motif_model.parameters(), lr=args.init_lr, weight_decay=args.weight_decay)\n",
    "        self.scheduler = scheduler_builder(self.optimizer, self.args)\n",
    "        if self.enable_multi_gpu:\n",
    "            self.optimizer = mgw.DistributedOptimizer(self.optimizer,\n",
    "                                                      named_parameters=self.model.named_parameters())\n",
    "        self.args = args\n",
    "        self.n_iter = 0\n",
    "\n",
    "    def broadcast_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Broadcast parameters before training.\n",
    "        :return: no return.\n",
    "        \"\"\"\n",
    "        if self.enable_multi_gpu:\n",
    "            # broadcast parameters & optimizer state.\n",
    "            mgw.broadcast_parameters(self.model.state_dict(), root_rank=0)\n",
    "            mgw.broadcast_optimizer_state(self.optimizer, root_rank=0)\n",
    "\n",
    "    def train(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The training iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return: the loss terms of current epoch.\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.train_data, train=True)\n",
    "        return self.iter(epoch, self.train_data, train=True)\n",
    "\n",
    "    def test(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The test/validaiion iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return:  the loss terms as a list\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.test_data, train=False)\n",
    "        return self.iter(epoch, self.test_data, train=False)\n",
    "\n",
    "    def mock_iter(self, epoch: int, data_loader: DataLoader, train: bool = True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a mock iteration. For test only.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        for _, _ in enumerate(data_loader):\n",
    "            self.scheduler.step()\n",
    "        cum_loss_sum = 0.0\n",
    "        self.n_iter += self.args.batch_size\n",
    "        return self.n_iter, cum_loss_sum, (0, 0, 0, 0, 0, 0)\n",
    "\n",
    "    def iter(self, epoch, data_loader, train=True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a training / validation iteration.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            self.motif_model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            self.motif_model.eval()\n",
    "\n",
    "        loss_sum, iter_count = 0, 0\n",
    "        cum_loss_sum, cum_iter_count = 0, 0\n",
    "        av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum, bv_dist_loss_sum, fg_dist_loss_sum, node_loss_sum, topo_loss_sum = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "        topo_acc_avg, node_acc_avg = 0, 0\n",
    "        # loss_func = self.model.get_loss_func(self.args)\n",
    "\n",
    "        for _, item in enumerate(data_loader):\n",
    "            batch_graph = item[\"graph_input\"]\n",
    "            targets = item[\"targets\"]\n",
    "            \n",
    "            # add this for motif generation\n",
    "            moltree_paths = item[\"moltree\"]\n",
    "            \n",
    "            moltree = list()\n",
    "            for _, item in enumerate(moltree_paths):\n",
    "                moltree.append(load_moltree(item[0],item[1]))\n",
    "\n",
    "            if next(self.model.parameters()).is_cuda:\n",
    "                targets[\"av_task\"] = targets[\"av_task\"].cuda()\n",
    "                targets[\"bv_task\"] = targets[\"bv_task\"].cuda()\n",
    "                targets[\"fg_task\"] = targets[\"fg_task\"].cuda()\n",
    "            \n",
    "            preds = self.model(batch_graph)\n",
    "            emb_vector = preds['emb_vec']\n",
    "\n",
    "            # add this for motif generation\n",
    "            if self.args.embedding_output_type == 'atom':\n",
    "                emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "                emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "                \n",
    "                node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = self.motif_model(moltree, emb_afa_grouped)\n",
    "                node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = self.motif_model(moltree, emb_afb_grouped)\n",
    "                \n",
    "                node_loss = node_afa_loss + node_afb_loss\n",
    "                topo_loss = topo_afa_loss + topo_afb_loss\n",
    "                node_acc = (node_afa_acc + node_afb_acc)/2\n",
    "                topo_acc = (topo_afa_acc + topo_afb_acc)/2\n",
    "                \n",
    "            elif self.args.embedding_output_type == 'bond':\n",
    "                emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "                emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "                \n",
    "                node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = self.motif_model(moltree, emb_bfa_grouped)\n",
    "                node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = self.motif_model(moltree, emb_bfb_grouped)\n",
    "                \n",
    "                node_loss = node_bfa_loss + node_bfb_loss\n",
    "                topo_loss = topo_bfa_loss + topo_bfb_loss\n",
    "                node_acc = (node_bfa_acc + node_bfb_acc)/2\n",
    "                topo_acc = (topo_bfa_acc + topo_bfb_acc)/2\n",
    "                \n",
    "            elif self.args.embedding_output_type == \"both\":\n",
    "                emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "                emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "                emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "                emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "                \n",
    "                node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = self.motif_model(moltree, emb_afa_grouped)\n",
    "                node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = self.motif_model(moltree, emb_afb_grouped)\n",
    "                node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = self.motif_model(moltree, emb_bfa_grouped)\n",
    "                node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = self.motif_model(moltree, emb_bfb_grouped)\n",
    "                \n",
    "                node_loss = node_afa_loss + node_afb_loss + node_bfa_loss + node_bfb_loss\n",
    "                topo_loss = topo_afa_loss + topo_afb_loss + topo_bfa_loss + topo_bfb_loss\n",
    "                node_acc = (node_afa_acc + node_afb_acc + node_bfa_acc + node_bfb_acc)/4\n",
    "                topo_acc = (topo_afa_acc + topo_afb_acc + topo_bfa_acc + topo_bfb_acc)/4\n",
    "\n",
    "            # # ad-hoc code, for visualizing a model, comment this block when it is not needed\n",
    "            # import dglt.contrib.grover.vis_model as vis_model\n",
    "            # for task in ['av_task', 'bv_task', 'fg_task']:\n",
    "            #     vis_graph = vis_model.make_dot(self.model(batch_graph)[task],\n",
    "            #                                    params=dict(self.model.named_parameters()))\n",
    "            #     # vis_graph.view()\n",
    "            #     vis_graph.render(f\"{self.args.backbone}_model_{task}_vis.png\", format=\"png\")\n",
    "            # exit()\n",
    "\n",
    "            loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss = self.loss_func(preds, targets)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            iter_count += self.args.batch_size\n",
    "            \n",
    "            # add for topology loss\n",
    "            loss += topo_loss\n",
    "            loss += node_loss\n",
    "            topo_loss_sum += topo_loss.item()\n",
    "            node_loss_sum += node_loss.item()\n",
    "\n",
    "            if train:\n",
    "                cum_loss_sum += loss.item()\n",
    "                # Run model\n",
    "                self.model.zero_grad()\n",
    "                self.motif_model.zero_grad()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.motif_optimizer.step()\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                # For eval model, only consider the loss of three task.\n",
    "                cum_loss_sum += av_loss.item()\n",
    "                cum_loss_sum += bv_loss.item()\n",
    "                cum_loss_sum += fg_loss.item()\n",
    "\n",
    "            av_loss_sum += av_loss.item()\n",
    "            bv_loss_sum += bv_loss.item()\n",
    "            fg_loss_sum += fg_loss.item()\n",
    "            av_dist_loss_sum += av_dist_loss.item() if type(av_dist_loss) != float else av_dist_loss\n",
    "            bv_dist_loss_sum += bv_dist_loss.item() if type(bv_dist_loss) != float else bv_dist_loss\n",
    "            fg_dist_loss_sum += fg_dist_loss.item() if type(fg_dist_loss) != float else fg_dist_loss\n",
    "\n",
    "            cum_iter_count += 1\n",
    "            self.n_iter += self.args.batch_size\n",
    "\n",
    "            # Debug only.\n",
    "            # if i % 50 == 0:\n",
    "            #     print(f\"epoch: {epoch}, batch_id: {i}, av_loss: {av_loss}, bv_loss: {bv_loss}, \"\n",
    "            #           f\"fg_loss: {fg_loss}, av_dist_loss: {av_dist_loss}, bv_dist_loss: {bv_dist_loss}, \"\n",
    "            #           f\"fg_dist_loss: {fg_dist_loss}\")\n",
    "\n",
    "        cum_loss_sum /= cum_iter_count\n",
    "        av_loss_sum /= cum_iter_count\n",
    "        bv_loss_sum /= cum_iter_count\n",
    "        fg_loss_sum /= cum_iter_count\n",
    "        av_dist_loss_sum /= cum_iter_count\n",
    "        bv_dist_loss_sum /= cum_iter_count\n",
    "        fg_dist_loss_sum /= cum_iter_count\n",
    "        \n",
    "        topo_loss_sum /= cum_iter_count\n",
    "        node_loss_sum /= cum_iter_count\n",
    "\n",
    "        return self.n_iter, cum_loss_sum, (av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum,\n",
    "                                           bv_dist_loss_sum, fg_dist_loss_sum, topo_loss_sum, node_loss_sum, topo_acc, node_acc)\n",
    "\n",
    "    def save(self, epoch, file_path, name=None) -> str:\n",
    "        \"\"\"\n",
    "        Save the intermediate models during training.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to save the model.\n",
    "        :return: the output path.\n",
    "        \"\"\"\n",
    "        # add specific time in model fine name, in order to distinguish different saved models\n",
    "        now = time.localtime()\n",
    "        if name is None:\n",
    "            name = \"_%04d_%02d_%02d_%02d_%02d_%02d\" % (\n",
    "                now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "        output_path = file_path + name + \".ep%d\" % epoch\n",
    "        scaler = None\n",
    "        features_scaler = None\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch,\n",
    "            'data_scaler': {\n",
    "                'means': scaler.means,\n",
    "                'stds': scaler.stds\n",
    "            } if scaler is not None else None,\n",
    "            'features_scaler': {\n",
    "                'means': features_scaler.means,\n",
    "                'stds': features_scaler.stds\n",
    "            } if features_scaler is not None else None\n",
    "        }\n",
    "        torch.save(state, output_path)\n",
    "\n",
    "        # Is this necessary?\n",
    "        # if self.with_cuda:\n",
    "        #    self.model = self.model.cuda()\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "\n",
    "    def save_tmp(self, epoch, file_path, rank=0):\n",
    "        \"\"\"\n",
    "        Save the models for auto-restore during training.\n",
    "        The model are stored in file_path/tmp folder and will replaced on each epoch.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        store_path = os.path.join(file_path, \"tmp\")\n",
    "        if not os.path.exists(store_path):\n",
    "            os.makedirs(store_path, exist_ok=True)\n",
    "        store_path = os.path.join(store_path, \"model.%d\" % rank)\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        torch.save(state, store_path)\n",
    "\n",
    "    def restore(self, file_path, rank=0) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Restore the training state saved by save_tmp.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return: the restored epoch number and the scheduler_step in scheduler.\n",
    "        \"\"\"\n",
    "        cpt_path = os.path.join(file_path, \"tmp\", \"model.%d\" % rank)\n",
    "        if not os.path.exists(cpt_path):\n",
    "            print(\"No checkpoint found %d\")\n",
    "            return 0, 0\n",
    "        cpt = torch.load(cpt_path)\n",
    "        self.model.load_state_dict(cpt[\"state_dict\"])\n",
    "        self.optimizer.load_state_dict(cpt[\"optimizer\"])\n",
    "        epoch = cpt[\"epoch\"]\n",
    "        scheduler_step = cpt[\"scheduler_step\"]\n",
    "        self.scheduler.current_step = scheduler_step\n",
    "        print(\"Restore checkpoint, current epoch: %d\" % (epoch))\n",
    "        return epoch, scheduler_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305dee55-3b44-454a-9f47-d2dc5363772b",
   "metadata": {},
   "source": [
    "# 실험장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d3ac4bd-2bb8-4658-9777-6cfc0b09f087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Splitting data with seed 0.\n",
      "Total size = 293 | train size = 200 | val size = 93\n",
      "atom vocab size: 148, bond vocab size: 193, Number of FG tasks: 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation='PReLU', atom_vocab_path='data/mgssl/mgssl_atom_vocab.pkl', backbone='gtrans', batch_size=100, bias=False, bond_drop_rate=0, bond_vocab_path='data/mgssl/mgssl_bond_vocab.pkl', cuda=True, data_path='data/mgssl', dense=False, depth=3, dist_coff=0.1, dropout=0.1, embedding_output_type='both', enable_multi_gpu=False, epochs=20, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=1200, init_lr=0.0002, max_lr=0.0004, motif_hidden_size=1200, motif_latent_size=56, motif_order='dfs', motif_vocab_path='data/mgssl/clique.txt', no_cache=True, num_attn_head=4, num_mt_block=1, parser_name='pretrain', save_dir='model/mgssl', save_interval=5, topology=True, undirected=False, wandb=False, wandb_name='pretrain', warmup_epochs=2.0, weight_decay=1e-07)\n",
      "Loading data:\n",
      "Number of files: 3\n",
      "Number of samples: 293\n",
      "Samples/file: 100\n"
     ]
    }
   ],
   "source": [
    "if logger is not None:\n",
    "    debug, _ = logger.debug, logger.info\n",
    "else:\n",
    "    debug = print\n",
    "\n",
    "# initialize the horovod library\n",
    "if args.enable_multi_gpu:\n",
    "    mgw.init()\n",
    "\n",
    "# binding training to GPUs.\n",
    "master_worker = (mgw.rank() == 0) if args.enable_multi_gpu else True\n",
    "# pin GPU to local rank. By default, we use gpu:0 for training.\n",
    "local_gpu_idx = mgw.local_rank() if args.enable_multi_gpu else 0\n",
    "with_cuda = args.cuda\n",
    "if with_cuda:\n",
    "    torch.cuda.set_device(local_gpu_idx)\n",
    "\n",
    "# get rank an  number of workers\n",
    "rank = mgw.rank() if args.enable_multi_gpu else 0\n",
    "num_replicas = mgw.size() if args.enable_multi_gpu else 1\n",
    "# print(\"Rank: %d Rep: %d\" % (rank, num_replicas))\n",
    "\n",
    "# load file paths of the data.\n",
    "if master_worker:\n",
    "    print(args)\n",
    "    if args.enable_multi_gpu:\n",
    "        debug(\"Total workers: %d\" % (mgw.size()))\n",
    "    debug('Loading data')\n",
    "data, sample_per_file = get_motif_data(data_path=args.data_path)\n",
    "\n",
    "# data splitting\n",
    "if master_worker:\n",
    "    debug(f'Splitting data with seed 0.')\n",
    "train_data, test_data, _ = split_data_motif(data=data, sizes=(0.9, 0.1, 0.0), seed=0, logger=logger)\n",
    "\n",
    "# Here the true train data size is the train_data divided by #GPUs\n",
    "if args.enable_multi_gpu:\n",
    "    args.train_data_size = len(train_data) // mgw.size()\n",
    "else:\n",
    "    args.train_data_size = len(train_data)\n",
    "if master_worker:\n",
    "    debug(f'Total size = {len(data):,} | '\n",
    "          f'train size = {len(train_data):,} | val size = {len(test_data):,}')\n",
    "\n",
    "# load atom and bond vocabulary and the semantic motif labels.\n",
    "atom_vocab = MolVocab.load_vocab(args.atom_vocab_path)\n",
    "bond_vocab = MolVocab.load_vocab(args.bond_vocab_path)\n",
    "atom_vocab_size, bond_vocab_size = len(atom_vocab), len(bond_vocab)\n",
    "\n",
    "# Load motif vocabulary for pretrain\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "motif_vocab = [x.strip(\"\\r\\n \") for x in open(args.motif_vocab_path)]\n",
    "motif_vocab = Motif_Vocab(motif_vocab)\n",
    "\n",
    "# Hard coding here, since we haven't load any data yet!\n",
    "fg_size = 85\n",
    "shared_dict = {}\n",
    "motif_collator = GroverMotifCollator(shared_dict=shared_dict, atom_vocab=atom_vocab, bond_vocab=bond_vocab, args=args)\n",
    "if master_worker:\n",
    "    debug(\"atom vocab size: %d, bond vocab size: %d, Number of FG tasks: %d\" % (atom_vocab_size,\n",
    "                                                                                bond_vocab_size, fg_size))\n",
    "\n",
    "# Define the distributed sampler. If using the single card, the sampler will be None.\n",
    "train_sampler = None\n",
    "test_sampler = None\n",
    "shuffle = True\n",
    "if args.enable_multi_gpu:\n",
    "    # If not shuffle, the performance may decayed.\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=True, sample_per_file=sample_per_file)\n",
    "    # Here sample_per_file in test_sampler is None, indicating the test sampler would not divide the test samples by\n",
    "    # rank. (TODO: bad design here.)\n",
    "    test_sampler = DistributedSampler(\n",
    "        test_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=False)\n",
    "    train_sampler.set_epoch(args.epochs)\n",
    "    test_sampler.set_epoch(1)\n",
    "    # if we enables multi_gpu training. shuffle should be disabled.\n",
    "    shuffle = False\n",
    "    \n",
    "    # Pre load data. (Maybe unnecessary. )\n",
    "pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
    "pre_load_data(test_data, rank, num_replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d33eb89-aa0b-4831-8e9a-089910a75798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found %d\n"
     ]
    }
   ],
   "source": [
    "# Build dataloader\n",
    "train_data_dl = DataLoader(train_data,\n",
    "                           batch_size=150,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=10,\n",
    "                           sampler=train_sampler,\n",
    "                           collate_fn=motif_collator)\n",
    "test_data_dl = DataLoader(test_data,\n",
    "                          batch_size=150,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=10,\n",
    "                          sampler=test_sampler,\n",
    "                          collate_fn=motif_collator)\n",
    "\n",
    "# Build the embedding model.\n",
    "grover_model = GROVEREmbedding(args)\n",
    "\n",
    "# build the topology predict model.\n",
    "motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order)\n",
    "\n",
    "#  Build the trainer.\n",
    "trainer = GROVERMotifTrainer(args=args,\n",
    "                        embedding_model=grover_model,\n",
    "                        topology_model=motif_model,\n",
    "                        atom_vocab_size=atom_vocab_size,\n",
    "                        bond_vocab_size=bond_vocab_size,\n",
    "                        fg_size=fg_size,\n",
    "                        train_dataloader=train_data_dl,\n",
    "                        test_dataloader=test_data_dl,\n",
    "                        optimizer_builder=build_optimizer,\n",
    "                        scheduler_builder=build_lr_scheduler,\n",
    "                        logger=logger,\n",
    "                        with_cuda=with_cuda,\n",
    "                        enable_multi_gpu=args.enable_multi_gpu)\n",
    "\n",
    "# Restore the interrupted training.\n",
    "model_dir = os.path.join(args.save_dir, \"model\")\n",
    "resume_from_epoch = 0\n",
    "resume_scheduler_step = 0\n",
    "if master_worker:\n",
    "    resume_from_epoch, resume_scheduler_step = trainer.restore(model_dir)\n",
    "if args.enable_multi_gpu:\n",
    "    resume_from_epoch = mgw.broadcast(torch.tensor(resume_from_epoch), root_rank=0, name=\"resume_from_epoch\").item()\n",
    "    resume_scheduler_step = mgw.broadcast(torch.tensor(resume_scheduler_step),\n",
    "                                          root_rank=0, name=\"resume_scheduler_step\").item()\n",
    "    trainer.scheduler.current_step = resume_scheduler_step\n",
    "    print(\"Restored epoch: %d Restored scheduler step: %d\" % (resume_from_epoch, trainer.scheduler.current_step))\n",
    "trainer.broadcast_parameters()\n",
    "\n",
    "# Print model details.\n",
    "#if master_worker:\n",
    "    # Change order here.\n",
    "    #print(grover_model)\n",
    "    #print(\"Total parameters: %d\" % param_count(trainer.grover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b35654c2-af2a-467e-a17b-78ee06e21976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROVEREmbedding(\n",
      "  (encoders): GTransEncoder(\n",
      "    (edge_blocks): ModuleList(\n",
      "      (0): MTBlock(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (W_i): Linear(in_features=165, out_features=1200, bias=False)\n",
      "        (attn): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            (1): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            (2): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (W_o): Linear(in_features=4800, out_features=1200, bias=False)\n",
      "        (sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (node_blocks): ModuleList(\n",
      "      (0): MTBlock(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (W_i): Linear(in_features=151, out_features=1200, bias=False)\n",
      "        (attn): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            (1): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            (2): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (W_o): Linear(in_features=4800, out_features=1200, bias=False)\n",
      "        (sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ffn_atom_from_atom): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=1351, out_features=4800, bias=True)\n",
      "      (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_atom_from_bond): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=1351, out_features=4800, bias=True)\n",
      "      (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_bond_from_atom): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=1365, out_features=4800, bias=True)\n",
      "      (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_bond_from_bond): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=1365, out_features=4800, bias=True)\n",
      "      (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (atom_from_atom_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (atom_from_bond_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (bond_from_atom_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (bond_from_bond_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (act_func_node): PReLU(num_parameters=1)\n",
      "    (act_func_edge): PReLU(num_parameters=1)\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(grover_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "473027ff-0cf5-4a47-bdc6-6160bea222ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = torch.load('grover_large.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b0c7a80-f1eb-42a5-af0f-1ee96911c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.utils import build_optimizer, build_lr_scheduler, makedirs, load_checkpoint, get_loss_func, \\\n",
    "    save_checkpoint, build_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d321235-3f3b-45a6-bade-1c7870a0ca37",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'features_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-fb20d994115d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grover_large.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/grover/grover/util/utils.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(path, current_args, cuda, logger)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[0;31m# Build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m     \u001b[0mmodel_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/util/utils.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(args, model_idx)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;31m# finetune and evaluation case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroverFinetuneTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m     \u001b[0minitialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/model/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReadout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmol_atom_from_atom_ffn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_ffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmol_atom_from_bond_ffn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_ffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m#self.ffn = nn.ModuleList()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/model/models.py\u001b[0m in \u001b[0;36mcreate_ffn\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \"\"\"\n\u001b[1;32m    399\u001b[0m         \u001b[0;31m# Note: args.features_dim is set according the real loaded features data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mfirst_linear_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'features_only'"
     ]
    }
   ],
   "source": [
    "model2 = load_checkpoint('grover_large.pt', args, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c72981e-be5f-47eb-ad47-978736138032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9634582996368408\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "for _, item in enumerate(train_data_dl):\n",
    "    batch_graph = item[\"graph_input\"]\n",
    "    targets = item[\"targets\"]\n",
    "\n",
    "    # add this for motif generation\n",
    "    moltree_paths = item[\"moltree\"]\n",
    "dtime = time.time()\n",
    "print(dtime-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ce35ec2-f3d4-4a39-a8fa-a79860688a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.utils import load_moltree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7daf2bc-7994-4302-9152-a83d40c52ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fac0f81b7a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fac0fa298c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.405169248580933\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "for _, item in enumerate(train_data_dl):\n",
    "    batch_graph = item[\"graph_input\"]\n",
    "    targets = item[\"targets\"]\n",
    "\n",
    "    # add this for motif generation\n",
    "    moltree_paths = item[\"moltree\"]\n",
    "    \n",
    "moltree = []\n",
    "for _, item in enumerate(moltree_paths):\n",
    "    moltree.append(load_moltree(item[0],item[1]))\n",
    "dtime = time.time()\n",
    "print(dtime-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "800e195e-1d2d-4e77-965d-2caecd3a155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<grover.topology.mol_tree.MolTree at 0x7fabf85fa410>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfe0b7fd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf9509ed0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfdfe3ed0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfe022250>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfe580c90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfdedd210>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf93a83d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf8737290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf8706e50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfe1d19d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfde01550>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf87ae290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01256f10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01094150>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac00d7b750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01112f10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac012ea750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01a511d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac012bd4d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf884b790>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01e9f310>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac00e62b10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfe13a290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf87d8510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf886e190>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf879c310>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01b23b50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac011d6950>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfdfb3910>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac00c0c190>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf9511750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfdef2510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf93609d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabfe270fd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01f907d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac00df6c10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac011f7c10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf86912d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf936c6d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac010db690>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf65c71d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac00f1f250>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01189f90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf9c863d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac0117ab90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf8817a90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac01ea7350>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fabf860aa50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7fac0102f590>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moltree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da42b855-a935-4d88-abf1-92b7295a30c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mppn0303\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/grover/wandb/run-20230311_140851-riselb0y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ppn0303/load_after_loader/runs/riselb0y' target=\"_blank\">vital-eon-1</a></strong> to <a href='https://wandb.ai/ppn0303/load_after_loader' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ppn0303/load_after_loader' target=\"_blank\">https://wandb.ai/ppn0303/load_after_loader</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ppn0303/load_after_loader/runs/riselb0y' target=\"_blank\">https://wandb.ai/ppn0303/load_after_loader/runs/riselb0y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #wandb\n",
    "wandb.init(project='load_after_loader')\n",
    "wandb.config = args\n",
    "wandb.watch(grover_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a42898f-67db-472a-bf9f-d93751f4bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_moltree(path: str, index: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loads features saved in a variety of formats.\n",
    "\n",
    "    Supported formats:\n",
    "    - .npz compressed (assumes features are saved with name \"features\")\n",
    "\n",
    "    All formats assume that the SMILES strings loaded elsewhere in the code are in the same\n",
    "    order as the features loaded here.\n",
    "\n",
    "    :param path: Path to a file containing features.\n",
    "    :return: A 2D numpy array of size (num_molecules, features_size) containing the features.\n",
    "    \"\"\"\n",
    "    extension = os.path.splitext(path)[1]\n",
    "\n",
    "    with open(path, 'rb') as f:\n",
    "        moltrees = pickle.load(f)[index]\n",
    "    f.close()\n",
    "    return moltrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad8460ae-0383-4f7f-a710-4d939cfdfdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.592586278915405\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "moltree = []\n",
    "for _, item in enumerate(moltree_paths):\n",
    "    moltree.append(load_moltrees(item[0],item[1]))\n",
    "dtime = time.time()\n",
    "print(dtime-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b22cf10-e7f7-4853-9bd6-0bf8d9ff535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.522074699401855\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "moltree = list(0 for i in range(len(moltree_paths)))\n",
    "for _, item in enumerate(moltree_paths):\n",
    "    moltree.append(load_moltrees(item[0],item[1]))\n",
    "dtime = time.time()\n",
    "print(dtime-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "982c7136-378f-4047-b80b-e3a9b5c4cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df278864-93af-4c46-93b5-0b54dc07a71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before train : 5779.71 MiB\n",
      "epochs start memory is 5779.71 MiB\n"
     ]
    }
   ],
   "source": [
    "# Perform training.\n",
    "best_val_loss = 0\n",
    "best_val_epoch = 0\n",
    "best_model_dir = os.path.join(args.save_dir, \"model_best\")\n",
    "print(f\"before train : {memory_usage()[0]:.2f} MiB\")\n",
    "for epoch in range(resume_from_epoch + 1, args.epochs):\n",
    "    s_time = time.time()\n",
    "    print(f\"epochs start memory is {memory_usage()[0]:.2f} MiB\")\n",
    "    # Data pre-loading.\n",
    "    if args.enable_multi_gpu:\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        train_data.clean_cache()\n",
    "        idxs = train_sampler.get_indices()\n",
    "        for local_gpu_idx in idxs:\n",
    "            train_data.load_data(local_gpu_idx)\n",
    "    \n",
    "    \n",
    "\n",
    "    d_time = time.time() - s_time\n",
    "    \n",
    "\n",
    "    # perform training and validation.\n",
    "    s_time = time.time()\n",
    "    _, train_loss, _ = trainer.train(epoch)\n",
    "    print(f\"after train memory is {memory_usage()[0]:.2f} MiB\")\n",
    "    t_time = time.time() - s_time\n",
    "    s_time = time.time()\n",
    "    _, val_loss, detailed_loss_val = trainer.test(epoch)\n",
    "    print(f\"after validation memory is {memory_usage()[0]:.2f} MiB\")\n",
    "    val_av_loss, val_bv_loss, val_fg_loss, _, _, _, val_topo_loss, val_node_loss, topo_acc, node_acc = detailed_loss_val\n",
    "    v_time = time.time() - s_time\n",
    "\n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_epoch = epoch\n",
    "        trainer.save(epoch, best_model_dir)\n",
    "\n",
    "    if args.wandb :         \n",
    "        wandb.log({\"train_loss\" : train_loss, \"val_loss\" : val_loss, \"topo_loss\" : val_topo_loss})\n",
    "\n",
    "    # print information.\n",
    "    if master_worker:\n",
    "        print('Epoch: {:04d}'.format(epoch),\n",
    "              'loss_train: {:.6f}'.format(train_loss),\n",
    "              'loss_val: {:.6f}'.format(val_loss),\n",
    "              'loss_val_av: {:.6f}'.format(val_av_loss),\n",
    "              'loss_val_bv: {:.6f}'.format(val_bv_loss),\n",
    "              'loss_val_fg: {:.6f}'.format(val_fg_loss),\n",
    "              'loss_val_topo: {:.6f}'.format(val_topo_loss),\n",
    "              'loss_val_node: {:.6f}'.format(val_node_loss),\n",
    "              'acc_topo: {:.6f}'.format(topo_acc),\n",
    "              'acc_node: {:.6f}'.format(node_acc),\n",
    "              'cur_lr: {:.5f}'.format(trainer.scheduler.get_lr()[0]),\n",
    "              't_time: {:.4f}s'.format(t_time),\n",
    "              'v_time: {:.4f}s'.format(v_time),\n",
    "              'd_time: {:.4f}s'.format(d_time), flush=True)\n",
    "\n",
    "\n",
    "        if epoch % args.save_interval == 0:\n",
    "            trainer.save(epoch, model_dir)\n",
    "\n",
    "\n",
    "        trainer.save_tmp(epoch, model_dir, rank)\n",
    "        print(f\"after save cp memory is {memory_usage()[0]:.2f} MiB\")\n",
    "\n",
    "# Only save final version.\n",
    "if master_worker:\n",
    "    trainer.save(args.epochs, model_dir, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e01f463-950a-4cce-b41e-500c38b0ae82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<grover.topology.mol_tree.MolTree at 0x7f7701cd5490>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_moltrees(item[0],item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4402f9c4-b735-4d85-bd2b-a341a456945f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom_from_atom': tensor([[ 0.5286,  0.1908,  0.1559,  ...,  0.0000,  0.2753, -1.2593],\n",
       "         [ 0.3252, -0.7777,  0.7843,  ..., -0.4200, -0.5239,  0.0000],\n",
       "         [ 0.8270, -0.7058,  1.4171,  ...,  0.0000, -0.8003, -1.1189],\n",
       "         ...,\n",
       "         [-0.0897, -0.7822,  0.4744,  ...,  0.5085,  0.0036, -0.2264],\n",
       "         [ 0.0317, -0.1056,  0.0000,  ...,  0.8393, -0.3412, -1.7130],\n",
       "         [ 0.1424,  0.0848,  0.7505,  ..., -0.0120, -0.7686, -0.9019]],\n",
       "        device='cuda:0', grad_fn=<FusedDropoutBackward>),\n",
       " 'bond_from_atom': tensor([[-0.8544,  1.2712,  1.2164,  ...,  0.0085,  1.2820,  0.0000],\n",
       "         [-0.3456,  0.9824,  0.7136,  ..., -0.1406,  1.1398,  0.0440],\n",
       "         [-1.0875,  0.2222,  0.0810,  ...,  0.0000,  0.1001, -0.5310],\n",
       "         ...,\n",
       "         [-0.6528,  0.9574,  1.1351,  ...,  0.6902,  1.4957, -2.3826],\n",
       "         [-1.0220,  0.9658,  0.4182,  ..., -0.7550,  0.5864, -1.0845],\n",
       "         [-1.2348,  1.4471,  1.8378,  ..., -0.2258,  0.9261, -1.6320]],\n",
       "        device='cuda:0', grad_fn=<FusedDropoutBackward>),\n",
       " 'atom_from_bond': tensor([[-1.7643,  0.6969, -1.0123,  ..., -0.7660, -0.8949,  0.0000],\n",
       "         [ 0.0000,  0.1532, -0.7414,  ..., -0.9163, -0.4130, -0.6147],\n",
       "         [-2.0566, -0.0820,  0.0000,  ..., -1.2234,  0.0000,  0.3236],\n",
       "         ...,\n",
       "         [-1.5876,  0.2523,  0.0000,  ...,  0.0000, -0.1168, -0.9727],\n",
       "         [-1.1891,  0.5603, -0.3692,  ..., -1.2241, -0.7257, -1.2921],\n",
       "         [-1.9694,  0.0000,  0.3959,  ..., -2.3634, -1.0931, -0.4333]],\n",
       "        device='cuda:0', grad_fn=<FusedDropoutBackward>),\n",
       " 'bond_from_bond': tensor([[ 2.5196,  0.2655, -0.6023,  ..., -0.6310,  1.2594,  1.3628],\n",
       "         [ 2.0130,  0.1514,  0.3598,  ..., -0.9702,  1.6945,  1.9319],\n",
       "         [ 1.7697,  0.7841,  0.7211,  ..., -0.8707, -1.1531,  1.8456],\n",
       "         ...,\n",
       "         [ 2.0334,  0.5090,  0.1859,  ..., -1.4931,  1.0023,  2.2143],\n",
       "         [ 1.0938,  1.1248,  0.1433,  ..., -0.6541,  0.6077,  2.0760],\n",
       "         [ 0.8495,  1.2360, -0.3523,  ..., -1.5887,  1.2687,  1.8391]],\n",
       "        device='cuda:0', grad_fn=<FusedDropoutBackward>)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c5fc3f4-1faa-43cc-80c2-4c6df1914134",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8881/407896911.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0memb_bfb_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_node_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoltree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bond_from_bond'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mnode_afa_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_afa_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_afa_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_afa_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotif_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoltree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_afa_grouped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mnode_afb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_afb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_afb_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_afb_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotif_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoltree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_afb_grouped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnode_bfa_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_bfa_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_bfa_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_bfa_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotif_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoltree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_bfa_grouped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "loss_sum, iter_count = 0, 0\n",
    "cum_loss_sum, cum_iter_count = 0, 0\n",
    "av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum, bv_dist_loss_sum, fg_dist_loss_sum, node_loss_sum, topo_loss_sum = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "topo_acc_avg, node_acc_avg = 0, 0\n",
    "# loss_func = self.model.get_loss_func(self.args)\n",
    "\n",
    "for _, item in enumerate(train_data_dl):\n",
    "    batch_graph = item[\"graph_input\"]\n",
    "    targets = item[\"targets\"]\n",
    "\n",
    "    # add this for motif generation\n",
    "    moltree_paths = item[\"moltree\"]\n",
    "\n",
    "    moltree = list()\n",
    "    for _, item in enumerate(moltree_paths):\n",
    "        moltree.append(load_moltrees(item[0],item[1]))\n",
    "\n",
    "\n",
    "    targets[\"av_task\"] = targets[\"av_task\"].cuda()\n",
    "    targets[\"bv_task\"] = targets[\"bv_task\"].cuda()\n",
    "    targets[\"fg_task\"] = targets[\"fg_task\"].cuda()\n",
    "\n",
    "    preds = grover_model(batch_graph)\n",
    "    emb_vector = preds#['emb_vec']\n",
    "\n",
    "    emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "    emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "    emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "    emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "\n",
    "    node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = motif_model(moltree, emb_afa_grouped)\n",
    "    node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = motif_model(moltree, emb_afb_grouped)\n",
    "    node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = motif_model(moltree, emb_bfa_grouped)\n",
    "    node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = motif_model(moltree, emb_bfb_grouped)\n",
    "\n",
    "    node_loss = node_afa_loss + node_afb_loss + node_bfa_loss + node_bfb_loss\n",
    "    topo_loss = topo_afa_loss + topo_afb_loss + topo_bfa_loss + topo_bfb_loss\n",
    "    node_acc = (node_afa_acc + node_afb_acc + node_bfa_acc + node_bfb_acc)/4\n",
    "    topo_acc = (topo_afa_acc + topo_afb_acc + topo_bfa_acc + topo_bfb_acc)/4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbb5c2-a79d-4e4a-8e1e-e99827386920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_model(args: Namespace, logger: Logger = None):\n",
    "    \"\"\"\n",
    "    The entrey of pretrain.\n",
    "    :param args: the argument.\n",
    "    :param logger: the logger.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # avoid auto optimized import by pycharm.\n",
    "    a = MolVocab\n",
    "    s_time = time.time()\n",
    "    if args.topology : \n",
    "        run_motif_training(args=args, logger=logger)\n",
    "    else : \n",
    "        run_training(args=args, logger=logger)\n",
    "    e_time = time.time()\n",
    "    print(\"Total Time: %.3f\" % (e_time - s_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "835e9bea-506a-41db-9ecf-1aee0bca1813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading data\n",
      "Splitting data with seed 0.\n",
      "Splitting data with seed 0.\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation='PReLU', atom_vocab_path='data/zinc10M/zinc10M_atom_vocab.pkl', backbone='gtrans', batch_size=100, bias=False, bond_drop_rate=0, bond_vocab_path='data/zinc10M/zinc10M_bond_vocab.pkl', cuda=True, data_path='data/zinc10M_0', dense=False, depth=3, dist_coff=0.1, dropout=0.1, embedding_output_type='both', enable_multi_gpu=False, epochs=20, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=1200, init_lr=0.0002, max_lr=0.0004, motif_hidden_size=1200, motif_latent_size=56, motif_order='dfs', motif_vocab_path='data/zinc10M/clique.txt', no_cache=True, num_attn_head=4, num_mt_block=1, parser_name='pretrain', save_dir='model/ChEMBL', save_interval=5, topology=True, train_data_size=450000, undirected=False, wandb=False, wandb_name='pretrain', warmup_epochs=2.0, weight_decay=1e-07)\n",
      "Loading data:\n",
      "Number of files: 501\n",
      "Number of samples: 500000\n",
      "Samples/file: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n",
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f71212227a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f712146d320>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/weakref.py\", line 109, in remove\n",
      "    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4023/3879309671.py\", line 3, in <module>\n",
      "    pretrain_model(args, logger)\n",
      "  File \"/root/grover/task/pretrain.py\", line 37, in pretrain_model\n",
      "    run_motif_training(args=args, logger=logger)\n",
      "  File \"/root/grover/task/pretrain.py\", line 339, in run_motif_training\n",
      "    pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
      "  File \"/root/grover/task/pretrain.py\", line 60, in pre_load_data\n",
      "    dataset.load_data(i)\n",
      "  File \"/root/grover/grover/data/groverdataset.py\", line 163, in load_data\n",
      "    def __init__(self, data: List[BatchDatapoint],\n",
      "  File \"/root/grover/grover/data/groverdataset.py\", line 299, in load_datapoints\n",
      "    moltrees = self.load_moltree()\n",
      "  File \"/root/grover/grover/data/groverdataset.py\", line 319, in load_moltree\n",
      "    return feautils.load_moltrees(self.moltree_file)\n",
      "  File \"/root/grover/grover/util/utils.py\", line 92, in load_moltrees\n",
      "    moltrees = pickle.load(f)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1464, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 735, in getmodule\n",
      "    if f == _filesbymodname.get(modname, None):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4023/3879309671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pretrain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpretrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/grover/task/pretrain.py\u001b[0m in \u001b[0;36mpretrain_model\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mrun_motif_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/task/pretrain.py\u001b[0m in \u001b[0;36mrun_motif_training\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;31m# Pre load data. (Maybe unnecessary. )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0mpre_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_replicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_per_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0mpre_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_replicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/task/pretrain.py\u001b[0m in \u001b[0;36mpre_load_data\u001b[0;34m(dataset, rank, num_replicas, sample_per_file, epoch)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpre_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/data/groverdataset.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBatchMolDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     def __init__(self, data: List[BatchDatapoint],\n\u001b[0m\u001b[1;32m    164\u001b[0m                  graph_per_file=None):\n",
      "\u001b[0;32m~/grover/grover/data/groverdataset.py\u001b[0m in \u001b[0;36mload_datapoints\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mmoltrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_moltree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/data/groverdataset.py\u001b[0m in \u001b[0;36mload_moltree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_moltree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeautils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_moltrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoltree_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/util/utils.py\u001b[0m in \u001b[0;36mload_moltrees\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mmoltrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2076\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2080\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "if args.parser_name == 'pretrain':\n",
    "    logger = create_logger(name='pretrain', save_dir=args.save_dir)\n",
    "    pretrain_model(args, logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf0120-5401-4349-a194-9e9782bba7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
