{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e9adfc-25e1-4b9d-93c9-6b127369029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6f78fc-4788-40a6-9d9b-6a433f378d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import RDLogger\n",
    "from socket import gethostname\n",
    "\n",
    "from grover.util.parsing import parse_args, get_newest_train_args\n",
    "from grover.util.utils import create_logger\n",
    "from task.cross_validate import cross_validate, randomsearch, gridsearch, make_confusion_matrix\n",
    "from task.fingerprint import generate_fingerprints, generate_embvec\n",
    "from task.predict import make_predictions, write_prediction\n",
    "from task.pretrain import pretrain_model, subset_learning\n",
    "from grover.data.torchvocab import MolVocab\n",
    "\n",
    "from grover.topology.mol_tree import *\n",
    "\n",
    "#add for gridsearch\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d363ab-3a1b-43aa-8fac-d95500a5d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(seed):\n",
    "    # frozen random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4561be-fbe1-4084-bcf6-a55748930529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup random seed\n",
    "setup(seed=42)\n",
    "# Avoid the pylint warning.\n",
    "a = MolVocab\n",
    "# supress rdkit logger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# Initialize MolVocab\n",
    "mol_vocab = MolVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e4bad-a530-463f-805e-b377a55d7bef",
   "metadata": {},
   "source": [
    "## args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ac4ad1-186e-4bca-b343-4ae75514f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.parsing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dffcbfe-3065-4a13-89e5-7e8fcf6e6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args() -> Namespace:\n",
    "    \"\"\"\n",
    "    Parses arguments for training and testing (includes modifying/validating arguments).\n",
    "\n",
    "    :return: A Namespace containing the parsed, modified, and validated args.\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "                                      dest=\"parser_name\",\n",
    "                                      help=\"Subcommands for fintune, prediction, and fingerprint.\")\n",
    "    parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "    add_pretrain_args(parser_pretrain)\n",
    "\n",
    "    args = parser.parse_args(['pretrain','--data_path','data/zinc10M_0','--save_dir','model/zinc10M_0','--atom_vocab_path','data/zinc10M/zinc10M_atom_vocab.pkl','--bond_vocab_path','data/zinc10M/zinc10M_bond_vocab.pkl',\n",
    "                          '--batch_size','100','--dropout','0.1','--depth','3','--num_attn_head','4','--hidden_size','1200','--epochs','20','--activation','PReLU','--backbone','gtrans','--embedding_output_type','both',\n",
    "                          '--save_interval','5','--init_lr', '0.0002', '--max_lr', '0.0004', '--final_lr', '0.0001', '--weight_decay', '0.0000001', \n",
    "                          '--topology','--motif_vocab_path','data/zinc10M/clique.txt','--motif_hidden_size','1200','--motif_latent_size','56','--motif_order','dfs'])\n",
    "    \n",
    "    if args.parser_name == 'finetune' or args.parser_name == 'eval':\n",
    "        modify_train_args(args)\n",
    "    elif args.parser_name == \"pretrain\":\n",
    "        modify_pretrain_args(args)\n",
    "    elif args.parser_name == 'predict':\n",
    "        modify_predict_args(args)\n",
    "    elif args.parser_name == 'fingerprint':\n",
    "        modify_fingerprint_args(args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19749732-d50a-4ab7-83f5-30784ac4dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args() -> Namespace:\n",
    "    \"\"\"\n",
    "    Parses arguments for training and testing (includes modifying/validating arguments).\n",
    "\n",
    "    :return: A Namespace containing the parsed, modified, and validated args.\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "                                      dest=\"parser_name\",\n",
    "                                      help=\"Subcommands for fintune, prediction, and fingerprint.\")\n",
    "    parser_finetune = subparser.add_parser('finetune', help=\"Fine tune the pre-trained model.\")\n",
    "    add_finetune_args(parser_finetune)\n",
    "    parser_eval = subparser.add_parser('eval', help=\"Evaluate the results of the pre-trained model.\")\n",
    "    add_finetune_args(parser_eval)\n",
    "    parser_predict = subparser.add_parser('predict', help=\"Predict results from fine tuned model.\")\n",
    "    add_predict_args(parser_predict)\n",
    "    parser_fp = subparser.add_parser('fingerprint', help=\"Get the fingerprints of SMILES.\")\n",
    "    add_fingerprint_args(parser_fp)\n",
    "    parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "    add_pretrain_args(parser_pretrain)\n",
    "\n",
    "    args = parser.parse_args(['finetune', '--data_path', 'data/tox21.csv', '--features_path', 'data/tox21.npz', '--save_dir', 'model/test/', '--checkpoint_path', 'grover_large.pt', '--no_features_scaling', '--split_type', 'scaffold_balanced', '--epochs', '2', '--ffn_hidden_size', '1300', '--num_folds', '2',\n",
    "                              '--randomsearch', '--n_iters', '2', '--batch_size', '96','--confusionmatrix'])\n",
    "    \n",
    "    if args.parser_name == 'finetune' or args.parser_name == 'eval':\n",
    "        modify_train_args(args)\n",
    "    elif args.parser_name == \"pretrain\":\n",
    "        modify_pretrain_args(args)\n",
    "    elif args.parser_name == 'predict':\n",
    "        modify_predict_args(args)\n",
    "    elif args.parser_name == 'fingerprint':\n",
    "        modify_fingerprint_args(args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d48325-495c-4af4-bfab-907a86ed4d58",
   "metadata": {},
   "source": [
    "    args = parser.parse_args(['pretrain','--data_path','data/zinc10M_0','--save_dir','model/zinc10M_0','--atom_vocab_path','data/zinc10M/zinc10M_atom_vocab.pkl','--bond_vocab_path','data/zinc10M/zinc10M_bond_vocab.pkl',\n",
    "                              '--batch_size','100','--dropout','0.1','--depth','3','--num_attn_head','4','--hidden_size','1200','--epochs','20','--activation','PReLU','--backbone','gtrans','--embedding_output_type','both',\n",
    "                              '--save_interval','5','--init_lr', '0.0002', '--max_lr', '0.0004', '--final_lr', '0.0001', '--weight_decay', '0.0000001', \n",
    "                              '--topology','--motif_vocab_path','data/zinc10M/clique.txt','--motif_hidden_size','1200','--motif_latent_size','56','--motif_order','dfs',\n",
    "                             '--wandb','--wandb_name', 'jupyter_zinc10M'])\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd6b21-f102-48b0-99af-88d427dec63b",
   "metadata": {},
   "source": [
    "    args = parser.parse_args(['pretrain','--data_path','data/mgssl','--save_dir','model/mgssl','--atom_vocab_path','data/mgssl/mgssl_atom_vocab.pkl','--bond_vocab_path','data/mgssl/mgssl_bond_vocab.pkl',\n",
    "                              '--batch_size','100','--dropout','0.1','--depth','3','--num_attn_head','4','--hidden_size','1200','--epochs','20','--activation','PReLU','--backbone','gtrans','--embedding_output_type','both',\n",
    "                              '--save_interval','5','--init_lr', '0.0002', '--max_lr', '0.0004', '--final_lr', '0.0001', '--weight_decay', '0.0000001', \n",
    "                              '--topology','--motif_vocab_path','data/mgssl/clique.txt','--motif_hidden_size','1200','--motif_latent_size','56','--motif_order','dfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c668498-919f-47ae-8272-d99984401c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(activation='ReLU', attn_hidden=128, attn_out=4, batch_size=96, bond_drop_rate=0, checkpoint_dir=None, checkpoint_path='grover_large.pt', checkpoint_paths=['grover_large.pt'], confusionmatrix=True, crossval_index_dir=None, crossval_index_file=None, cuda=True, data_path='data/tox21.csv', dataset_type='classification', dist_coff=0.1, distinct_init=False, dropout=0.0, early_stop_epoch=1000, embedding_output_type='atom', enbl_multi_gpu=False, ensemble_size=1, epochs=2, features_generator=None, features_only=False, features_path=['data/tox21.npz'], features_scaling=False, ffn_hidden_size=1300, ffn_last_size=None, ffn_mid_size=None, ffn_num_layers=2, final_lr=0.0001, fine_tune_coff=1, fingerprint=False, folds_file=None, gpu=0, gridsearch=False, init_lr=0.0001, max_data_size=None, max_lr=0.001, metric='auc', minimize_score=False, multi_class=False, multi_class_num=3, n_iters=2, no_cache=True, num_folds=2, num_lrs=1, parser_name='finetune', randomsearch=True, save_dir='model/test/', save_smiles_splits=False, seed=0, select_by_loss=False, self_attention=False, separate_test_features_path=None, separate_test_path=None, separate_val_features_path=None, separate_val_path=None, show_individual_scores=False, smote=False, smote_rate=1, split_sizes=[0.8, 0.1, 0.1], split_type='scaffold_balanced', tensorboard=False, test_fold_index=None, use_compound_names=False, use_input_features=['data/tox21.npz'], val_fold_index=None, wandb=False, wandb_name='finetune', warmup_epochs=2.0, weight_decay=0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parse_args()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aea4093-1a23-4446-a898-496a76f4d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger(name='train', save_dir=args.save_dir, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6e3642-40be-4be4-9e2c-565871e82fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grovermotiftrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909f31c4-2444-455f-8c0a-5559254ed88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from grover.util.utils import get_task_names\n",
    "from grover.util.utils import makedirs\n",
    "from task.run_evaluation import run_evaluation, run_evaluation_cfm\n",
    "from task.train import run_training\n",
    "\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9888a6-b17f-47ec-aab1-d8d056a46a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = logger.info if logger is not None else print\n",
    "\n",
    "# Initialize relevant variables\n",
    "init_seed = args.seed\n",
    "save_dir = args.save_dir\n",
    "task_names = get_task_names(args.data_path)\n",
    "\n",
    "#randomize parameter list\n",
    "max_lr_list = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]#, 0.0009, 0.001]\n",
    "lr_rate=[2,3,4,5,6,7,8,9,10]\n",
    "dropout_list = [0, 0.05, 0.1, 0.15, 0.2]\n",
    "attn_hidden_list = 128\n",
    "attn_out_list = [4, 8]\n",
    "dist_coff_list = [0.05, 0.1, 0.15]\n",
    "bond_drop_rate_list = [0, 0.2, 0.4, 0.6]\n",
    "ffn_num_layers_list = [2, 3]\n",
    "ffn_num_layers_list = [2, 3, 4, 5]\n",
    "ffn_dense_list = [300, 500, 700, 900, 1100, 1300]\n",
    "smote_rate_list = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# Run training with different random seeds for each fold\n",
    "all_scores = []\n",
    "params = []\n",
    "time_start = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c57210da-aeb4-4083-9e1d-e7f99d109b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0th search parameter : init_lr is 0.0001 \n",
      " final_lr rate is 3.3333333333333335e-05 \n",
      " dropout is 0.0 \n",
      " attn_out is 8 \n",
      " dist_coff is 0.05 \n",
      " bond_drop_rate is 0.6 \n",
      " ffn_num_layers is 5 \n",
      " ffn_hidden_size is 300 \n",
      " batch_size is 96\n"
     ]
    }
   ],
   "source": [
    "iter_num=0\n",
    "np.random.seed()\n",
    "random.seed()\n",
    "args.init_lr = args.max_lr / 10\n",
    "args.max_lr = np.random.choice(max_lr_list, 1)[0]\n",
    "args.final_lr = args.max_lr / np.random.choice(lr_rate, 1)[0]\n",
    "args.dropout = np.random.choice(dropout_list, 1)[0]\n",
    "args.attn_out = np.random.choice(attn_out_list, 1)[0]\n",
    "args.dist_coff = np.random.choice(dist_coff_list, 1)[0]\n",
    "args.bond_drop_rate = np.random.choice(bond_drop_rate_list, 1)[0]\n",
    "args.ffn_num_layers = np.random.choice(ffn_num_layers_list, 1)[0]\n",
    "args.ffn_hidden_size = np.random.choice(ffn_dense_list, 1)[0]\n",
    "if args.smote==True : \n",
    "    args.smote_rate = np.random.choice(smote_rate_list, 1)[0]\n",
    "    params.append(f'\\n{iter_num}th search parameter : init_lr is {args.init_lr} \\n final_lr rate is {args.final_lr} \\n dropout is {args.dropout} \\n attn_out is {args.attn_out} \\n dist_coff is {args.dist_coff} \\n bond_drop_rate is {args.bond_drop_rate} \\n ffn_num_layers is {args.ffn_num_layers} \\n ffn_hidden_size is {args.ffn_hidden_size} \\n batch_size is {args.batch_size} \\n smote_rate is {args.smote_rate}')\n",
    "else : \n",
    "    params.append(f'\\n{iter_num}th search parameter : init_lr is {args.init_lr} \\n final_lr rate is {args.final_lr} \\n dropout is {args.dropout} \\n attn_out is {args.attn_out} \\n dist_coff is {args.dist_coff} \\n bond_drop_rate is {args.bond_drop_rate} \\n ffn_num_layers is {args.ffn_num_layers} \\n ffn_hidden_size is {args.ffn_hidden_size} \\n batch_size is {args.batch_size}')\n",
    "info(params[iter_num])\n",
    "\n",
    "args.seed = init_seed                        # if change this, result will be change\n",
    "iter_dir = os.path.join(save_dir, f'iter_{iter_num}')\n",
    "args.save_dir = iter_dir\n",
    "makedirs(args.save_dir)\n",
    "\n",
    "fold_scores = []\n",
    "if args.confusionmatrix:\n",
    "    scores_AUC = []\n",
    "    scores_ACC = []\n",
    "    scores_REC = []\n",
    "    scores_PREC = []\n",
    "    scores_SPEC = []\n",
    "    scores_F1 = []\n",
    "    scores_BA = []\n",
    "    scores_TP = []\n",
    "    scores_FP = []\n",
    "    scores_TN = []\n",
    "    scores_FN = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd2597b9-4da8-425d-b8ae-98942f6d0ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    }
   ],
   "source": [
    "fold_num=0\n",
    "info(f'Fold {fold_num}')\n",
    "args.seed = init_seed + fold_num\n",
    "args.save_dir = os.path.join(iter_dir, f'fold_{fold_num}')\n",
    "makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2acec8d-9a75-49f4-8da6-92845ebb50a3",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45e70af1-438a-4e4b-9a0a-e5ae9ccb156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from grover.data import MolCollator\n",
    "from grover.data import StandardScaler\n",
    "from grover.util.metrics import get_metric_func\n",
    "from grover.util.nn_utils import initialize_weights, param_count\n",
    "from grover.util.scheduler import NoamLR\n",
    "from grover.util.utils import build_optimizer, build_lr_scheduler, makedirs, load_checkpoint, get_loss_func, \\\n",
    "    save_checkpoint, build_model\n",
    "from grover.util.utils import get_class_sizes, get_data, split_data, get_task_names\n",
    "from task.predict import predict, evaluate, evaluate_predictions, evaluate_predictions_cfm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b0f3314-6309-4744-abd6-42543f0376a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, debug, logger):\n",
    "    \"\"\"\n",
    "    load the training data.\n",
    "    :param args:\n",
    "    :param debug:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Get data\n",
    "    debug('Loading data')\n",
    "    args.task_names = get_task_names(args.data_path)\n",
    "    data = get_data(path=args.data_path, args=args, logger=logger)\n",
    "    if data.data[0].features is not None:\n",
    "        args.features_dim = len(data.data[0].features)\n",
    "    else:\n",
    "        args.features_dim = 0\n",
    "    shared_dict = {}\n",
    "    args.num_tasks = data.num_tasks()\n",
    "    args.features_size = data.features_size()\n",
    "    debug(f'Number of tasks = {args.num_tasks}')\n",
    "    # Split data\n",
    "    debug(f'Splitting data with seed {args.seed}')\n",
    "    if args.separate_test_path:\n",
    "        test_data = get_data(path=args.separate_test_path, args=args,\n",
    "                             features_path=args.separate_test_features_path, logger=logger)\n",
    "    if args.separate_val_path:\n",
    "        val_data = get_data(path=args.separate_val_path, args=args,\n",
    "                            features_path=args.separate_val_features_path, logger=logger)\n",
    "    if args.separate_val_path and args.separate_test_path:\n",
    "        train_data = data\n",
    "    elif args.separate_val_path:\n",
    "        train_data, _, test_data = split_data(data=data, split_type=args.split_type,\n",
    "                                              sizes=(0.8, 0.2, 0.0), seed=args.seed, args=args, logger=logger)\n",
    "    elif args.separate_test_path:\n",
    "        train_data, val_data, _ = split_data(data=data, split_type=args.split_type,\n",
    "                                             sizes=(0.8, 0.2, 0.0), seed=args.seed, args=args, logger=logger)\n",
    "    else:\n",
    "        train_data, val_data, test_data = split_data(data=data, split_type=args.split_type,\n",
    "                                                     sizes=args.split_sizes, seed=args.seed, args=args, logger=logger)\n",
    "    if args.features_scaling:\n",
    "        features_scaler = train_data.normalize_features(replace_nan_token=0)\n",
    "        val_data.normalize_features(features_scaler)\n",
    "        test_data.normalize_features(features_scaler)\n",
    "    else:\n",
    "        features_scaler = None\n",
    "\n",
    "    if args.smote == True:\n",
    "        if args.dataset_type == 'classification':\n",
    "            class_sizes = get_class_sizes(data)\n",
    "            debug('Origin Class sizes')\n",
    "            for i, task_class_sizes in enumerate(class_sizes):\n",
    "                debug(f'{args.task_names[i]} '\n",
    "                      f'{\", \".join(f\"{cls}: {int(size*args.train_data_size)}({size * 100:.2f}%)\" for cls, size in enumerate(task_class_sizes))}')\n",
    "        debug(f'Total size = {len(data):,} | '\n",
    "              f'train size = {args.train_data_size:,} | val size = {len(val_data):,} | test size = {len(test_data):,}')\n",
    "        args.train_data_size = len(train_data)\n",
    "    \n",
    "        debug('Smoted Class sizes')\n",
    "        smoted_class_sizes = get_class_sizes(train_data)\n",
    "        for i, task_class_sizes in enumerate(smoted_class_sizes):\n",
    "            debug(f'{args.task_names[i]} '\n",
    "                  f'{\", \".join(f\"{cls}: {int(size*args.train_data_size)}({size * 100:.2f}%)\" for cls, size in enumerate(task_class_sizes))}')\n",
    "        #note : there is some error of number because class_count is class_rate*data_length\n",
    "        debug(f'Total size = {len(test_data)+len(train_data)+len(val_data):,} | '\n",
    "              f'train size = {args.train_data_size:,} | val size = {len(val_data):,} | test size = {len(test_data):,}')\n",
    "    else:\n",
    "        if args.dataset_type == 'classification':\n",
    "            class_sizes = get_class_sizes(data, args)\n",
    "            debug('Class sizes')\n",
    "            if not args.multi_class:\n",
    "                for i, task_class_sizes in enumerate(class_sizes):\n",
    "                    debug(f'{args.task_names[i]} '\n",
    "                          f'{\", \".join(f\"{cls}: {size * 100:.2f}%\" for cls, size in enumerate(task_class_sizes))}')\n",
    "            elif args.multi_class:\n",
    "                for i in range(args.multi_class_num):\n",
    "                    print(f'{i} : {class_sizes[i][0]:.2f}')\n",
    "                    \n",
    "        args.train_data_size = len(train_data)\n",
    "        debug(f'Total size = {len(data):,} | '\n",
    "              f'train size = {len(train_data):,} | val size = {len(val_data):,} | test size = {len(test_data):,}')\n",
    "\n",
    "    # Initialize scaler and scale training targets by subtracting mean and dividing standard deviation (regression only)\n",
    "    if args.dataset_type == 'regression':\n",
    "        debug('Fitting scaler')\n",
    "        _, train_targets = train_data.smiles(), train_data.targets()\n",
    "        scaler = StandardScaler().fit(train_targets)\n",
    "        scaled_targets = scaler.transform(train_targets).tolist()\n",
    "        train_data.set_targets(scaled_targets)\n",
    "\n",
    "        val_targets = val_data.targets()\n",
    "        scaled_val_targets = scaler.transform(val_targets).tolist()\n",
    "        val_data.set_targets(scaled_val_targets)\n",
    "    else:\n",
    "        scaler = None\n",
    "    return features_scaler, scaler, shared_dict, test_data, train_data, val_data\n",
    "\n",
    "\n",
    "def save_splits(args, test_data, train_data, val_data):\n",
    "    \"\"\"\n",
    "    Save the splits.\n",
    "    :param args:\n",
    "    :param test_data:\n",
    "    :param train_data:\n",
    "    :param val_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(args.data_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "\n",
    "        lines_by_smiles = {}\n",
    "        indices_by_smiles = {}\n",
    "        for i, line in enumerate(reader):\n",
    "            smiles = line[0]\n",
    "            lines_by_smiles[smiles] = line\n",
    "            indices_by_smiles[smiles] = i\n",
    "\n",
    "    all_split_indices = []\n",
    "    for dataset, name in [(train_data, 'train'), (val_data, 'val'), (test_data, 'test')]:\n",
    "        with open(os.path.join(args.save_dir, name + '_smiles.csv'), 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['smiles'])\n",
    "            for smiles in dataset.smiles():\n",
    "                writer.writerow([smiles])\n",
    "        with open(os.path.join(args.save_dir, name + '_full.csv'), 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)\n",
    "            for smiles in dataset.smiles():\n",
    "                writer.writerow(lines_by_smiles[smiles])\n",
    "        split_indices = []\n",
    "        for smiles in dataset.smiles():\n",
    "            split_indices.append(indices_by_smiles[smiles])\n",
    "            split_indices = sorted(split_indices)\n",
    "        all_split_indices.append(split_indices)\n",
    "    with open(os.path.join(args.save_dir, 'split_indices.pckl'), 'wb') as f:\n",
    "        pickle.dump(all_split_indices, f)\n",
    "    return writer\n",
    "def train(epoch, model, data, loss_func, optimizer, scheduler,\n",
    "          shared_dict, args: Namespace, n_iter: int = 0,\n",
    "          logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Trains a model for an epoch.\n",
    "\n",
    "    :param model: Model.\n",
    "    :param data: A MoleculeDataset (or a list of MoleculeDatasets if using moe).\n",
    "    :param loss_func: Loss function.\n",
    "    :param optimizer: An Optimizer.\n",
    "    :param scheduler: A learning rate scheduler.\n",
    "    :param args: Arguments.\n",
    "    :param n_iter: The number of iterations (training examples) trained on so far.\n",
    "    :param logger: A logger for printing intermediate results.\n",
    "    :param writer: A tensorboardX SummaryWriter.\n",
    "    :return: The total number of iterations (training examples) trained on so far.\n",
    "    \"\"\"\n",
    "    # debug = logger.debug if logger is not None else print\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # data.shuffle()\n",
    "\n",
    "    loss_sum, iter_count = 0, 0\n",
    "    cum_loss_sum, cum_iter_count = 0, 0\n",
    "\n",
    "\n",
    "    mol_collator = MolCollator(shared_dict=shared_dict, args=args)\n",
    "\n",
    "    num_workers = 4\n",
    "    if type(data) == DataLoader:\n",
    "        mol_loader = data\n",
    "    else:\n",
    "        mol_loader = DataLoader(data, batch_size=args.batch_size, shuffle=True,\n",
    "                            num_workers=num_workers, collate_fn=mol_collator)\n",
    "\n",
    "    for _, item in enumerate(mol_loader):\n",
    "        _, batch, features_batch, mask, targets = item\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            mask, targets = mask.cuda(), targets.cuda()\n",
    "        class_weights = torch.ones(targets.shape)\n",
    "\n",
    "        if args.cuda:\n",
    "            class_weights = class_weights.cuda()\n",
    "\n",
    "        # Run model\n",
    "        model.zero_grad()\n",
    "        preds = model(batch, features_batch)\n",
    "        loss = loss_func(preds, targets) * class_weights * mask\n",
    "        loss = loss.sum() / mask.sum()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        iter_count += args.batch_size\n",
    "\n",
    "        cum_loss_sum += loss.item()\n",
    "        cum_iter_count += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if isinstance(scheduler, NoamLR):\n",
    "            scheduler.step()\n",
    "\n",
    "        n_iter += args.batch_size\n",
    "\n",
    "        #if (n_iter // args.batch_size) % args.log_frequency == 0:\n",
    "        #    lrs = scheduler.get_lr()\n",
    "        #    loss_avg = loss_sum / iter_count\n",
    "        #    loss_sum, iter_count = 0, 0\n",
    "        #    lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))\n",
    "\n",
    "    return n_iter, cum_loss_sum / cum_iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70eba092-6773-473c-ae8b-21358ae468ca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "100%|##########| 7831/7831 [00:01<00:00, 5433.96it/s]\n",
      "Total scaffolds = 2,326 | train scaffolds = 1,535 | val scaffolds = 367 | test scaffolds = 424\n",
      "/workspace/grover/grover/util/utils.py:714: RuntimeWarning: Mean of empty slice\n",
      "  target_avgs.append(np.nanmean(targets, axis=0))\n",
      "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([0.01736614, 0.0152439 , 0.12600321, 0.02879581, 0.11440329,\n",
      "       0.03917221, 0.01794072, 0.11601307, 0.0257732 , 0.04442771,\n",
      "       0.16479724, 0.03924528]), array([1382, 1312, 1246, 1146, 1215, 1353, 1282, 1224, 1358, 1328, 1159,\n",
      "       1325])), (array([0.01056958, 0.01393939, 0.01355514, 0.01047806, 0.06081946,\n",
      "       0.02207637, 0.02308627, 0.09354414, 0.02058824, 0.04649721,\n",
      "       0.03804348, 0.02112251]), array([1703, 1650, 1623, 1527, 1562, 1676, 1646, 1518, 1700, 1613, 1472,\n",
      "       1657])), (array([nan, nan,  0., nan, nan,  0.,  1., nan, nan,  0.,  1.,  1.]), array([0, 0, 4, 0, 0, 3, 1, 0, 0, 1, 4, 3])), (array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., nan,  0., nan,  0.,  0.]), array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1])), (array([0.        , 0.        , 0.16666667, 0.02857143, 0.02857143,\n",
      "       0.03846154, 0.        , 0.09333333, 0.03896104, 0.01298701,\n",
      "       0.08571429, 0.01298701]), array([77, 74, 72, 70, 70, 78, 73, 75, 77, 77, 70, 77])), (array([0. , 0. , 0. , 1. , 1. , 1. , 0. , 0.5, 0. , 0. , 0.5, 0. ]), array([2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2])), (array([nan, nan, nan, nan, nan, nan, nan,  0., nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0])), (array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., nan,  0., nan, nan,  0.]), array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])), (array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., nan,  0., nan,  0.,  0.]), array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1])), (array([0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
      "       0.33333333, 0.        , 0.5       , 0.        , 0.        ,\n",
      "       1.        , 0.        ]), array([2, 2, 2, 2, 2, 3, 1, 2, 3, 3, 1, 2]))]\n",
      "Class sizes\n",
      "NR-AR 0: 95.75%, 1: 4.25%\n",
      "NR-AR-LBD 0: 96.49%, 1: 3.51%\n",
      "NR-AhR 0: 88.27%, 1: 11.73%\n",
      "NR-Aromatase 0: 94.85%, 1: 5.15%\n",
      "NR-ER 0: 87.20%, 1: 12.80%\n",
      "NR-ER-LBD 0: 94.97%, 1: 5.03%\n",
      "NR-PPAR-gamma 0: 97.12%, 1: 2.88%\n",
      "SR-ARE 0: 83.85%, 1: 16.15%\n",
      "SR-ATAD5 0: 96.27%, 1: 3.73%\n",
      "SR-HSE 0: 94.25%, 1: 5.75%\n",
      "SR-MMP 0: 84.20%, 1: 15.80%\n",
      "SR-p53 0: 93.76%, 1: 6.24%\n",
      "Total size = 7,831 | train size = 6,264 | val size = 783 | test size = 784\n",
      "Loading model 0 from grover_large.pt\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
      "GroverFinetuneTask(\n",
      "  (grover): GROVEREmbedding(\n",
      "    (encoders): GTransEncoder(\n",
      "      (edge_blocks): ModuleList(\n",
      "        (0): MTBlock(\n",
      "          (heads): ModuleList(\n",
      "            (0): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (1): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (3): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (act_func): PReLU(num_parameters=1)\n",
      "          (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "          (layernorm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "          (W_i): Linear(in_features=165, out_features=1200, bias=False)\n",
      "          (attn): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "              (1): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "              (2): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (W_o): Linear(in_features=4800, out_features=1200, bias=False)\n",
      "          (sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (node_blocks): ModuleList(\n",
      "        (0): MTBlock(\n",
      "          (heads): ModuleList(\n",
      "            (0): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (1): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (3): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (act_func): PReLU(num_parameters=1)\n",
      "          (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "          (layernorm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "          (W_i): Linear(in_features=151, out_features=1200, bias=False)\n",
      "          (attn): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "              (1): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "              (2): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (W_o): Linear(in_features=4800, out_features=1200, bias=False)\n",
      "          (sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ffn_atom_from_atom): PositionwiseFeedForward(\n",
      "        (W_1): Linear(in_features=1351, out_features=4800, bias=True)\n",
      "        (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (ffn_atom_from_bond): PositionwiseFeedForward(\n",
      "        (W_1): Linear(in_features=1351, out_features=4800, bias=True)\n",
      "        (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (ffn_bond_from_atom): PositionwiseFeedForward(\n",
      "        (W_1): Linear(in_features=1365, out_features=4800, bias=True)\n",
      "        (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (ffn_bond_from_bond): PositionwiseFeedForward(\n",
      "        (W_1): Linear(in_features=1365, out_features=4800, bias=True)\n",
      "        (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (atom_from_atom_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (atom_from_bond_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (bond_from_atom_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (bond_from_bond_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (act_func_node): PReLU(num_parameters=1)\n",
      "      (act_func_edge): PReLU(num_parameters=1)\n",
      "      (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (readout): Readout()\n",
      "  (mol_atom_from_atom_ffn): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1400, out_features=300, bias=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Dropout(p=0.0, inplace=False)\n",
      "    (7): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (8): PReLU(num_parameters=1)\n",
      "    (9): Dropout(p=0.0, inplace=False)\n",
      "    (10): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Dropout(p=0.0, inplace=False)\n",
      "    (13): Linear(in_features=300, out_features=12, bias=True)\n",
      "  )\n",
      "  (mol_atom_from_bond_ffn): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1400, out_features=300, bias=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (5): PReLU(num_parameters=1)\n",
      "    (6): Dropout(p=0.0, inplace=False)\n",
      "    (7): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (8): PReLU(num_parameters=1)\n",
      "    (9): Dropout(p=0.0, inplace=False)\n",
      "    (10): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (11): PReLU(num_parameters=1)\n",
      "    (12): Dropout(p=0.0, inplace=False)\n",
      "    (13): Linear(in_features=300, out_features=12, bias=True)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Number of parameters = 108,532,858\n",
      "Moving model to cuda\n",
      "/workspace/grover/grover/util/scheduler.py:71: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.exponential_gamma = (self.final_lr / self.max_lr) ** (1 / (self.total_steps - self.warmup_steps))\n"
     ]
    }
   ],
   "source": [
    "if logger is not None:\n",
    "    debug, info = logger.debug, logger.info\n",
    "else:\n",
    "    debug = info = print\n",
    "\n",
    "\n",
    "# pin GPU to local rank.\n",
    "idx = args.gpu\n",
    "if args.gpu is not None:\n",
    "    torch.cuda.set_device(idx)\n",
    "\n",
    "features_scaler, scaler, shared_dict, test_data, train_data, val_data = load_data(args, debug, logger)\n",
    "metric_func = get_metric_func(metric=args.metric)\n",
    "\n",
    "# Set up test set evaluation\n",
    "test_smiles, test_targets = test_data.smiles(), test_data.targets()\n",
    "if args.multi_class:\n",
    "    sum_test_preds = np.zeros((len(test_smiles), args.multi_class_num))\n",
    "else:\n",
    "    sum_test_preds = np.zeros((len(test_smiles), args.num_tasks))\n",
    "\n",
    "model_idx=0\n",
    "save_dir = os.path.join(args.save_dir, f'model_{model_idx}')\n",
    "makedirs(save_dir)\n",
    "\n",
    "# Load/build model\n",
    "if args.checkpoint_paths is not None:\n",
    "    if len(args.checkpoint_paths) == 1:\n",
    "        cur_model = 0\n",
    "    else:\n",
    "        cur_model = model_idx\n",
    "    debug(f'Loading model {cur_model} from {args.checkpoint_paths[cur_model]}')\n",
    "    model = load_checkpoint(args.checkpoint_paths[cur_model], current_args=args, logger=logger)\n",
    "else:\n",
    "    debug(f'Building model {model_idx}')\n",
    "    model = build_model(model_idx=model_idx, args=args)\n",
    "\n",
    "if args.fine_tune_coff != 1 and args.checkpoint_paths is not None:\n",
    "    debug(\"Fine tune fc layer with different lr\")\n",
    "    initialize_weights(model_idx=model_idx, model=model.ffn, distinct_init=args.distinct_init)\n",
    "\n",
    "# Get loss and metric functions\n",
    "loss_func = get_loss_func(args, model)\n",
    "\n",
    "optimizer = build_optimizer(model, args)\n",
    "\n",
    "debug(model)\n",
    "debug(f'Number of parameters = {param_count(model):,}')\n",
    "if args.cuda:\n",
    "    debug('Moving model to cuda')\n",
    "    model = model.cuda()\n",
    "\n",
    "# set up wandb\n",
    "if args.wandb : \n",
    "    wandb.init(project=args.wandb_name)\n",
    "    wandb.config = args\n",
    "    wandb.watch(model)\n",
    "\n",
    "\n",
    "# Ensure that model is saved in correct location for evaluation if 0 epochs\n",
    "save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "\n",
    "# Learning rate schedulers\n",
    "scheduler = build_lr_scheduler(optimizer, args)\n",
    "\n",
    "# Bulid data_loader\n",
    "shuffle = True\n",
    "mol_collator = MolCollator(shared_dict={}, args=args)\n",
    "train_data = DataLoader(train_data,\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        num_workers=10,\n",
    "                        collate_fn=mol_collator)\n",
    "\n",
    "# Run training\n",
    "best_score = float('inf') if args.minimize_score else -float('inf')\n",
    "best_epoch, n_iter = 0, 0\n",
    "min_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8662747e-de4c-45e7-bad6-a4fc1f5e0f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 loss_train: 0.527613 loss_val: 0.715554 auc_val: 0.7568 cur_lr: 0.00015 t_time: 25.6708s v_time: 2.0701s\n",
      "Epoch: 0001 loss_train: 0.421411 loss_val: 0.715770 auc_val: 0.8045 cur_lr: 0.00003 t_time: 24.2523s v_time: 2.0245s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    s_time = time.time()\n",
    "    n_iter, train_loss = train(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        data=train_data,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        args=args,\n",
    "        n_iter=n_iter,\n",
    "        shared_dict=shared_dict,\n",
    "        logger=logger\n",
    "    )\n",
    "    t_time = time.time() - s_time\n",
    "    s_time = time.time()\n",
    "    val_scores, val_loss = evaluate(\n",
    "        model=model,\n",
    "        data=val_data,\n",
    "        loss_func=loss_func,\n",
    "        num_tasks=args.num_tasks,\n",
    "        metric_func=metric_func,\n",
    "        batch_size=args.batch_size,\n",
    "        dataset_type=args.dataset_type,\n",
    "        scaler=scaler,\n",
    "        shared_dict=shared_dict,\n",
    "        logger=logger,\n",
    "        args=args\n",
    "    )\n",
    "    v_time = time.time() - s_time\n",
    "\n",
    "\n",
    "    # Average validation score\n",
    "    avg_val_score = np.nanmean(val_scores)\n",
    "\n",
    "\n",
    "    # Logged after lr step\n",
    "    if isinstance(scheduler, ExponentialLR):\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.show_individual_scores:\n",
    "        # Individual validation scores\n",
    "        for task_name, val_score in zip(args.task_names, val_scores):\n",
    "            debug(f'Validation {task_name} {args.metric} = {val_score:.6f}')\n",
    "    print('Epoch: {:04d}'.format(epoch),\n",
    "          'loss_train: {:.6f}'.format(train_loss),\n",
    "          'loss_val: {:.6f}'.format(val_loss),\n",
    "          f'{args.metric}_val: {avg_val_score:.4f}',\n",
    "          # 'auc_val: {:.4f}'.format(avg_val_score),\n",
    "          'cur_lr: {:.5f}'.format(scheduler.get_lr()[-1]),\n",
    "          't_time: {:.4f}s'.format(t_time),\n",
    "          'v_time: {:.4f}s'.format(v_time))\n",
    "\n",
    "    if args.tensorboard:\n",
    "        writer.add_scalar('loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('loss/val', val_loss, epoch)\n",
    "        writer.add_scalar(f'{args.metric}_val', avg_val_score, epoch)\n",
    "\n",
    "    if args.wandb :         \n",
    "        wandb.log({\"val_loss\" : val_loss, \"val_metrics\" : val_scores})\n",
    "\n",
    "\n",
    "    # Save model checkpoint if improved validation score\n",
    "    if args.select_by_loss:\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss, best_epoch = val_loss, epoch\n",
    "            save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "    else:\n",
    "        if args.minimize_score and avg_val_score < best_score or \\\n",
    "                not args.minimize_score and avg_val_score > best_score:\n",
    "            best_score, best_epoch = avg_val_score, epoch\n",
    "            save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "\n",
    "    if epoch - best_epoch > args.early_stop_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3889a45b-c9f5-4654-91e9-ed0936772c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 0 best val loss = 0.715554 on epoch 0\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
      "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.5.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.7.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.7.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.8.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.10.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.10.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.11.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.13.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.13.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.5.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.7.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.7.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.8.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.10.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.10.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.11.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.13.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.13.bias\".\n",
      "Moving model to cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ensemble_scores = 0.0\n",
    "\n",
    "# Evaluate on test set using model with best validation score\n",
    "if args.select_by_loss:\n",
    "    info(f'Model {model_idx} best val loss = {min_val_loss:.6f} on epoch {best_epoch}')\n",
    "else:\n",
    "    info(f'Model {model_idx} best validation {args.metric} = {best_score:.6f} on epoch {best_epoch}')\n",
    "model = load_checkpoint(os.path.join(save_dir, 'model.pt'), cuda=args.cuda, logger=logger)\n",
    "\n",
    "test_preds, _ = predict(\n",
    "    model=model,\n",
    "    data=test_data,\n",
    "    loss_func=loss_func,\n",
    "    batch_size=args.batch_size,\n",
    "    logger=logger,\n",
    "    shared_dict=shared_dict,\n",
    "    scaler=scaler,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55805d76-94e9-453a-b02c-cef83d9c6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = evaluate_predictions(\n",
    "            preds=test_preds,\n",
    "            targets=test_targets,\n",
    "            num_tasks=args.num_tasks,\n",
    "            metric_func=metric_func,\n",
    "            dataset_type=args.dataset_type,\n",
    "            arg=args,\n",
    "            logger=logger\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4b9cdd-2e11-43b6-9810-b4e03ef37ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.723381314356569,\n",
       " 0.6980876628871214,\n",
       " 0.7870563674321502,\n",
       " 0.68391994478951,\n",
       " 0.7033121916842847,\n",
       " 0.7877631868534958,\n",
       " 0.6172839506172839,\n",
       " 0.6417086481947943,\n",
       " 0.7322822105430801,\n",
       " 0.6724259974259974,\n",
       " 0.7397490386561425,\n",
       " 0.7045931008195159]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12253ef2-c841-4c43-9200-81063eab7aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 0 test auc = 0.707630\n",
      "Ensemble test auc = 0.707630\n"
     ]
    }
   ],
   "source": [
    "if len(test_preds) != 0:\n",
    "    sum_test_preds += np.array(test_preds, dtype=float)\n",
    "\n",
    "# Average test score\n",
    "avg_test_score = np.nanmean(test_scores)\n",
    "info(f'Model {model_idx} test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "if args.show_individual_scores:\n",
    "    # Individual test scores\n",
    "    for task_name, test_score in zip(args.task_names, test_scores):\n",
    "        info(f'Model {model_idx} test {task_name} {args.metric} = {test_score:.6f}')\n",
    "\n",
    "# Evaluate ensemble on test set\n",
    "avg_test_preds = (sum_test_preds / args.ensemble_size).tolist()\n",
    "\n",
    "ensemble_scores = test_scores\n",
    "\n",
    "ind = [['preds'] * args.num_tasks + ['targets'] * args.num_tasks, args.task_names * 2]\n",
    "ind = pd.MultiIndex.from_tuples(list(zip(*ind)))\n",
    "if args.multi_class:\n",
    "    data = np.concatenate([np.array([np.argmax(x) for x in avg_test_preds]).reshape(-1,1), np.array(test_targets)], 1)\n",
    "else:\n",
    "    data = np.concatenate([np.array(avg_test_preds), np.array(test_targets)], 1)\n",
    "test_result = pd.DataFrame(data, index=test_smiles, columns=ind)\n",
    "test_result.to_csv(os.path.join(args.save_dir, 'test_result.csv'))\n",
    "\n",
    "# Average ensemble score\n",
    "avg_ensemble_test_score = np.nanmean(ensemble_scores)\n",
    "info(f'Ensemble test {args.metric} = {avg_ensemble_test_score:.6f}')\n",
    "\n",
    "# Individual ensemble scores\n",
    "if args.show_individual_scores:\n",
    "    for task_name, ensemble_score in zip(args.task_names, ensemble_scores):\n",
    "        info(f'Ensemble test {task_name} {args.metric} = {ensemble_score:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44281ca4-33fc-475d-9669-0e7d094b45f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.723381314356569,\n",
       " 0.6980876628871214,\n",
       " 0.7870563674321502,\n",
       " 0.68391994478951,\n",
       " 0.7033121916842847,\n",
       " 0.7877631868534958,\n",
       " 0.6172839506172839,\n",
       " 0.6417086481947943,\n",
       " 0.7322822105430801,\n",
       " 0.6724259974259974,\n",
       " 0.7397490386561425,\n",
       " 0.7045931008195159]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    else:\n",
    "        for fold_num in range(args.num_folds):\n",
    "            info(f'Fold {fold_num}')\n",
    "            args.seed = init_seed + fold_num\n",
    "            args.save_dir = os.path.join(save_dir, f'fold_{fold_num}')\n",
    "            makedirs(args.save_dir)\n",
    "            \n",
    "            AUC, ACC, REC, PREC, SPEC, F1, BA, TP, FP, TN, FN = run_evaluation_cfm(args, logger)\n",
    "            scores_AUC.append(AUC)\n",
    "            scores_ACC.append(ACC)\n",
    "            scores_REC.append(REC)\n",
    "            scores_PREC.append(PREC)\n",
    "            scores_SPEC.append(SPEC)\n",
    "            scores_F1.append(F1)\n",
    "            scores_BA.append(BA)\n",
    "            scores_TP.append(TP)\n",
    "            scores_FP.append(FP)\n",
    "            scores_TN.append(TN)\n",
    "            scores_FN.append(FN)\n",
    "        scores_AUC = np.array(scores_AUC)\n",
    "        scores_ACC = np.array(scores_ACC)\n",
    "        scores_REC = np.array(scores_REC)\n",
    "        scores_PREC = np.array(scores_PREC)\n",
    "        scores_SPEC = np.array(scores_SPEC)\n",
    "        scores_F1 = np.array(scores_F1)\n",
    "        scores_BA = np.array(scores_BA)\n",
    "        scores_TN = np.array(scores_TN)\n",
    "        scores_FN = np.array(scores_FN)\n",
    "        scores_TP = np.array(scores_TP)\n",
    "        scores_FP = np.array(scores_FP)\n",
    "\n",
    "        # Report scores for each fold\n",
    "        info(f'{args.num_folds}-fold cross validation')\n",
    "\n",
    "        # Report scores across models\n",
    "        avg_scores_AUC = np.nanmean(scores_AUC, axis=1)  # average score for each model across tasks\n",
    "        mean_score_AUC, std_score_AUC = np.nanmean(avg_scores_AUC), np.nanstd(avg_scores_AUC)\n",
    "        info(f'overall_{args.split_type}_test_AUC={mean_score_AUC:.6f}')\n",
    "        info(f'std={std_score_AUC:.6f}')\n",
    "\n",
    "        avg_scores_ACC = np.nanmean(scores_ACC, axis=1)  # average score for each model across tasks\n",
    "        mean_score_ACC, std_score_ACC = np.nanmean(avg_scores_ACC), np.nanstd(avg_scores_ACC)\n",
    "        info(f'overall_{args.split_type}_test_Accuracy={mean_score_ACC:.6f}')\n",
    "        info(f'std={std_score_ACC:.6f}')\n",
    "\n",
    "        avg_scores_REC = np.nanmean(scores_REC, axis=1)  # average score for each model across tasks\n",
    "        mean_score_REC, std_score_REC = np.nanmean(avg_scores_REC), np.nanstd(avg_scores_REC)\n",
    "        info(f'overall_{args.split_type}_test_Recall={mean_score_REC:.6f}')\n",
    "        info(f'std={std_score_REC:.6f}')\n",
    "\n",
    "        avg_scores_PREC = np.nanmean(scores_PREC, axis=1)  # average score for each model across tasks\n",
    "        mean_score_PREC, std_score_PREC = np.nanmean(avg_scores_PREC), np.nanstd(avg_scores_PREC)\n",
    "        info(f'overall_{args.split_type}_test_Precision={mean_score_PREC:.6f}')\n",
    "        info(f'std={std_score_PREC:.6f}')\n",
    "\n",
    "        avg_scores_SPEC = np.nanmean(scores_SPEC, axis=1)  # average score for each model across tasks\n",
    "        mean_score_SPEC, std_score_SPEC = np.nanmean(avg_scores_SPEC), np.nanstd(avg_scores_SPEC)\n",
    "        info(f'overall_{args.split_type}_test_Specificity={mean_score_SPEC:.6f}')\n",
    "        info(f'std={std_score_SPEC:.6f}')\n",
    "\n",
    "        avg_scores_F1 = np.nanmean(scores_F1, axis=1)  # average score for each model across tasks\n",
    "        mean_score_F1, std_score_F1 = np.nanmean(avg_scores_F1), np.nanstd(avg_scores_F1)\n",
    "        info(f'overall_{args.split_type}_test_F1={mean_score_F1:.6f}')\n",
    "        info(f'std={std_score_F1:.6f}')\n",
    "\n",
    "        avg_scores_BA = np.nanmean(scores_BA, axis=1)  # average score for each model across tasks\n",
    "        mean_score_BA, std_score_BA = np.nanmean(avg_scores_BA), np.nanstd(avg_scores_BA)\n",
    "        info(f'overall_{args.split_type}_test_BA={mean_score_BA:.6f}')\n",
    "        info(f'std={std_score_BA:.6f}')\n",
    "\n",
    "        avg_scores_TP = np.nanmean(scores_TP)  # average score for each model across tasks\n",
    "        mean_score_TP, std_score_TP = np.nanmean(avg_scores_TP), np.nanstd(avg_scores_TP)\n",
    "\n",
    "        avg_scores_FP = np.nanmean(scores_FP)  # average score for each model across tasks\n",
    "        mean_score_FP, std_score_FP = np.nanmean(avg_scores_FP), np.nanstd(avg_scores_FP)\n",
    "\n",
    "        avg_scores_TN = np.nanmean(scores_TN)  # average score for each model across tasks\n",
    "        mean_score_TN, std_score_TN = np.nanmean(avg_scores_TN), np.nanstd(avg_scores_TN)\n",
    "\n",
    "        avg_scores_FN = np.nanmean(scores_FN)  # average score for each model across tasks\n",
    "        mean_score_FN, std_score_FN = np.nanmean(avg_scores_FN), np.nanstd(avg_scores_FN)\n",
    "        info(f'TP : {mean_score_TP:.6f}\\tFP : {mean_score_FP:.6f}')\n",
    "        info(f'FN : {mean_score_FN:.6f}\\tTN : {mean_score_TN:.6f}')\n",
    "\n",
    "\n",
    "        if args.show_individual_scores:\n",
    "            for task_num, task_name in enumerate(task_names):\n",
    "                info(f'Overall test {task_name} {args.metric} = '\n",
    "                     f'{np.nanmean(all_scores[:, task_num]):.6f} +/- {np.nanstd(all_scores[:, task_num]):.6f}')\n",
    "\n",
    "        return mean_score_AUC, std_score_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20cb30a4-2454-4a01-8c3f-9fd91e22a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_scores_AUC, test_scores_ACC, test_scores_REC, test_scores_PREC, test_scores_SPEC, test_scores_F1, test_scores_BA, test_TP, test_FP, test_TN, test_FN = evaluate_predictions_cfm(\n",
    "        preds=test_preds,\n",
    "        targets=test_targets,\n",
    "        num_tasks=args.num_tasks,\n",
    "        metric_func=metric_func,\n",
    "        dataset_type=args.dataset_type,\n",
    "        args=args, logger=logger\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801a71f-7be2-4a4f-9139-4ff0f268a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.confusionmatrix:\n",
    "    test_scores_AUC, test_scores_ACC, test_scores_REC, test_scores_PREC, test_scores_SPEC, test_scores_F1, test_scores_BA, test_TP, test_FP, test_TN, test_FN = evaluate_predictions_cfm(\n",
    "        preds=test_preds,\n",
    "        targets=test_targets,\n",
    "        num_tasks=args.num_tasks,\n",
    "        metric_func=metric_func,\n",
    "        dataset_type=args.dataset_type,\n",
    "        args=args, logger=logger\n",
    "    )\n",
    "    # Average test score\n",
    "    avg_test_score_AUC = np.nanmean(test_scores_AUC)\n",
    "    avg_test_score_ACC = np.nanmean(test_scores_ACC)\n",
    "    avg_test_score_REC = np.nanmean(test_scores_REC)\n",
    "    avg_test_score_PREC = np.nanmean(test_scores_PREC)\n",
    "    avg_test_score_SPEC = np.nanmean(test_scores_SPEC)\n",
    "    avg_test_score_F1 = np.nanmean(test_scores_F1)\n",
    "    avg_test_score_BA = np.nanmean(test_scores_BA)\n",
    "    avg_test_TP = np.nanmean(test_TP)\n",
    "    avg_test_TN = np.nanmean(test_TN)\n",
    "    avg_test_FP = np.nanmean(test_FP)\n",
    "    avg_test_FN = np.nanmean(test_FN)\n",
    "    info(f'Model test AUC = {avg_test_score_AUC:.6f}')\n",
    "    info(f'Model test ACC = {avg_test_score_ACC:.6f}')\n",
    "    info(f'Model test REC = {avg_test_score_REC:.6f}')\n",
    "    info(f'Model test PREC = {avg_test_score_PREC:.6f}')\n",
    "    info(f'Model test SPEC = {avg_test_score_SPEC:.6f}')\n",
    "    info(f'Model test F1 = {avg_test_score_F1:.6f}')\n",
    "    info(f'Model test BA = {avg_test_score_BA:.6f}')\n",
    "    info(f'Confusion matrix\\nTP : {avg_test_TP:.6f}\\tFP : {avg_test_FP:.6f}')\n",
    "    info(f'FN : {avg_test_FN:.6f}\\tTN : {avg_test_TN:.6f}')\n",
    "\n",
    "    if args.metric=='f1':\n",
    "        test_score = test_scores_F1\n",
    "        avg_test_score = avg_test_score_F1\n",
    "    elif args.metric=='recall':\n",
    "        test_score = test_scores_REC\n",
    "        avg_test_score = avg_test_score_REC\n",
    "    elif args.metric=='auc':\n",
    "        test_score = test_scores_AUC\n",
    "        avg_test_score = avg_test_score_AUC\n",
    "    else:\n",
    "        raise ValueError(f'in confusionmatrix, Metric \"{metric}\" not supported. add the metric in code')\n",
    "\n",
    "else:\n",
    "    test_scores = evaluate_predictions(\n",
    "        preds=test_preds,\n",
    "        targets=test_targets,\n",
    "        num_tasks=args.num_tasks,\n",
    "        metric_func=metric_func,\n",
    "        dataset_type=args.dataset_type,\n",
    "        arg=args,\n",
    "        logger=logger\n",
    "    )\n",
    "    # Average test score\n",
    "    avg_test_score = np.nanmean(test_scores)\n",
    "    info(f'Model {model_idx} test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "if len(test_preds) != 0:\n",
    "    sum_test_preds += np.array(test_preds, dtype=float)\n",
    "\n",
    "if args.show_individual_scores:\n",
    "    # Individual test scores\n",
    "    for task_name, test_score in zip(args.task_names, test_scores):\n",
    "        info(f'Model {model_idx} test {task_name} {args.metric} = {test_score:.6f}')\n",
    "\n",
    "# Evaluate ensemble on test set\n",
    "avg_test_preds = (sum_test_preds / args.ensemble_size).tolist()\n",
    "\n",
    "\"\"\"ensemble_scores = evaluate_predictions(\n",
    "    preds=avg_test_preds,\n",
    "    targets=test_targets,\n",
    "    num_tasks=args.num_tasks,\n",
    "    metric_func=metric_func,\n",
    "    dataset_type=args.dataset_type,\n",
    "    arg=args,\n",
    "    logger=logger\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "ind = [['preds'] * args.num_tasks + ['targets'] * args.num_tasks, args.task_names * 2]\n",
    "ind = pd.MultiIndex.from_tuples(list(zip(*ind)))\n",
    "if args.multi_class:\n",
    "    data = np.concatenate([np.array([np.argmax(x) for x in avg_test_preds]).reshape(-1,1), np.array(test_targets)], 1)\n",
    "else:\n",
    "    data = np.concatenate([np.array(avg_test_preds), np.array(test_targets)], 1)\n",
    "test_result = pd.DataFrame(data, index=test_smiles, columns=ind)\n",
    "test_result.to_csv(os.path.join(args.save_dir, 'test_result.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa7d4b47-2238-430a-b22a-4c975b3a00ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6245438013237361]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c85479a-6667-4d75-b8cc-b4eb2139e6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6245438013237361]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa567d-5ab2-40b6-b2a2-0c59e5710b64",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run_motif_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac507d5a-1734-4e38-a5a3-bf84c0047667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "from grover.data.dist_sampler import DistributedSampler\n",
    "from grover.data.groverdataset import get_data, split_data, GroverCollator, BatchMolDataset, get_motif_data, split_data_motif, GroverMotifCollator, BatchMolDataset_motif\n",
    "from grover.data.torchvocab import MolVocab\n",
    "from grover.model.models import GROVEREmbedding\n",
    "from grover.util.multi_gpu_wrapper import MultiGpuWrapper as mgw\n",
    "from grover.util.nn_utils import param_count\n",
    "from grover.util.utils import build_optimizer, build_lr_scheduler\n",
    "from task.grovertrainer import GROVERTrainer, GROVERMotifTrainer\n",
    "\n",
    "from grover.topology.mol_tree import Motif_Vocab\n",
    "from grover.topology.motif_generation import Motif_Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "848fe2c1-2007-4e74-b062-78b1ffcf79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_motif(data,\n",
    "               split_type='random',\n",
    "               sizes=(0.8, 0.1, 0.1),\n",
    "               seed=0,\n",
    "               logger=None):\n",
    "    \"\"\"\n",
    "    Split data with given train/validation/test ratio.\n",
    "    :param data:\n",
    "    :param split_type:\n",
    "    :param sizes:\n",
    "    :param seed:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(sizes) == 3 and sum(sizes) == 1\n",
    "\n",
    "    if split_type == \"random\":\n",
    "        data.shuffle(seed=seed)\n",
    "        data = data.data\n",
    "\n",
    "        train_size = int(sizes[0] * len(data))\n",
    "        train_val_size = int((sizes[0] + sizes[1]) * len(data))\n",
    "\n",
    "        train = data[:train_size]\n",
    "        val = data[train_size:train_val_size]\n",
    "        test = data[train_val_size:]\n",
    "\n",
    "        return BatchMolDataset_motif(train), BatchMolDataset_motif(val), BatchMolDataset_motif(test)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Do not support %s splits\" % split_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a27791-9346-4b28-9a12-468aa1de6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_load_data(dataset: BatchMolDataset, rank: int, num_replicas: int, sample_per_file: int = None, epoch: int = 0):\n",
    "\n",
    "    mock_sampler = DistributedSampler(dataset, num_replicas=num_replicas, rank=rank, shuffle=False,\n",
    "                                      sample_per_file=sample_per_file)\n",
    "    mock_sampler.set_epoch(epoch)\n",
    "    pre_indices = mock_sampler.get_indices()\n",
    "    for i in pre_indices:\n",
    "        dataset.load_data(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a279da9-13dc-4e47-b0cb-0306b906ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_motif_training(args, logger):\n",
    "    \"\"\"\n",
    "    Run the pretrain task with topology predict.\n",
    "    :param args:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # initalize the logger.\n",
    "    if logger is not None:\n",
    "        debug, _ = logger.debug, logger.info\n",
    "    else:\n",
    "        debug = print\n",
    "\n",
    "    # initialize the horovod library\n",
    "    if args.enable_multi_gpu:\n",
    "        mgw.init()\n",
    "\n",
    "    # binding training to GPUs.\n",
    "    master_worker = (mgw.rank() == 0) if args.enable_multi_gpu else True\n",
    "    # pin GPU to local rank. By default, we use gpu:0 for training.\n",
    "    local_gpu_idx = mgw.local_rank() if args.enable_multi_gpu else 0\n",
    "    with_cuda = args.cuda\n",
    "    if with_cuda:\n",
    "        torch.cuda.set_device(local_gpu_idx)\n",
    "\n",
    "    # get rank an  number of workers\n",
    "    rank = mgw.rank() if args.enable_multi_gpu else 0\n",
    "    num_replicas = mgw.size() if args.enable_multi_gpu else 1\n",
    "    # print(\"Rank: %d Rep: %d\" % (rank, num_replicas))\n",
    "\n",
    "    # load file paths of the data.\n",
    "    if master_worker:\n",
    "        print(args)\n",
    "        if args.enable_multi_gpu:\n",
    "            debug(\"Total workers: %d\" % (mgw.size()))\n",
    "        debug('Loading data')\n",
    "    data, sample_per_file = get_motif_data(data_path=args.data_path)\n",
    "\n",
    "    # data splitting\n",
    "    if master_worker:\n",
    "        debug(f'Splitting data with seed 0.')\n",
    "    train_data, test_data, _ = split_data_motif(data=data, sizes=(0.9, 0.1, 0.0), seed=0, logger=logger)\n",
    "\n",
    "    # Here the true train data size is the train_data divided by #GPUs\n",
    "    if args.enable_multi_gpu:\n",
    "        args.train_data_size = len(train_data) // mgw.size()\n",
    "    else:\n",
    "        args.train_data_size = len(train_data)\n",
    "    if master_worker:\n",
    "        debug(f'Total size = {len(data):,} | '\n",
    "              f'train size = {len(train_data):,} | val size = {len(test_data):,}')\n",
    "\n",
    "    # load atom and bond vocabulary and the semantic motif labels.\n",
    "    atom_vocab = MolVocab.load_vocab(args.atom_vocab_path)\n",
    "    bond_vocab = MolVocab.load_vocab(args.bond_vocab_path)\n",
    "    atom_vocab_size, bond_vocab_size = len(atom_vocab), len(bond_vocab)\n",
    "\n",
    "    # Load motif vocabulary for pretrain\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    motif_vocab = [x.strip(\"\\r\\n \") for x in open(args.motif_vocab_path)]\n",
    "    motif_vocab = Motif_Vocab(motif_vocab)\n",
    "\n",
    "    # Hard coding here, since we haven't load any data yet!\n",
    "    fg_size = 85\n",
    "    shared_dict = {}\n",
    "    motif_collator = GroverMotifCollator(shared_dict=shared_dict, atom_vocab=atom_vocab, bond_vocab=bond_vocab, args=args)\n",
    "    if master_worker:\n",
    "        debug(\"atom vocab size: %d, bond vocab size: %d, Number of FG tasks: %d\" % (atom_vocab_size,\n",
    "                                                                                    bond_vocab_size, fg_size))\n",
    "\n",
    "    # Define the distributed sampler. If using the single card, the sampler will be None.\n",
    "    train_sampler = None\n",
    "    test_sampler = None\n",
    "    shuffle = True\n",
    "    if args.enable_multi_gpu:\n",
    "        # If not shuffle, the performance may decayed.\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=True, sample_per_file=sample_per_file)\n",
    "        # Here sample_per_file in test_sampler is None, indicating the test sampler would not divide the test samples by\n",
    "        # rank. (TODO: bad design here.)\n",
    "        test_sampler = DistributedSampler(\n",
    "            test_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=False)\n",
    "        train_sampler.set_epoch(args.epochs)\n",
    "        test_sampler.set_epoch(1)\n",
    "        # if we enables multi_gpu training. shuffle should be disabled.\n",
    "        shuffle = False\n",
    "\n",
    "    # Pre load data. (Maybe unnecessary. )\n",
    "    pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
    "    pre_load_data(test_data, rank, num_replicas)\n",
    "    if master_worker:\n",
    "        # print(\"Pre-loaded training data: %d\" % train_data.count_loaded_datapoints())\n",
    "        print(\"Pre-loaded test data: %d\" % test_data.count_loaded_datapoints())\n",
    "\n",
    "    # Build dataloader\n",
    "    train_data_dl = DataLoader(train_data,\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=shuffle,\n",
    "                               num_workers=12,\n",
    "                               sampler=train_sampler,\n",
    "                               collate_fn=motif_collator)\n",
    "    test_data_dl = DataLoader(test_data,\n",
    "                              batch_size=args.batch_size,\n",
    "                              shuffle=shuffle,\n",
    "                              num_workers=10,\n",
    "                              sampler=test_sampler,\n",
    "                              collate_fn=motif_collator)\n",
    "\n",
    "    # Build the embedding model.\n",
    "    grover_model = GROVEREmbedding(args)\n",
    "    \n",
    "    # build the topology predict model.\n",
    "    motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order)\n",
    "\n",
    "    #  Build the trainer.\n",
    "    trainer = GROVERMotifTrainer(args=args,\n",
    "                            embedding_model=grover_model,\n",
    "                            topology_model=motif_model,\n",
    "                            atom_vocab_size=atom_vocab_size,\n",
    "                            bond_vocab_size=bond_vocab_size,\n",
    "                            fg_size=fg_size,\n",
    "                            train_dataloader=train_data_dl,\n",
    "                            test_dataloader=test_data_dl,\n",
    "                            optimizer_builder=build_optimizer,\n",
    "                            scheduler_builder=build_lr_scheduler,\n",
    "                            logger=logger,\n",
    "                            with_cuda=with_cuda,\n",
    "                            enable_multi_gpu=args.enable_multi_gpu)\n",
    "\n",
    "    # Restore the interrupted training.\n",
    "    model_dir = os.path.join(args.save_dir, \"model\")\n",
    "    resume_from_epoch = 0\n",
    "    resume_scheduler_step = 0\n",
    "    if master_worker:\n",
    "        resume_from_epoch, resume_scheduler_step = trainer.restore(model_dir)\n",
    "    if args.enable_multi_gpu:\n",
    "        resume_from_epoch = mgw.broadcast(torch.tensor(resume_from_epoch), root_rank=0, name=\"resume_from_epoch\").item()\n",
    "        resume_scheduler_step = mgw.broadcast(torch.tensor(resume_scheduler_step),\n",
    "                                              root_rank=0, name=\"resume_scheduler_step\").item()\n",
    "        trainer.scheduler.current_step = resume_scheduler_step\n",
    "        print(\"Restored epoch: %d Restored scheduler step: %d\" % (resume_from_epoch, trainer.scheduler.current_step))\n",
    "    trainer.broadcast_parameters()\n",
    "\n",
    "    # Print model details.\n",
    "    if master_worker:\n",
    "        # Change order here.\n",
    "        print(grover_model)\n",
    "        print(\"Total parameters: %d\" % param_count(trainer.grover))\n",
    "\n",
    "    #wandb\n",
    "    if args.wandb :\n",
    "        wandb.init(project=args.wandb_name)\n",
    "        wandb.config = args\n",
    "        wandb.watch(grover_model)\n",
    "        \n",
    "    # Perform training.\n",
    "    best_val_loss = 0\n",
    "    best_val_epoch = 0\n",
    "    best_model_dir = os.path.join(args.save_dir, \"model_best\")\n",
    "    for epoch in range(resume_from_epoch + 1, args.epochs):\n",
    "        s_time = time.time()\n",
    "\n",
    "        # Data pre-loading.\n",
    "        if args.enable_multi_gpu:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "            train_data.clean_cache()\n",
    "            idxs = train_sampler.get_indices()\n",
    "            for local_gpu_idx in idxs:\n",
    "                train_data.load_data(local_gpu_idx)\n",
    "        d_time = time.time() - s_time\n",
    "\n",
    "        # perform training and validation.\n",
    "        s_time = time.time()\n",
    "        _, train_loss, _ = trainer.train(epoch)\n",
    "        t_time = time.time() - s_time\n",
    "        s_time = time.time()\n",
    "        _, val_loss, detailed_loss_val = trainer.test(epoch)\n",
    "        val_av_loss, val_bv_loss, val_fg_loss, _, _, _, val_topo_loss, val_node_loss, topo_acc, node_acc = detailed_loss_val\n",
    "        v_time = time.time() - s_time\n",
    "        \n",
    "        if best_val_loss > val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_epoch = epoch\n",
    "            trainer.save(epoch, best_model_dir)\n",
    "\n",
    "        if args.wandb :         \n",
    "            wandb.log({\"train_loss\" : train_loss, \"val_loss\" : val_loss, \"topo_loss\" : val_topo_loss})\n",
    "        \n",
    "        # print information.\n",
    "        if master_worker:\n",
    "            print('Epoch: {:04d}'.format(epoch),\n",
    "                  'loss_train: {:.6f}'.format(train_loss),\n",
    "                  'loss_val: {:.6f}'.format(val_loss),\n",
    "                  'loss_val_av: {:.6f}'.format(val_av_loss),\n",
    "                  'loss_val_bv: {:.6f}'.format(val_bv_loss),\n",
    "                  'loss_val_fg: {:.6f}'.format(val_fg_loss),\n",
    "                  'loss_val_topo: {:.6f}'.format(val_topo_loss),\n",
    "                  'loss_val_node: {:.6f}'.format(val_node_loss),\n",
    "                  'acc_topo: {:.6f}'.format(topo_acc),\n",
    "                  'acc_node: {:.6f}'.format(node_acc),\n",
    "                  'cur_lr: {:.5f}'.format(trainer.scheduler.get_lr()[0]),\n",
    "                  't_time: {:.4f}s'.format(t_time),\n",
    "                  'v_time: {:.4f}s'.format(v_time),\n",
    "                  'd_time: {:.4f}s'.format(d_time), flush=True)\n",
    "            \n",
    "        \n",
    "            if epoch % args.save_interval == 0:\n",
    "                trainer.save(epoch, model_dir)\n",
    "\n",
    "\n",
    "            trainer.save_tmp(epoch, model_dir, rank)\n",
    "\n",
    "    # Only save final version.\n",
    "    if master_worker:\n",
    "        trainer.save(args.epochs, model_dir, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23d49d-8669-42e7-bed6-6c9b7722c639",
   "metadata": {
    "tags": []
   },
   "source": [
    "# dataset 관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8eb4868-bff7-4de5-b7b5-266d3b0003cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from argparse import Namespace\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from grover.data.molfeaturegenerator import get_features_generator\n",
    "from grover.data.scaler import StandardScaler\n",
    "\n",
    "import grover.util.utils as feautils\n",
    "from grover.data import mol2graph\n",
    "from grover.data.moldataset import MoleculeDatapoint\n",
    "from grover.data.task_labels import atom_to_vocab, bond_to_vocab\n",
    "\n",
    "from grover.topology.mol_tree import MolTree, MolTree_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb5b661d-320c-4462-9872-91eb63abdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이건 pretrain.py로\n",
    "def pre_load_data_motif(dataset: BatchMolDataset, rank: int, num_replicas: int, sample_per_file: int = None, epoch: int = 0):\n",
    "    \"\"\"\n",
    "    Pre-load data at the beginning of each epoch.\n",
    "    :param dataset: the training dataset.\n",
    "    :param rank: the rank of the current worker.\n",
    "    :param num_replicas: the replicas.\n",
    "    :param sample_per_file: the number of the data points in each file. When sample_per_file is None, all data will be\n",
    "    loaded. It implies the testing phase. (TODO: bad design here.)\n",
    "    :param epoch: the epoch number.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mock_sampler = DistributedSampler(dataset, num_replicas=num_replicas, rank=rank, shuffle=False,\n",
    "                                      sample_per_file=sample_per_file)\n",
    "    mock_sampler.set_epoch(epoch)\n",
    "    pre_indices = mock_sampler.get_indices()\n",
    "    for i in pre_indices:\n",
    "        dataset.load_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00fbb3be-0b46-4439-9f26-00d122d36a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDatapoint_motif:\n",
    "    \"\"\"A MoleculeDatapoint contains a single molecule and its associated features and targets.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 line: List[str],\n",
    "                 args: Namespace = None,\n",
    "                 features: np.ndarray = None,\n",
    "                 moltrees: object = None,\n",
    "                 use_compound_names: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes a MoleculeDatapoint, which contains a single molecule.\n",
    "\n",
    "        :param line: A list of strings generated by separating a line in a data CSV file by comma.\n",
    "        :param args: Arguments.\n",
    "        :param features: A numpy array containing additional features (ex. Morgan fingerprint).\n",
    "        :param use_compound_names: Whether the data CSV includes the compound name on each line.\n",
    "        \"\"\"\n",
    "        self.features_generator = None\n",
    "        self.args = None\n",
    "        if args is not None:\n",
    "            if hasattr(args, \"features_generator\"):\n",
    "                self.features_generator = args.features_generator\n",
    "            self.args = args\n",
    "\n",
    "        if features is not None and self.features_generator is not None:\n",
    "            raise ValueError('Currently cannot provide both loaded features and a features generator.')\n",
    "\n",
    "        self.features = features\n",
    "        self.moltree_path = moltrees[0]\n",
    "        self.moltree_index = moltrees[1]\n",
    "        self.moltrees = moltrees\n",
    "\n",
    "        if use_compound_names:\n",
    "            self.compound_name = line[0]  # str\n",
    "            line = line[1:]\n",
    "        else:\n",
    "            self.compound_name = None\n",
    "\n",
    "        self.smiles = line[0]  # str\n",
    "\n",
    "\n",
    "        # Generate additional features if given a generator\n",
    "        if self.features_generator is not None:\n",
    "            self.features = []\n",
    "            mol = Chem.MolFromSmiles(self.smiles)\n",
    "            for fg in self.features_generator:\n",
    "                features_generator = get_features_generator(fg)\n",
    "                if mol is not None and mol.GetNumHeavyAtoms() > 0:\n",
    "                    if fg in ['morgan', 'morgan_count']:\n",
    "                        self.features.extend(features_generator(mol, num_bits=args.num_bits))\n",
    "                    else:\n",
    "                        self.features.extend(features_generator(mol))\n",
    "\n",
    "            self.features = np.array(self.features)\n",
    "\n",
    "        # Fix nans in features\n",
    "        if self.features is not None:\n",
    "            replace_token = 0\n",
    "            self.features = np.where(np.isnan(self.features), replace_token, self.features)\n",
    "\n",
    "        # Create targets\n",
    "        self.targets = [float(x) if x != '' else None for x in line[1:]]\n",
    "\n",
    "    def set_features(self, features: np.ndarray):\n",
    "        \"\"\"\n",
    "        Sets the features of the molecule.\n",
    "\n",
    "        :param features: A 1-D numpy array of features for the molecule.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        \n",
    "    def set_moltrees(self, moltrees: list):\n",
    "        \"\"\"\n",
    "        Sets the moltree of the molecule.\n",
    "\n",
    "        :param moltree: moltree object\n",
    "        \"\"\"\n",
    "        self.moltrees = moltrees\n",
    "        \n",
    "    def load_moltree(self):\n",
    "        \"\"\"\n",
    "        load moltree of the molecule.\n",
    "        \"\"\"\n",
    "        with open(self.moltree_path, 'rb') as f:\n",
    "            moltreefile = pickle.load(f)\n",
    "            self.moltrees = moltreefile[self.moltree_index]\n",
    "        f.close()\n",
    "        \n",
    "    def clean_moltree(self):\n",
    "        \"\"\"\n",
    "        clean moltree for memory\n",
    "        \"\"\"\n",
    "        self.moltrees = None\n",
    "\n",
    "    def num_tasks(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of prediction tasks.\n",
    "\n",
    "        :return: The number of tasks.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def set_targets(self, targets: List[float]):\n",
    "        \"\"\"\n",
    "        Sets the targets of a molecule.\n",
    "\n",
    "        :param targets: A list of floats containing the targets.\n",
    "        \"\"\"\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __getitem__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95193341-07ea-450b-9523-81affebd9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDatapoint_motif:\n",
    "    def __init__(self,\n",
    "                 smiles_file,\n",
    "                 feature_file,\n",
    "                 moltree_file,\n",
    "                 n_samples,\n",
    "                 ):\n",
    "        self.smiles_file = smiles_file\n",
    "        self.feature_file = feature_file\n",
    "        self.moltree_file = moltree_file\n",
    "        # deal with the last batch graph numbers.\n",
    "        self.n_samples = n_samples\n",
    "        self.datapoints = None\n",
    "\n",
    "    def load_datapoints(self):\n",
    "        features = self.load_feature()\n",
    "        #moltrees = self.load_moltree()\n",
    "        moltrees = self.moltree_file\n",
    "        self.datapoints = []\n",
    "\n",
    "        with open(self.smiles_file) as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for i, line in enumerate(reader):\n",
    "                # line = line[0]\n",
    "#                d = MoleculeDatapoint_motif(line=line,\n",
    "#                                      features=features[i],\n",
    "#                                      moltrees=moltrees[i])\n",
    "                d = MoleculeDatapoint_motif(line=line,\n",
    "                                      features=features[i],\n",
    "                                      moltrees=[str(self.moltree_file),i])\n",
    "                self.datapoints.append(d)\n",
    "        f.close()\n",
    "\n",
    "        assert len(self.datapoints) == self.n_samples\n",
    "\n",
    "    def load_feature(self):\n",
    "        return feautils.load_features(self.feature_file)\n",
    "    \n",
    "    def load_moltree(self):\n",
    "        return feautils.load_moltrees(self.moltree_file)\n",
    "\n",
    "    def shuffle(self):\n",
    "        pass\n",
    "\n",
    "    def clean_cache(self):\n",
    "        del self.datapoints\n",
    "        self.datapoints = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert self.datapoints is not None\n",
    "        return self.datapoints[idx]\n",
    "\n",
    "    def is_loaded(self):\n",
    "        return self.datapoints is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2188d4-ed70-43ad-ad74-a1c5e2b91a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMolDataset_motif(Dataset):\n",
    "    def __init__(self, data: List[BatchDatapoint_motif],\n",
    "                 graph_per_file=None):\n",
    "        self.data = data\n",
    "\n",
    "        self.len = 0\n",
    "        for d in self.data:\n",
    "            self.len += len(d)\n",
    "        if graph_per_file is not None:\n",
    "            self.sample_per_file = graph_per_file\n",
    "        else:\n",
    "            self.sample_per_file = len(self.data[0]) if len(self.data) != 0 else None\n",
    "\n",
    "    def shuffle(self, seed: int = None):\n",
    "        pass\n",
    "\n",
    "    def clean_cache(self):\n",
    "        for d in self.data:\n",
    "            d.clean_cache()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx) -> Union[MoleculeDatapoint_motif, List[MoleculeDatapoint_motif]]:\n",
    "        # print(idx)\n",
    "        dp_idx = int(idx / self.sample_per_file)\n",
    "        real_idx = idx % self.sample_per_file\n",
    "        return self.data[dp_idx][real_idx]\n",
    "\n",
    "    def load_data(self, idx):\n",
    "        dp_idx = int(idx / self.sample_per_file)\n",
    "        if not self.data[dp_idx].is_loaded():\n",
    "            self.data[dp_idx].load_datapoints()\n",
    "\n",
    "    def count_loaded_datapoints(self):\n",
    "        res = 0\n",
    "        for d in self.data:\n",
    "            if d.is_loaded():\n",
    "                res += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a30fdb13-8acf-4f1a-8eaf-c341c091c77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GroverMotifCollator(object):\n",
    "    def __init__(self, shared_dict, atom_vocab, bond_vocab, args):\n",
    "        self.args = args\n",
    "        self.shared_dict = shared_dict\n",
    "        self.atom_vocab = atom_vocab\n",
    "        self.bond_vocab = bond_vocab\n",
    "\n",
    "    def atom_random_mask(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Perform the random mask operation on atoms.\n",
    "        :param smiles_batch:\n",
    "        :return: The corresponding atom labels.\n",
    "        \"\"\"\n",
    "        # There is a zero padding.\n",
    "        vocab_label = [0]\n",
    "        percent = 0.15\n",
    "        for smi in smiles_batch:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            mlabel = [0] * mol.GetNumAtoms()\n",
    "            n_mask = math.ceil(mol.GetNumAtoms() * percent)\n",
    "            perm = np.random.permutation(mol.GetNumAtoms())[:n_mask]\n",
    "            for p in perm:\n",
    "                atom = mol.GetAtomWithIdx(int(p))\n",
    "                mlabel[p] = self.atom_vocab.stoi.get(atom_to_vocab(mol, atom), self.atom_vocab.other_index)\n",
    "\n",
    "            vocab_label.extend(mlabel)\n",
    "        return vocab_label\n",
    "\n",
    "    def bond_random_mask(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Perform the random mask operaiion on bonds.\n",
    "        :param smiles_batch:\n",
    "        :return: The corresponding bond labels.\n",
    "        \"\"\"\n",
    "        # There is a zero padding.\n",
    "        vocab_label = [0]\n",
    "        percent = 0.15\n",
    "        for smi in smiles_batch:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            nm_atoms = mol.GetNumAtoms()\n",
    "            nm_bonds = mol.GetNumBonds()\n",
    "            mlabel = []\n",
    "            n_mask = math.ceil(nm_bonds * percent)\n",
    "            perm = np.random.permutation(nm_bonds)[:n_mask]\n",
    "            virtual_bond_id = 0\n",
    "            for a1 in range(nm_atoms):\n",
    "                for a2 in range(a1 + 1, nm_atoms):\n",
    "                    bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                    if bond is None:\n",
    "                        continue\n",
    "                    if virtual_bond_id in perm:\n",
    "                        label = self.bond_vocab.stoi.get(bond_to_vocab(mol, bond), self.bond_vocab.other_index)\n",
    "                        mlabel.extend([label])\n",
    "                    else:\n",
    "                        mlabel.extend([0])\n",
    "\n",
    "                    virtual_bond_id += 1\n",
    "            # todo: might need to consider bond_drop_rate\n",
    "            # todo: double check reverse bond\n",
    "            vocab_label.extend(mlabel)\n",
    "        return vocab_label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles_batch = [d.smiles for d in batch] # 여기서 말하는 batch는 batchmoldataset_motif다 그리고 d는 batchdatapoint_motif고\n",
    "        batchgraph = mol2graph(smiles_batch, self.shared_dict, self.args).get_components()\n",
    "\n",
    "        atom_vocab_label = torch.Tensor(self.atom_random_mask(smiles_batch)).long()\n",
    "        bond_vocab_label = torch.Tensor(self.bond_random_mask(smiles_batch)).long()\n",
    "        fgroup_label = torch.Tensor(np.array([d.features for d in batch])).float()\n",
    "        moltree_batch = [d.moltrees for d in batch]\n",
    "        \n",
    "        # may be some mask here\n",
    "        res = {\"graph_input\": batchgraph,\n",
    "               \"targets\": {\"av_task\": atom_vocab_label,\n",
    "                           \"bv_task\": bond_vocab_label,\n",
    "                           \"fg_task\": fgroup_label},\n",
    "               \"moltree\" : moltree_batch\n",
    "               }\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cc103-3584-417b-9421-e319b0ef7b7d",
   "metadata": {},
   "source": [
    "## 혹시 이거 불러들여서?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14fab91e-57b9-4650-b8bd-5e5330b9b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae7448c3-3007-4ba7-bf04-573e21239442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_data(data_path, logger=None):\n",
    "    \"\"\"\n",
    "    Load data from the data_path.\n",
    "    :param data_path: the data_path.\n",
    "    :param logger: the logger.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    debug = logger.debug if logger is not None else print\n",
    "    summary_path = os.path.join(data_path, \"summary.txt\")\n",
    "    smiles_path = os.path.join(data_path, \"graph\")\n",
    "    feature_path = os.path.join(data_path, \"feature\")\n",
    "    moltree_path = os.path.join(data_path, \"moltrees\")\n",
    "\n",
    "    fin = open(summary_path)\n",
    "    n_files = int(fin.readline().strip().split(\":\")[-1])\n",
    "    n_samples = int(fin.readline().strip().split(\":\")[-1])\n",
    "    sample_per_file = int(fin.readline().strip().split(\":\")[-1])\n",
    "    debug(\"Loading data:\")\n",
    "    debug(\"Number of files: %d\" % n_files)\n",
    "    debug(\"Number of samples: %d\" % n_samples)\n",
    "    debug(\"Samples/file: %d\" % sample_per_file)\n",
    "\n",
    "    datapoints = []\n",
    "    for i in range(n_files):\n",
    "        smiles_path_i = os.path.join(smiles_path, str(i) + \".csv\")\n",
    "        feature_path_i = os.path.join(feature_path, str(i) + \".npz\")\n",
    "        moltree_path_i = os.path.join(moltree_path, str(i) + \".p\")\n",
    "        n_samples_i = sample_per_file if i != (n_files - 1) else n_samples % sample_per_file\n",
    "        datapoints.append(BatchDatapoint_motif(smiles_path_i, feature_path_i, moltree_path_i, n_samples_i))\n",
    "    return BatchMolDataset_motif(datapoints), sample_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eb6c208-48e9-44bd-898e-148495c2ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_motif(data,\n",
    "               split_type='random',\n",
    "               sizes=(0.8, 0.1, 0.1),\n",
    "               seed=0,\n",
    "               logger=None):\n",
    "    \"\"\"\n",
    "    Split data with given train/validation/test ratio.\n",
    "    :param data:\n",
    "    :param split_type:\n",
    "    :param sizes:\n",
    "    :param seed:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(sizes) == 3 and sum(sizes) == 1\n",
    "\n",
    "    if split_type == \"random\":\n",
    "        data.shuffle(seed=seed)\n",
    "        data = data.data\n",
    "\n",
    "        train_size = int(sizes[0] * len(data))\n",
    "        train_val_size = int((sizes[0] + sizes[1]) * len(data))\n",
    "\n",
    "        train = data[:train_size]\n",
    "        val = data[train_size:train_val_size]\n",
    "        test = data[train_val_size:]\n",
    "\n",
    "        return BatchMolDataset_motif(train), BatchMolDataset_motif(val), BatchMolDataset_motif(test)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Do not support %s splits\" % split_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d50aac7-9f74-485f-90a2-0682e61b9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GROVERMotifTrainer:\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 embedding_model: Module,\n",
    "                 topology_model: Module,\n",
    "                 atom_vocab_size: int,  # atom vocab size\n",
    "                 bond_vocab_size: int,\n",
    "                 fg_size: int,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 test_dataloader: DataLoader,\n",
    "                 optimizer_builder: Callable,\n",
    "                 scheduler_builder: Callable,\n",
    "                 logger: Logger = None,\n",
    "                 with_cuda: bool = False,\n",
    "                 enable_multi_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        The init function of GROVERTrainer\n",
    "        :param args: the input arguments.\n",
    "        :param embedding_model: the model to generate atom/bond embeddings.\n",
    "        :param topology_model : the model to predict topology of molecule from embeddings\n",
    "        :param atom_vocab_size: the vocabulary size of atoms.\n",
    "        :param bond_vocab_size: the vocabulary size of bonds.\n",
    "        :param fg_size: the size of semantic motifs (functional groups)\n",
    "        :param train_dataloader: the data loader of train data.\n",
    "        :param test_dataloader: the data loader of validation data.\n",
    "        :param optimizer_builder: the function of building the optimizer.\n",
    "        :param scheduler_builder: the function of building the scheduler.\n",
    "        :param logger: the logger\n",
    "        :param with_cuda: enable gpu training.\n",
    "        :param enable_multi_gpu: enable multi_gpu traning.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.with_cuda = with_cuda\n",
    "        self.grover = embedding_model\n",
    "        self.model = GroverMotifTask(args, embedding_model, atom_vocab_size, bond_vocab_size, fg_size)\n",
    "        self.motif_model = topology_model\n",
    "        self.loss_func = self.model.get_loss_func(args)\n",
    "        self.enable_multi_gpu = enable_multi_gpu\n",
    "\n",
    "        self.atom_vocab_size = atom_vocab_size\n",
    "        self.bond_vocab_size = bond_vocab_size\n",
    "        self.debug = logger.debug if logger is not None else print\n",
    "\n",
    "        if self.with_cuda:\n",
    "            # print(\"Using %d GPUs for training.\" % (torch.cuda.device_count()))\n",
    "            self.model = self.model.cuda()\n",
    "            self.motif_model = self.motif_model.cuda()\n",
    "\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        self.optimizer = optimizer_builder(self.model, self.args)\n",
    "        self.motif_optimizer = torch.optim.Adam(self.motif_model.parameters(), lr=args.init_lr, weight_decay=args.weight_decay)\n",
    "        self.scheduler = scheduler_builder(self.optimizer, self.args)\n",
    "        if self.enable_multi_gpu:\n",
    "            self.optimizer = mgw.DistributedOptimizer(self.optimizer,\n",
    "                                                      named_parameters=self.model.named_parameters())\n",
    "        self.args = args\n",
    "        self.n_iter = 0\n",
    "\n",
    "    def broadcast_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Broadcast parameters before training.\n",
    "        :return: no return.\n",
    "        \"\"\"\n",
    "        if self.enable_multi_gpu:\n",
    "            # broadcast parameters & optimizer state.\n",
    "            mgw.broadcast_parameters(self.model.state_dict(), root_rank=0)\n",
    "            mgw.broadcast_optimizer_state(self.optimizer, root_rank=0)\n",
    "\n",
    "    def train(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The training iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return: the loss terms of current epoch.\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.train_data, train=True)\n",
    "        return self.iter(epoch, self.train_data, train=True)\n",
    "\n",
    "    def test(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The test/validaiion iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return:  the loss terms as a list\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.test_data, train=False)\n",
    "        return self.iter(epoch, self.test_data, train=False)\n",
    "\n",
    "    def mock_iter(self, epoch: int, data_loader: DataLoader, train: bool = True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a mock iteration. For test only.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        for _, _ in enumerate(data_loader):\n",
    "            self.scheduler.step()\n",
    "        cum_loss_sum = 0.0\n",
    "        self.n_iter += self.args.batch_size\n",
    "        return self.n_iter, cum_loss_sum, (0, 0, 0, 0, 0, 0)\n",
    "\n",
    "    def iter(self, epoch, data_loader, train=True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a training / validation iteration.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            self.motif_model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            self.motif_model.eval()\n",
    "\n",
    "        loss_sum, iter_count = 0, 0\n",
    "        cum_loss_sum, cum_iter_count = 0, 0\n",
    "        av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum, bv_dist_loss_sum, fg_dist_loss_sum, node_loss_sum, topo_loss_sum = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "        topo_acc_avg, node_acc_avg = 0, 0\n",
    "        # loss_func = self.model.get_loss_func(self.args)\n",
    "\n",
    "        for _, item in enumerate(data_loader):\n",
    "            batch_graph = item[\"graph_input\"]\n",
    "            targets = item[\"targets\"]\n",
    "            \n",
    "            # add this for motif generation\n",
    "            moltree_paths = item[\"moltree\"]\n",
    "            \n",
    "            moltree = list()\n",
    "            for _, item in enumerate(moltree_paths):\n",
    "                moltree.append(load_moltree(item[0],item[1]))\n",
    "\n",
    "            if next(self.model.parameters()).is_cuda:\n",
    "                targets[\"av_task\"] = targets[\"av_task\"].cuda()\n",
    "                targets[\"bv_task\"] = targets[\"bv_task\"].cuda()\n",
    "                targets[\"fg_task\"] = targets[\"fg_task\"].cuda()\n",
    "            \n",
    "            preds = self.model(batch_graph)\n",
    "            emb_vector = preds['emb_vec']\n",
    "\n",
    "            # add this for motif generation\n",
    "            if self.args.embedding_output_type == 'atom':\n",
    "                emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "                emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "                \n",
    "                node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = self.motif_model(moltree, emb_afa_grouped)\n",
    "                node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = self.motif_model(moltree, emb_afb_grouped)\n",
    "                \n",
    "                node_loss = node_afa_loss + node_afb_loss\n",
    "                topo_loss = topo_afa_loss + topo_afb_loss\n",
    "                node_acc = (node_afa_acc + node_afb_acc)/2\n",
    "                topo_acc = (topo_afa_acc + topo_afb_acc)/2\n",
    "                \n",
    "            elif self.args.embedding_output_type == 'bond':\n",
    "                emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "                emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "                \n",
    "                node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = self.motif_model(moltree, emb_bfa_grouped)\n",
    "                node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = self.motif_model(moltree, emb_bfb_grouped)\n",
    "                \n",
    "                node_loss = node_bfa_loss + node_bfb_loss\n",
    "                topo_loss = topo_bfa_loss + topo_bfb_loss\n",
    "                node_acc = (node_bfa_acc + node_bfb_acc)/2\n",
    "                topo_acc = (topo_bfa_acc + topo_bfb_acc)/2\n",
    "                \n",
    "            elif self.args.embedding_output_type == \"both\":\n",
    "                emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "                emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "                emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "                emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "                \n",
    "                node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = self.motif_model(moltree, emb_afa_grouped)\n",
    "                node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = self.motif_model(moltree, emb_afb_grouped)\n",
    "                node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = self.motif_model(moltree, emb_bfa_grouped)\n",
    "                node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = self.motif_model(moltree, emb_bfb_grouped)\n",
    "                \n",
    "                node_loss = node_afa_loss + node_afb_loss + node_bfa_loss + node_bfb_loss\n",
    "                topo_loss = topo_afa_loss + topo_afb_loss + topo_bfa_loss + topo_bfb_loss\n",
    "                node_acc = (node_afa_acc + node_afb_acc + node_bfa_acc + node_bfb_acc)/4\n",
    "                topo_acc = (topo_afa_acc + topo_afb_acc + topo_bfa_acc + topo_bfb_acc)/4\n",
    "\n",
    "            # # ad-hoc code, for visualizing a model, comment this block when it is not needed\n",
    "            # import dglt.contrib.grover.vis_model as vis_model\n",
    "            # for task in ['av_task', 'bv_task', 'fg_task']:\n",
    "            #     vis_graph = vis_model.make_dot(self.model(batch_graph)[task],\n",
    "            #                                    params=dict(self.model.named_parameters()))\n",
    "            #     # vis_graph.view()\n",
    "            #     vis_graph.render(f\"{self.args.backbone}_model_{task}_vis.png\", format=\"png\")\n",
    "            # exit()\n",
    "\n",
    "            loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss = self.loss_func(preds, targets)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            iter_count += self.args.batch_size\n",
    "            \n",
    "            # add for topology loss\n",
    "            loss += topo_loss\n",
    "            loss += node_loss\n",
    "            topo_loss_sum += topo_loss.item()\n",
    "            node_loss_sum += node_loss.item()\n",
    "\n",
    "            if train:\n",
    "                cum_loss_sum += loss.item()\n",
    "                # Run model\n",
    "                self.model.zero_grad()\n",
    "                self.motif_model.zero_grad()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.motif_optimizer.step()\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                # For eval model, only consider the loss of three task.\n",
    "                cum_loss_sum += av_loss.item()\n",
    "                cum_loss_sum += bv_loss.item()\n",
    "                cum_loss_sum += fg_loss.item()\n",
    "\n",
    "            av_loss_sum += av_loss.item()\n",
    "            bv_loss_sum += bv_loss.item()\n",
    "            fg_loss_sum += fg_loss.item()\n",
    "            av_dist_loss_sum += av_dist_loss.item() if type(av_dist_loss) != float else av_dist_loss\n",
    "            bv_dist_loss_sum += bv_dist_loss.item() if type(bv_dist_loss) != float else bv_dist_loss\n",
    "            fg_dist_loss_sum += fg_dist_loss.item() if type(fg_dist_loss) != float else fg_dist_loss\n",
    "\n",
    "            cum_iter_count += 1\n",
    "            self.n_iter += self.args.batch_size\n",
    "\n",
    "            # Debug only.\n",
    "            # if i % 50 == 0:\n",
    "            #     print(f\"epoch: {epoch}, batch_id: {i}, av_loss: {av_loss}, bv_loss: {bv_loss}, \"\n",
    "            #           f\"fg_loss: {fg_loss}, av_dist_loss: {av_dist_loss}, bv_dist_loss: {bv_dist_loss}, \"\n",
    "            #           f\"fg_dist_loss: {fg_dist_loss}\")\n",
    "\n",
    "        cum_loss_sum /= cum_iter_count\n",
    "        av_loss_sum /= cum_iter_count\n",
    "        bv_loss_sum /= cum_iter_count\n",
    "        fg_loss_sum /= cum_iter_count\n",
    "        av_dist_loss_sum /= cum_iter_count\n",
    "        bv_dist_loss_sum /= cum_iter_count\n",
    "        fg_dist_loss_sum /= cum_iter_count\n",
    "        \n",
    "        topo_loss_sum /= cum_iter_count\n",
    "        node_loss_sum /= cum_iter_count\n",
    "\n",
    "        return self.n_iter, cum_loss_sum, (av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum,\n",
    "                                           bv_dist_loss_sum, fg_dist_loss_sum, topo_loss_sum, node_loss_sum, topo_acc, node_acc)\n",
    "\n",
    "    def save(self, epoch, file_path, name=None) -> str:\n",
    "        \"\"\"\n",
    "        Save the intermediate models during training.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to save the model.\n",
    "        :return: the output path.\n",
    "        \"\"\"\n",
    "        # add specific time in model fine name, in order to distinguish different saved models\n",
    "        now = time.localtime()\n",
    "        if name is None:\n",
    "            name = \"_%04d_%02d_%02d_%02d_%02d_%02d\" % (\n",
    "                now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "        output_path = file_path + name + \".ep%d\" % epoch\n",
    "        scaler = None\n",
    "        features_scaler = None\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch,\n",
    "            'data_scaler': {\n",
    "                'means': scaler.means,\n",
    "                'stds': scaler.stds\n",
    "            } if scaler is not None else None,\n",
    "            'features_scaler': {\n",
    "                'means': features_scaler.means,\n",
    "                'stds': features_scaler.stds\n",
    "            } if features_scaler is not None else None\n",
    "        }\n",
    "        torch.save(state, output_path)\n",
    "\n",
    "        # Is this necessary?\n",
    "        # if self.with_cuda:\n",
    "        #    self.model = self.model.cuda()\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "\n",
    "    def save_tmp(self, epoch, file_path, rank=0):\n",
    "        \"\"\"\n",
    "        Save the models for auto-restore during training.\n",
    "        The model are stored in file_path/tmp folder and will replaced on each epoch.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        store_path = os.path.join(file_path, \"tmp\")\n",
    "        if not os.path.exists(store_path):\n",
    "            os.makedirs(store_path, exist_ok=True)\n",
    "        store_path = os.path.join(store_path, \"model.%d\" % rank)\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        torch.save(state, store_path)\n",
    "\n",
    "    def restore(self, file_path, rank=0) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Restore the training state saved by save_tmp.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return: the restored epoch number and the scheduler_step in scheduler.\n",
    "        \"\"\"\n",
    "        cpt_path = os.path.join(file_path, \"tmp\", \"model.%d\" % rank)\n",
    "        if not os.path.exists(cpt_path):\n",
    "            print(\"No checkpoint found %d\")\n",
    "            return 0, 0\n",
    "        cpt = torch.load(cpt_path)\n",
    "        self.model.load_state_dict(cpt[\"state_dict\"])\n",
    "        self.optimizer.load_state_dict(cpt[\"optimizer\"])\n",
    "        epoch = cpt[\"epoch\"]\n",
    "        scheduler_step = cpt[\"scheduler_step\"]\n",
    "        self.scheduler.current_step = scheduler_step\n",
    "        print(\"Restore checkpoint, current epoch: %d\" % (epoch))\n",
    "        return epoch, scheduler_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305dee55-3b44-454a-9f47-d2dc5363772b",
   "metadata": {},
   "source": [
    "# 실험장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d3ac4bd-2bb8-4658-9777-6cfc0b09f087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading data\n",
      "Loading data\n",
      "Loading data\n",
      "Splitting data with seed 0.\n",
      "Splitting data with seed 0.\n",
      "Splitting data with seed 0.\n",
      "Splitting data with seed 0.\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation='PReLU', atom_vocab_path='data/zinc10M/zinc10M_atom_vocab.pkl', backbone='gtrans', batch_size=100, bias=False, bond_drop_rate=0, bond_vocab_path='data/zinc10M/zinc10M_bond_vocab.pkl', cuda=True, data_path='data/zinc10M_0', dense=False, depth=3, dist_coff=0.1, dropout=0.1, embedding_output_type='both', enable_multi_gpu=False, epochs=20, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=1200, init_lr=0.0002, max_lr=0.0004, motif_hidden_size=1200, motif_latent_size=56, motif_order='dfs', motif_vocab_path='data/zinc10M/clique.txt', no_cache=True, num_attn_head=4, num_mt_block=1, parser_name='pretrain', save_dir='model/zinc10M_0', save_interval=5, topology=True, undirected=False, wandb=False, wandb_name='pretrain', warmup_epochs=2.0, weight_decay=1e-07)\n",
      "Loading data:\n",
      "Number of files: 501\n",
      "Number of samples: 500000\n",
      "Samples/file: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n",
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n",
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n",
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n"
     ]
    }
   ],
   "source": [
    "if logger is not None:\n",
    "    debug, _ = logger.debug, logger.info\n",
    "else:\n",
    "    debug = print\n",
    "\n",
    "# initialize the horovod library\n",
    "if args.enable_multi_gpu:\n",
    "    mgw.init()\n",
    "\n",
    "# binding training to GPUs.\n",
    "master_worker = (mgw.rank() == 0) if args.enable_multi_gpu else True\n",
    "# pin GPU to local rank. By default, we use gpu:0 for training.\n",
    "local_gpu_idx = mgw.local_rank() if args.enable_multi_gpu else 0\n",
    "with_cuda = args.cuda\n",
    "if with_cuda:\n",
    "    torch.cuda.set_device(local_gpu_idx)\n",
    "\n",
    "# get rank an  number of workers\n",
    "rank = mgw.rank() if args.enable_multi_gpu else 0\n",
    "num_replicas = mgw.size() if args.enable_multi_gpu else 1\n",
    "# print(\"Rank: %d Rep: %d\" % (rank, num_replicas))\n",
    "\n",
    "# load file paths of the data.\n",
    "if master_worker:\n",
    "    print(args)\n",
    "    if args.enable_multi_gpu:\n",
    "        debug(\"Total workers: %d\" % (mgw.size()))\n",
    "    debug('Loading data')\n",
    "data, sample_per_file = get_motif_data(data_path=args.data_path)\n",
    "\n",
    "# data splitting\n",
    "if master_worker:\n",
    "    debug(f'Splitting data with seed 0.')\n",
    "train_data, test_data, _ = split_data_motif(data=data, sizes=(0.9, 0.1, 0.0), seed=0, logger=logger)\n",
    "\n",
    "# Here the true train data size is the train_data divided by #GPUs\n",
    "if args.enable_multi_gpu:\n",
    "    args.train_data_size = len(train_data) // mgw.size()\n",
    "else:\n",
    "    args.train_data_size = len(train_data)\n",
    "if master_worker:\n",
    "    debug(f'Total size = {len(data):,} | '\n",
    "          f'train size = {len(train_data):,} | val size = {len(test_data):,}')\n",
    "\n",
    "# load atom and bond vocabulary and the semantic motif labels.\n",
    "atom_vocab = MolVocab.load_vocab(args.atom_vocab_path)\n",
    "bond_vocab = MolVocab.load_vocab(args.bond_vocab_path)\n",
    "atom_vocab_size, bond_vocab_size = len(atom_vocab), len(bond_vocab)\n",
    "\n",
    "# Load motif vocabulary for pretrain\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "motif_vocab = [x.strip(\"\\r\\n \") for x in open(args.motif_vocab_path)]\n",
    "motif_vocab = Motif_Vocab(motif_vocab)\n",
    "\n",
    "# Hard coding here, since we haven't load any data yet!\n",
    "fg_size = 85\n",
    "shared_dict = {}\n",
    "motif_collator = GroverMotifCollator(shared_dict=shared_dict, atom_vocab=atom_vocab, bond_vocab=bond_vocab, args=args)\n",
    "if master_worker:\n",
    "    debug(\"atom vocab size: %d, bond vocab size: %d, Number of FG tasks: %d\" % (atom_vocab_size,\n",
    "                                                                                bond_vocab_size, fg_size))\n",
    "\n",
    "# Define the distributed sampler. If using the single card, the sampler will be None.\n",
    "train_sampler = None\n",
    "test_sampler = None\n",
    "shuffle = True\n",
    "if args.enable_multi_gpu:\n",
    "    # If not shuffle, the performance may decayed.\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=True, sample_per_file=sample_per_file)\n",
    "    # Here sample_per_file in test_sampler is None, indicating the test sampler would not divide the test samples by\n",
    "    # rank. (TODO: bad design here.)\n",
    "    test_sampler = DistributedSampler(\n",
    "        test_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=False)\n",
    "    train_sampler.set_epoch(args.epochs)\n",
    "    test_sampler.set_epoch(1)\n",
    "    # if we enables multi_gpu training. shuffle should be disabled.\n",
    "    shuffle = False\n",
    "    \n",
    "    # Pre load data. (Maybe unnecessary. )\n",
    "pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
    "pre_load_data(test_data, rank, num_replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d33eb89-aa0b-4831-8e9a-089910a75798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found %d\n"
     ]
    }
   ],
   "source": [
    "# Build dataloader\n",
    "train_data_dl = DataLoader(train_data,\n",
    "                           batch_size=150,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=10,\n",
    "                           sampler=train_sampler,\n",
    "                           collate_fn=motif_collator)\n",
    "test_data_dl = DataLoader(test_data,\n",
    "                          batch_size=150,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=10,\n",
    "                          sampler=test_sampler,\n",
    "                          collate_fn=motif_collator)\n",
    "\n",
    "# Build the embedding model.\n",
    "grover_model = GROVEREmbedding(args)\n",
    "\n",
    "# build the topology predict model.\n",
    "motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order)\n",
    "\n",
    "#  Build the trainer.\n",
    "trainer = GROVERMotifTrainer(args=args,\n",
    "                        embedding_model=grover_model,\n",
    "                        topology_model=motif_model,\n",
    "                        atom_vocab_size=atom_vocab_size,\n",
    "                        bond_vocab_size=bond_vocab_size,\n",
    "                        fg_size=fg_size,\n",
    "                        train_dataloader=train_data_dl,\n",
    "                        test_dataloader=test_data_dl,\n",
    "                        optimizer_builder=build_optimizer,\n",
    "                        scheduler_builder=build_lr_scheduler,\n",
    "                        logger=logger,\n",
    "                        with_cuda=with_cuda,\n",
    "                        enable_multi_gpu=args.enable_multi_gpu)\n",
    "\n",
    "# Restore the interrupted training.\n",
    "model_dir = os.path.join(args.save_dir, \"model\")\n",
    "resume_from_epoch = 0\n",
    "resume_scheduler_step = 0\n",
    "if master_worker:\n",
    "    resume_from_epoch, resume_scheduler_step = trainer.restore(model_dir)\n",
    "if args.enable_multi_gpu:\n",
    "    resume_from_epoch = mgw.broadcast(torch.tensor(resume_from_epoch), root_rank=0, name=\"resume_from_epoch\").item()\n",
    "    resume_scheduler_step = mgw.broadcast(torch.tensor(resume_scheduler_step),\n",
    "                                          root_rank=0, name=\"resume_scheduler_step\").item()\n",
    "    trainer.scheduler.current_step = resume_scheduler_step\n",
    "    print(\"Restored epoch: %d Restored scheduler step: %d\" % (resume_from_epoch, trainer.scheduler.current_step))\n",
    "trainer.broadcast_parameters()\n",
    "\n",
    "# Print model details.\n",
    "#if master_worker:\n",
    "    # Change order here.\n",
    "    #print(grover_model)\n",
    "    #print(\"Total parameters: %d\" % param_count(trainer.grover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da42b855-a935-4d88-abf1-92b7295a30c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mppn0303\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/grover/wandb/run-20230311_140851-riselb0y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ppn0303/load_after_loader/runs/riselb0y' target=\"_blank\">vital-eon-1</a></strong> to <a href='https://wandb.ai/ppn0303/load_after_loader' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ppn0303/load_after_loader' target=\"_blank\">https://wandb.ai/ppn0303/load_after_loader</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ppn0303/load_after_loader/runs/riselb0y' target=\"_blank\">https://wandb.ai/ppn0303/load_after_loader/runs/riselb0y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #wandb\n",
    "wandb.init(project='load_after_loader')\n",
    "wandb.config = args\n",
    "wandb.watch(grover_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a42898f-67db-472a-bf9f-d93751f4bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_moltree(path: str, index: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loads features saved in a variety of formats.\n",
    "\n",
    "    Supported formats:\n",
    "    - .npz compressed (assumes features are saved with name \"features\")\n",
    "\n",
    "    All formats assume that the SMILES strings loaded elsewhere in the code are in the same\n",
    "    order as the features loaded here.\n",
    "\n",
    "    :param path: Path to a file containing features.\n",
    "    :return: A 2D numpy array of size (num_molecules, features_size) containing the features.\n",
    "    \"\"\"\n",
    "    extension = os.path.splitext(path)[1]\n",
    "\n",
    "    with open(path, 'rb') as f:\n",
    "        moltrees = pickle.load(f)[index]\n",
    "    f.close()\n",
    "    return moltrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad8460ae-0383-4f7f-a710-4d939cfdfdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.592586278915405\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "moltree = []\n",
    "for _, item in enumerate(moltree_paths):\n",
    "    moltree.append(load_moltrees(item[0],item[1]))\n",
    "dtime = time.time()\n",
    "print(dtime-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b22cf10-e7f7-4853-9bd6-0bf8d9ff535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.522074699401855\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "moltree = list(0 for i in range(len(moltree_paths)))\n",
    "for _, item in enumerate(moltree_paths):\n",
    "    moltree.append(load_moltrees(item[0],item[1]))\n",
    "dtime = time.time()\n",
    "print(dtime-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "982c7136-378f-4047-b80b-e3a9b5c4cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df278864-93af-4c46-93b5-0b54dc07a71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before train : 5779.71 MiB\n",
      "epochs start memory is 5779.71 MiB\n"
     ]
    }
   ],
   "source": [
    "# Perform training.\n",
    "best_val_loss = 0\n",
    "best_val_epoch = 0\n",
    "best_model_dir = os.path.join(args.save_dir, \"model_best\")\n",
    "print(f\"before train : {memory_usage()[0]:.2f} MiB\")\n",
    "for epoch in range(resume_from_epoch + 1, args.epochs):\n",
    "    s_time = time.time()\n",
    "    print(f\"epochs start memory is {memory_usage()[0]:.2f} MiB\")\n",
    "    # Data pre-loading.\n",
    "    if args.enable_multi_gpu:\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        train_data.clean_cache()\n",
    "        idxs = train_sampler.get_indices()\n",
    "        for local_gpu_idx in idxs:\n",
    "            train_data.load_data(local_gpu_idx)\n",
    "    \n",
    "    \n",
    "\n",
    "    d_time = time.time() - s_time\n",
    "    \n",
    "\n",
    "    # perform training and validation.\n",
    "    s_time = time.time()\n",
    "    _, train_loss, _ = trainer.train(epoch)\n",
    "    print(f\"after train memory is {memory_usage()[0]:.2f} MiB\")\n",
    "    t_time = time.time() - s_time\n",
    "    s_time = time.time()\n",
    "    _, val_loss, detailed_loss_val = trainer.test(epoch)\n",
    "    print(f\"after validation memory is {memory_usage()[0]:.2f} MiB\")\n",
    "    val_av_loss, val_bv_loss, val_fg_loss, _, _, _, val_topo_loss, val_node_loss, topo_acc, node_acc = detailed_loss_val\n",
    "    v_time = time.time() - s_time\n",
    "\n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_epoch = epoch\n",
    "        trainer.save(epoch, best_model_dir)\n",
    "\n",
    "    if args.wandb :         \n",
    "        wandb.log({\"train_loss\" : train_loss, \"val_loss\" : val_loss, \"topo_loss\" : val_topo_loss})\n",
    "\n",
    "    # print information.\n",
    "    if master_worker:\n",
    "        print('Epoch: {:04d}'.format(epoch),\n",
    "              'loss_train: {:.6f}'.format(train_loss),\n",
    "              'loss_val: {:.6f}'.format(val_loss),\n",
    "              'loss_val_av: {:.6f}'.format(val_av_loss),\n",
    "              'loss_val_bv: {:.6f}'.format(val_bv_loss),\n",
    "              'loss_val_fg: {:.6f}'.format(val_fg_loss),\n",
    "              'loss_val_topo: {:.6f}'.format(val_topo_loss),\n",
    "              'loss_val_node: {:.6f}'.format(val_node_loss),\n",
    "              'acc_topo: {:.6f}'.format(topo_acc),\n",
    "              'acc_node: {:.6f}'.format(node_acc),\n",
    "              'cur_lr: {:.5f}'.format(trainer.scheduler.get_lr()[0]),\n",
    "              't_time: {:.4f}s'.format(t_time),\n",
    "              'v_time: {:.4f}s'.format(v_time),\n",
    "              'd_time: {:.4f}s'.format(d_time), flush=True)\n",
    "\n",
    "\n",
    "        if epoch % args.save_interval == 0:\n",
    "            trainer.save(epoch, model_dir)\n",
    "\n",
    "\n",
    "        trainer.save_tmp(epoch, model_dir, rank)\n",
    "        print(f\"after save cp memory is {memory_usage()[0]:.2f} MiB\")\n",
    "\n",
    "# Only save final version.\n",
    "if master_worker:\n",
    "    trainer.save(args.epochs, model_dir, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e01f463-950a-4cce-b41e-500c38b0ae82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<grover.topology.mol_tree.MolTree at 0x7f7701cd5490>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_moltrees(item[0],item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4402f9c4-b735-4d85-bd2b-a341a456945f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom_from_atom': tensor([[ 0.5286,  0.1908,  0.1559,  ...,  0.0000,  0.2753, -1.2593],\n",
       "         [ 0.3252, -0.7777,  0.7843,  ..., -0.4200, -0.5239,  0.0000],\n",
       "         [ 0.8270, -0.7058,  1.4171,  ...,  0.0000, -0.8003, -1.1189],\n",
       "         ...,\n",
       "         [-0.0897, -0.7822,  0.4744,  ...,  0.5085,  0.0036, -0.2264],\n",
       "         [ 0.0317, -0.1056,  0.0000,  ...,  0.8393, -0.3412, -1.7130],\n",
       "         [ 0.1424,  0.0848,  0.7505,  ..., -0.0120, -0.7686, -0.9019]],\n",
       "        device='cuda:0', grad_fn=<FusedDropoutBackward>),\n",
       " 'bond_from_atom': tensor([[-0.8544,  1.2712,  1.2164,  ...,  0.0085,  1.2820,  0.0000],\n",
       "         [-0.3456,  0.9824,  0.7136,  ..., -0.1406,  1.1398,  0.0440],\n",
       "         [-1.0875,  0.2222,  0.0810,  ...,  0.0000,  0.1001, -0.5310],\n",
       "         ...,\n",
       "         [-0.6528,  0.9574,  1.1351,  ...,  0.6902,  1.4957, -2.3826],\n",
       "         [-1.0220,  0.9658,  0.4182,  ..., -0.7550,  0.5864, -1.0845],\n",
       "         [-1.2348,  1.4471,  1.8378,  ..., -0.2258,  0.9261, -1.6320]],\n",
       "        device='cuda:0', grad_fn=<FusedDropoutBackward>),\n",
       " 'atom_from_bond': tensor([[-1.7643,  0.6969, -1.0123,  ..., -0.7660, -0.8949,  0.0000],\n",
       "         [ 0.0000,  0.1532, -0.7414,  ..., -0.9163, -0.4130, -0.6147],\n",
       "         [-2.0566, -0.0820,  0.0000,  ..., -1.2234,  0.0000,  0.3236],\n",
       "         ...,\n",
       "         [-1.5876,  0.2523,  0.0000,  ...,  0.0000, -0.1168, -0.9727],\n",
       "         [-1.1891,  0.5603, -0.3692,  ..., -1.2241, -0.7257, -1.2921],\n",
       "         [-1.9694,  0.0000,  0.3959,  ..., -2.3634, -1.0931, -0.4333]],\n",
       "        device='cuda:0', grad_fn=<FusedDropoutBackward>),\n",
       " 'bond_from_bond': tensor([[ 2.5196,  0.2655, -0.6023,  ..., -0.6310,  1.2594,  1.3628],\n",
       "         [ 2.0130,  0.1514,  0.3598,  ..., -0.9702,  1.6945,  1.9319],\n",
       "         [ 1.7697,  0.7841,  0.7211,  ..., -0.8707, -1.1531,  1.8456],\n",
       "         ...,\n",
       "         [ 2.0334,  0.5090,  0.1859,  ..., -1.4931,  1.0023,  2.2143],\n",
       "         [ 1.0938,  1.1248,  0.1433,  ..., -0.6541,  0.6077,  2.0760],\n",
       "         [ 0.8495,  1.2360, -0.3523,  ..., -1.5887,  1.2687,  1.8391]],\n",
       "        device='cuda:0', grad_fn=<FusedDropoutBackward>)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c5fc3f4-1faa-43cc-80c2-4c6df1914134",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8881/407896911.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0memb_bfb_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_node_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoltree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bond_from_bond'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mnode_afa_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_afa_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_afa_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_afa_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotif_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoltree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_afa_grouped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mnode_afb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_afb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_afb_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_afb_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotif_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoltree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_afb_grouped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnode_bfa_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_bfa_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_bfa_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_bfa_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotif_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoltree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_bfa_grouped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "loss_sum, iter_count = 0, 0\n",
    "cum_loss_sum, cum_iter_count = 0, 0\n",
    "av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum, bv_dist_loss_sum, fg_dist_loss_sum, node_loss_sum, topo_loss_sum = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "topo_acc_avg, node_acc_avg = 0, 0\n",
    "# loss_func = self.model.get_loss_func(self.args)\n",
    "\n",
    "for _, item in enumerate(train_data_dl):\n",
    "    batch_graph = item[\"graph_input\"]\n",
    "    targets = item[\"targets\"]\n",
    "\n",
    "    # add this for motif generation\n",
    "    moltree_paths = item[\"moltree\"]\n",
    "\n",
    "    moltree = list()\n",
    "    for _, item in enumerate(moltree_paths):\n",
    "        moltree.append(load_moltrees(item[0],item[1]))\n",
    "\n",
    "\n",
    "    targets[\"av_task\"] = targets[\"av_task\"].cuda()\n",
    "    targets[\"bv_task\"] = targets[\"bv_task\"].cuda()\n",
    "    targets[\"fg_task\"] = targets[\"fg_task\"].cuda()\n",
    "\n",
    "    preds = grover_model(batch_graph)\n",
    "    emb_vector = preds#['emb_vec']\n",
    "\n",
    "    emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "    emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "    emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "    emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "\n",
    "    node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = motif_model(moltree, emb_afa_grouped)\n",
    "    node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = motif_model(moltree, emb_afb_grouped)\n",
    "    node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = motif_model(moltree, emb_bfa_grouped)\n",
    "    node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = motif_model(moltree, emb_bfb_grouped)\n",
    "\n",
    "    node_loss = node_afa_loss + node_afb_loss + node_bfa_loss + node_bfb_loss\n",
    "    topo_loss = topo_afa_loss + topo_afb_loss + topo_bfa_loss + topo_bfb_loss\n",
    "    node_acc = (node_afa_acc + node_afb_acc + node_bfa_acc + node_bfb_acc)/4\n",
    "    topo_acc = (topo_afa_acc + topo_afb_acc + topo_bfa_acc + topo_bfb_acc)/4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbb5c2-a79d-4e4a-8e1e-e99827386920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_model(args: Namespace, logger: Logger = None):\n",
    "    \"\"\"\n",
    "    The entrey of pretrain.\n",
    "    :param args: the argument.\n",
    "    :param logger: the logger.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # avoid auto optimized import by pycharm.\n",
    "    a = MolVocab\n",
    "    s_time = time.time()\n",
    "    if args.topology : \n",
    "        run_motif_training(args=args, logger=logger)\n",
    "    else : \n",
    "        run_training(args=args, logger=logger)\n",
    "    e_time = time.time()\n",
    "    print(\"Total Time: %.3f\" % (e_time - s_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "835e9bea-506a-41db-9ecf-1aee0bca1813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading data\n",
      "Splitting data with seed 0.\n",
      "Splitting data with seed 0.\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation='PReLU', atom_vocab_path='data/zinc10M/zinc10M_atom_vocab.pkl', backbone='gtrans', batch_size=100, bias=False, bond_drop_rate=0, bond_vocab_path='data/zinc10M/zinc10M_bond_vocab.pkl', cuda=True, data_path='data/zinc10M_0', dense=False, depth=3, dist_coff=0.1, dropout=0.1, embedding_output_type='both', enable_multi_gpu=False, epochs=20, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=1200, init_lr=0.0002, max_lr=0.0004, motif_hidden_size=1200, motif_latent_size=56, motif_order='dfs', motif_vocab_path='data/zinc10M/clique.txt', no_cache=True, num_attn_head=4, num_mt_block=1, parser_name='pretrain', save_dir='model/ChEMBL', save_interval=5, topology=True, train_data_size=450000, undirected=False, wandb=False, wandb_name='pretrain', warmup_epochs=2.0, weight_decay=1e-07)\n",
      "Loading data:\n",
      "Number of files: 501\n",
      "Number of samples: 500000\n",
      "Samples/file: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n",
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f71212227a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f712146d320>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/weakref.py\", line 109, in remove\n",
      "    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4023/3879309671.py\", line 3, in <module>\n",
      "    pretrain_model(args, logger)\n",
      "  File \"/root/grover/task/pretrain.py\", line 37, in pretrain_model\n",
      "    run_motif_training(args=args, logger=logger)\n",
      "  File \"/root/grover/task/pretrain.py\", line 339, in run_motif_training\n",
      "    pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
      "  File \"/root/grover/task/pretrain.py\", line 60, in pre_load_data\n",
      "    dataset.load_data(i)\n",
      "  File \"/root/grover/grover/data/groverdataset.py\", line 163, in load_data\n",
      "    def __init__(self, data: List[BatchDatapoint],\n",
      "  File \"/root/grover/grover/data/groverdataset.py\", line 299, in load_datapoints\n",
      "    moltrees = self.load_moltree()\n",
      "  File \"/root/grover/grover/data/groverdataset.py\", line 319, in load_moltree\n",
      "    return feautils.load_moltrees(self.moltree_file)\n",
      "  File \"/root/grover/grover/util/utils.py\", line 92, in load_moltrees\n",
      "    moltrees = pickle.load(f)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1464, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 735, in getmodule\n",
      "    if f == _filesbymodname.get(modname, None):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4023/3879309671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pretrain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpretrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/grover/task/pretrain.py\u001b[0m in \u001b[0;36mpretrain_model\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mrun_motif_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/task/pretrain.py\u001b[0m in \u001b[0;36mrun_motif_training\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;31m# Pre load data. (Maybe unnecessary. )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0mpre_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_replicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_per_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0mpre_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_replicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/task/pretrain.py\u001b[0m in \u001b[0;36mpre_load_data\u001b[0;34m(dataset, rank, num_replicas, sample_per_file, epoch)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpre_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/data/groverdataset.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBatchMolDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     def __init__(self, data: List[BatchDatapoint],\n\u001b[0m\u001b[1;32m    164\u001b[0m                  graph_per_file=None):\n",
      "\u001b[0;32m~/grover/grover/data/groverdataset.py\u001b[0m in \u001b[0;36mload_datapoints\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mmoltrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_moltree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/data/groverdataset.py\u001b[0m in \u001b[0;36mload_moltree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_moltree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeautils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_moltrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoltree_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/util/utils.py\u001b[0m in \u001b[0;36mload_moltrees\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mmoltrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2076\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2080\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "if args.parser_name == 'pretrain':\n",
    "    logger = create_logger(name='pretrain', save_dir=args.save_dir)\n",
    "    pretrain_model(args, logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf0120-5401-4349-a194-9e9782bba7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
