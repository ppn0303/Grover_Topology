{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72308b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from grover.util.parsing import parse_args, get_newest_train_args\n",
    "from grover.util.utils import create_logger\n",
    "from task.cross_validate import cross_validate, randomsearch, gridsearch, make_confusion_matrix\n",
    "from task.fingerprint import generate_fingerprints\n",
    "from task.predict import make_predictions, write_prediction\n",
    "from task.pretrain import pretrain_model\n",
    "from grover.data.torchvocab import MolVocab\n",
    "\n",
    "#add for gridsearch\n",
    "from argparse import ArgumentParser, Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92cf93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(seed):\n",
    "    # frozen random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe52fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup random seed\n",
    "setup(seed=42)\n",
    "# Avoid the pylint warning.\n",
    "a = MolVocab\n",
    "# supress rdkit logger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# Initialize MolVocab\n",
    "mol_vocab = MolVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9188265",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. save_features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bf952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from multiprocessing import Pool\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from grover.util.utils import get_data, makedirs, load_features, save_features\n",
    "from grover.data.molfeaturegenerator import get_available_features_generators, \\\n",
    "    get_features_generator\n",
    "from grover.data.task_labels import rdkit_functional_group_label_features_generator\n",
    "\n",
    "def load_temp(temp_dir: str) -> Tuple[List[List[float]], int]:\n",
    "    \"\"\"\n",
    "    Loads all features saved as .npz files in load_dir.\n",
    "\n",
    "    Assumes temporary files are named in order 0.npz, 1.npz, ...\n",
    "\n",
    "    :param temp_dir: Directory in which temporary .npz files containing features are stored.\n",
    "    :return: A tuple with a list of molecule features, where each molecule's features is a list of floats,\n",
    "    and the number of temporary files.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    temp_num = 0\n",
    "    temp_path = os.path.join(temp_dir, f'{temp_num}.npz')\n",
    "\n",
    "    while os.path.exists(temp_path):\n",
    "        features.extend(load_features(temp_path))\n",
    "        temp_num += 1\n",
    "        temp_path = os.path.join(temp_dir, f'{temp_num}.npz')\n",
    "\n",
    "    return features, temp_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6e8e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='data/CO2.csv', features_generator='fgtasklabel', max_data_size=None, restart=True, save_frequency=10000, save_path='data/CO2.npz', sequential=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--data_path', type=str, required=True,\n",
    "                    help='Path to data CSV')\n",
    "parser.add_argument('--features_generator', type=str, required=True,\n",
    "                    choices=get_available_features_generators(),\n",
    "                    help='Type of features to generate')\n",
    "parser.add_argument('--save_path', type=str, default=None,\n",
    "                    help='Path to .npz file where features will be saved as a compressed numpy archive')\n",
    "parser.add_argument('--save_frequency', type=int, default=10000,\n",
    "                    help='Frequency with which to save the features')\n",
    "parser.add_argument('--restart', action='store_true', default=False,\n",
    "                    help='Whether to not load partially complete featurization and instead start from scratch')\n",
    "parser.add_argument('--max_data_size', type=int,\n",
    "                    help='Maximum number of data points to load')\n",
    "parser.add_argument('--sequential', action='store_true', default=False,\n",
    "                    help='Whether to task sequentially rather than in parallel')\n",
    "args = parser.parse_args(['--data_path','data/testfiles/mgssl.csv','--save_path', 'data/pretrain/mgssl/CO2.npz', '--features_generator','fgtasklabel','--restart'])\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90295f8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 0-1. generate_and_save_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521af09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0-1-1. fgtasklabel\n",
    "- RDKIT을 통해 smiles에서 찾을 수 있는 FG(모티프)들을 feature로써 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0e9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Callable, Union\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from descriptastorus.descriptors import rdDescriptors\n",
    "\n",
    "from grover.data.molfeaturegenerator import register_features_generator\n",
    "\n",
    "Molecule = Union[str, Chem.Mol]\n",
    "FeaturesGenerator = Callable[[Molecule], np.ndarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20aac514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functional group descriptors in RDkit.\n",
    "RDKIT_PROPS = ['fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN',\n",
    "               'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2',\n",
    "               'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0',\n",
    "               'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2',\n",
    "               'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide',\n",
    "               'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl',\n",
    "               'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine',\n",
    "               'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester',\n",
    "               'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone',\n",
    "               'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone',\n",
    "               'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine',\n",
    "               'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho',\n",
    "               'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol',\n",
    "               'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine',\n",
    "               'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN',\n",
    "               'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole',\n",
    "               'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef2a7eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RDKIT_PROPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce199c65",
   "metadata": {},
   "source": [
    "### 0-1-1-1 rdkit_fg_label_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ff7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_features_generator('fgtasklabel')\n",
    "def rdkit_functional_group_label_features_generator(mol: Molecule) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates functional group label for a molecule using RDKit.\n",
    "\n",
    "    :param mol: A molecule (i.e. either a SMILES string or an RDKit molecule).\n",
    "    :return: A 1D numpy array containing the RDKit 2D features.\n",
    "    \"\"\"\n",
    "    smiles = Chem.MolToSmiles(mol, isomericSmiles=True) if type(mol) != str else mol\n",
    "    generator = rdDescriptors.RDKit2D(RDKIT_PROPS)\n",
    "    features = generator.process(smiles)[1:]\n",
    "    features = np.array(features)\n",
    "    features[features != 0] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273e621",
   "metadata": {},
   "source": [
    "### 0-1-1-2 rdkit_fg_label_feature 예시\n",
    "- 아래의 글은 참고용이다. 그대로 쓰면 오류떠서 안된다."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8596a91d",
   "metadata": {},
   "source": [
    "from descriptastorus.descriptors.DescriptorGenerator import DescriptorGenerator\n",
    "RDKIT_PROPS = {\"1.0.0\": ['BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n',\n",
    "                         'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v',\n",
    "                         'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2',\n",
    "                         'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6',\n",
    "                         'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'ExactMolWt',\n",
    "                         'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3',\n",
    "                         'FractionCSP3', 'HallKierAlpha', 'HeavyAtomCount', 'HeavyAtomMolWt',\n",
    "                         'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'MaxAbsEStateIndex',\n",
    "                         'MaxAbsPartialCharge', 'MaxEStateIndex', 'MaxPartialCharge',\n",
    "                         'MinAbsEStateIndex', 'MinAbsPartialCharge', 'MinEStateIndex',\n",
    "                         'MinPartialCharge', 'MolLogP', 'MolMR', 'MolWt', 'NHOHCount',\n",
    "                         'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles',\n",
    "                         'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles',\n",
    "                         'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms',\n",
    "                         'NumRadicalElectrons', 'NumRotatableBonds', 'NumSaturatedCarbocycles',\n",
    "                         'NumSaturatedHeterocycles', 'NumSaturatedRings', 'NumValenceElectrons',\n",
    "                         'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13',\n",
    "                         'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5',\n",
    "                         'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'RingCount',\n",
    "                         'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5',\n",
    "                         'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10',\n",
    "                         'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4',\n",
    "                         'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9',\n",
    "                         'TPSA', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3',\n",
    "                         'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8',\n",
    "                         'VSA_EState9', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN',\n",
    "                         'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2',\n",
    "                         'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0',\n",
    "                         'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2',\n",
    "                         'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide',\n",
    "                         'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl',\n",
    "                         'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine',\n",
    "                         'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester',\n",
    "                         'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone',\n",
    "                         'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone',\n",
    "                         'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine',\n",
    "                         'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho',\n",
    "                         'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol',\n",
    "                         'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine',\n",
    "                         'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN',\n",
    "                         'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole',\n",
    "                         'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea', 'qed']\n",
    "               }\n",
    "CURRENT_VERSION = \"1.0.0\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c085d97",
   "metadata": {},
   "source": [
    "class RDKit2D(DescriptorGenerator):\n",
    "    \"\"\"Computes all RDKit Descriptors\"\"\"\n",
    "    NAME = \"RDKit2D\"\n",
    "    def __init__(self, properties=RDKIT_PROPS[CURRENT_VERSION]):\n",
    "        DescriptorGenerator.__init__(self)\n",
    "        # specify names and numpy types for all columns\n",
    "        if not properties:\n",
    "            self.columns = [ (name, numpy.float64) for name,func in sorted(Descriptors.descList) ]\n",
    "        else:\n",
    "            columns = self.columns\n",
    "            failed = []\n",
    "            \n",
    "            for name in properties:\n",
    "                if name in sorted(FUNCS):\n",
    "                    columns.append((name, numpy.float64))\n",
    "                else:\n",
    "                    logger.error(\"Unable to find specified property %s\"%name)\n",
    "                    failed.append(name)\n",
    "            if failed:\n",
    "                raise ValueError(\"%s: Failed to initialize: unable to find specified properties:\\n\\t%s\"%(\n",
    "                    self.__class__.__name__,\n",
    "                    \"\\n\\t\".join(failed)))\n",
    "        \n",
    "    def calculateMol(self, m, smiles, internalParsing=False):\n",
    "        res = [ applyFunc(name, m) for name, _ in self.columns ]\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d032265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles = 'C(O)O'\n",
    "generator = rdDescriptors.RDKit2D(RDKIT_PROPS)\n",
    "features = generator.process(smiles)[1:]\n",
    "features2 = np.array(features)\n",
    "features2[features2 != 0] = 1\n",
    "features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd92420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec182bb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0-1. generate_and_save_features 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff6682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_features(args: Namespace):\n",
    "    \"\"\"\n",
    "    Computes and saves features for a dataset of molecules as a 2D array in a .npz file.\n",
    "\n",
    "    :param args: Arguments.\n",
    "    \"\"\"\n",
    "    # Create directory for save_path\n",
    "    makedirs(args.save_path, isfile=True)\n",
    "\n",
    "    # Get data and features function\n",
    "    data = get_data(path=args.data_path, max_data_size=None)\n",
    "    features_generator = get_features_generator(args.features_generator)\n",
    "    temp_save_dir = args.save_path + '_temp'\n",
    "\n",
    "    # Load partially complete data\n",
    "    if args.restart:\n",
    "        if os.path.exists(args.save_path):\n",
    "            os.remove(args.save_path)\n",
    "        if os.path.exists(temp_save_dir):\n",
    "            shutil.rmtree(temp_save_dir)\n",
    "    else:\n",
    "        if os.path.exists(args.save_path):\n",
    "            raise ValueError(f'\"{args.save_path}\" already exists and args.restart is False.')\n",
    "\n",
    "        if os.path.exists(temp_save_dir):\n",
    "            features, temp_num = load_temp(temp_save_dir)\n",
    "\n",
    "    if not os.path.exists(temp_save_dir):\n",
    "        makedirs(temp_save_dir)\n",
    "        features, temp_num = [], 0\n",
    "\n",
    "    # Build features map function\n",
    "    data = data[len(features):]  # restrict to data for which features have not been computed yet\n",
    "    mols = (d.smiles for d in data)\n",
    "\n",
    "    if args.sequential:\n",
    "        features_map = map(features_generator, mols)\n",
    "    else:\n",
    "        features_map = Pool(30).imap(features_generator, mols)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Get features\n",
    "    temp_features = []\n",
    "    for i, feats in tqdm(enumerate(features_map), total=len(data)):\n",
    "        temp_features.append(feats)\n",
    "\n",
    "        # Save temporary features every save_frequency\n",
    "        if (i > 0 and (i + 1) % args.save_frequency == 0) or i == len(data) - 1:\n",
    "            save_features(os.path.join(temp_save_dir, f'{temp_num}.npz'), temp_features)\n",
    "            features.extend(temp_features)\n",
    "            temp_features = []\n",
    "            temp_num += 1\n",
    "\n",
    "    try:\n",
    "        # Save all features\n",
    "        save_features(args.save_path, features)\n",
    "\n",
    "        # Remove temporary features\n",
    "        shutil.rmtree(temp_save_dir)\n",
    "    except OverflowError:\n",
    "        print('Features array is too large to save as a single file. Instead keeping features as a directory of files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05064b91",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 6821.12it/s]\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-11:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-26:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-23:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-25:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-30:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-27:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/rt/anaconda3/envs/tox/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "if args.save_path is None:\n",
    "    args.save_path = args.data_path.split('csv')[0] + 'npz'\n",
    "generate_and_save_features(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672108a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. build_vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4724f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "#from grover.data.torchvocab import MolVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bbd1a4",
   "metadata": {},
   "source": [
    "## 1-1.MolVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "542e6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The contextual property.\n",
    "\"\"\"\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import tqdm\n",
    "from rdkit import Chem\n",
    "\n",
    "#from grover.data.task_labels import atom_to_vocab\n",
    "#from grover.data.task_labels import bond_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d031fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchVocab(object):\n",
    "    \"\"\"\n",
    "    Defines the vocabulary for atoms/bonds in molecular.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=('<pad>', '<other>'), vocab_type='atom'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param counter:\n",
    "        :param max_size:\n",
    "        :param min_freq:\n",
    "        :param specials:\n",
    "        :param vocab_type: 'atom': atom atom_vocab; 'bond': bond atom_vocab.\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "        if vocab_type in ('atom', 'bond'):\n",
    "            self.vocab_type = vocab_type\n",
    "        else:\n",
    "            raise ValueError('Wrong input for vocab_type!')\n",
    "        self.itos = list(specials)\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "        self.other_index = 1\n",
    "        self.pad_index = 0\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        # if self.vectors != other.vectors:\n",
    "        #    return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "                self.freqs[w] = 0\n",
    "            self.freqs[w] += v.freqs[w]\n",
    "\n",
    "    def mol_to_seq(self, mol, with_len=False):\n",
    "        mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "        if self.vocab_type == 'atom':\n",
    "            seq = [self.stoi.get(atom_to_vocab(mol, atom), self.other_index) for i, atom in enumerate(mol.GetAtoms())]\n",
    "        else:\n",
    "            seq = [self.stoi.get(bond_to_vocab(mol, bond), self.other_index) for i, bond in enumerate(mol.GetBonds())]\n",
    "        return (seq, len(seq)) if with_len else seq\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32020a87",
   "metadata": {},
   "source": [
    "### 1-1-1. atom/bond_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8605fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Callable, Union\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from descriptastorus.descriptors import rdDescriptors\n",
    "\n",
    "#from grover.data.molfeaturegenerator import register_features_generator\n",
    "\n",
    "BOND_FEATURES = ['BondType', 'Stereo', 'BondDir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851fba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_to_vocab(mol, atom):\n",
    "    \"\"\"\n",
    "    Convert atom to vocabulary. The convention is based on atom type and bond type.\n",
    "    :param mol: the molecular.\n",
    "    :param atom: the target atom.\n",
    "    :return: the generated atom vocabulary with its contexts.\n",
    "    \"\"\"\n",
    "    nei = Counter()\n",
    "    for a in atom.GetNeighbors():\n",
    "        bond = mol.GetBondBetweenAtoms(atom.GetIdx(), a.GetIdx())\n",
    "        nei[str(a.GetSymbol()) + \"-\" + str(bond.GetBondType())] += 1\n",
    "    keys = nei.keys()\n",
    "    keys = list(keys)\n",
    "    keys.sort()\n",
    "    output = atom.GetSymbol()\n",
    "    for k in keys:\n",
    "        output = \"%s_%s%d\" % (output, k, nei[k])\n",
    "\n",
    "    # The generated atom_vocab is too long?\n",
    "    return output\n",
    "\n",
    "\n",
    "def bond_to_vocab(mol, bond):\n",
    "    \"\"\"\n",
    "    Convert bond to vocabulary. The convention is based on atom type and bond type.\n",
    "    Considering one-hop neighbor atoms\n",
    "    :param mol: the molecular.\n",
    "    :param atom: the target atom.\n",
    "    :return: the generated bond vocabulary with its contexts.\n",
    "    \"\"\"\n",
    "    nei = Counter()\n",
    "    two_neighbors = (bond.GetBeginAtom(), bond.GetEndAtom())\n",
    "    two_indices = [a.GetIdx() for a in two_neighbors]\n",
    "    for nei_atom in two_neighbors:\n",
    "        for a in nei_atom.GetNeighbors():\n",
    "            a_idx = a.GetIdx()\n",
    "            if a_idx in two_indices:\n",
    "                continue\n",
    "            tmp_bond = mol.GetBondBetweenAtoms(nei_atom.GetIdx(), a_idx)\n",
    "            nei[str(nei_atom.GetSymbol()) + '-' + get_bond_feature_name(tmp_bond)] += 1\n",
    "    keys = list(nei.keys())\n",
    "    keys.sort()\n",
    "    output = get_bond_feature_name(bond)\n",
    "    for k in keys:\n",
    "        output = \"%s_%s%d\" % (output, k, nei[k])\n",
    "    return output\n",
    "\n",
    "def get_bond_feature_name(bond):\n",
    "    \"\"\"\n",
    "    Return the string format of bond features.\n",
    "    Bond features are surrounded with ()\n",
    "\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for bond_feature in BOND_FEATURES:\n",
    "        fea = eval(f\"bond.Get{bond_feature}\")()\n",
    "        ret.append(str(fea))\n",
    "\n",
    "    return '(' + '-'.join(ret) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c03ae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_O-SINGLE2 O_C-SINGLE1 O_C-SINGLE1\n"
     ]
    }
   ],
   "source": [
    "mol = Chem.MolFromSmiles('C(O)O')\n",
    "print(atom_to_vocab(mol, mol.GetAtoms()[0]), atom_to_vocab(mol, mol.GetAtoms()[1]), atom_to_vocab(mol, mol.GetAtoms()[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8de66cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SINGLE-STEREONONE-NONE)_C-(SINGLE-STEREONONE-NONE)1 (SINGLE-STEREONONE-NONE)_C-(SINGLE-STEREONONE-NONE)1\n"
     ]
    }
   ],
   "source": [
    "print(bond_to_vocab(mol, mol.GetBonds()[0]), bond_to_vocab(mol, mol.GetBonds()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13bdc58",
   "metadata": {},
   "source": [
    "## 1-1. MolVocab클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7228271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolVocab(TorchVocab):\n",
    "    def __init__(self, smiles, max_size=None, min_freq=1, vocab_type='atom'):\n",
    "        if vocab_type in ('atom', 'bond'):\n",
    "            self.vocab_type = vocab_type\n",
    "        else:\n",
    "            raise ValueError('Wrong input for vocab_type!')\n",
    "\n",
    "        print(\"Building %s vocab from smiles: %d\" % (self.vocab_type, len(smiles)))\n",
    "        counter = Counter()\n",
    "\n",
    "        for smi in tqdm.tqdm(smiles):\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if self.vocab_type == 'atom':\n",
    "                for _, atom in enumerate(mol.GetAtoms()):\n",
    "                    v = atom_to_vocab(mol, atom)\n",
    "                    counter[v] += 1\n",
    "            else:\n",
    "                for _, bond in enumerate(mol.GetBonds()):\n",
    "                    v = bond_to_vocab(mol, bond)\n",
    "                    counter[v] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq, vocab_type=vocab_type)\n",
    "\n",
    "    def __init__(self, file_path, max_size=None, min_freq=1, num_workers=1, total_lines=None, vocab_type='atom'):\n",
    "        if vocab_type in ('atom', 'bond'):\n",
    "            self.vocab_type = vocab_type\n",
    "        else:\n",
    "            raise ValueError('Wrong input for vocab_type!')\n",
    "        print(\"Building %s vocab from file: %s\" % (self.vocab_type, file_path))\n",
    "\n",
    "        from rdkit import RDLogger\n",
    "        lg = RDLogger.logger()\n",
    "        lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "        if total_lines is None:\n",
    "            def file_len(fname):\n",
    "                f_len = 0\n",
    "                with open(fname) as f:\n",
    "                    for f_len, _ in enumerate(f):\n",
    "                        pass\n",
    "                return f_len + 1\n",
    "\n",
    "            total_lines = file_len(file_path)\n",
    "\n",
    "        counter = Counter()\n",
    "        pbar = tqdm.tqdm(total=total_lines)\n",
    "        pool = Pool(num_workers)\n",
    "        res = []\n",
    "        batch = 50000\n",
    "        callback = lambda a: pbar.update(batch)\n",
    "        for i in range(int(total_lines / batch + 1)):\n",
    "            start = int(batch * i)\n",
    "            end = min(total_lines, batch * (i + 1))\n",
    "            # print(\"Start: %d, End: %d\"%(start, end))\n",
    "            res.append(pool.apply_async(MolVocab.read_smiles_from_file,\n",
    "                                        args=(file_path, start, end, vocab_type,),\n",
    "                                        callback=callback))\n",
    "            # read_smiles_from_file(lock, file_path, start, end)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        for r in res:\n",
    "            sub_counter = r.get()\n",
    "            for k in sub_counter:\n",
    "                if k not in counter:\n",
    "                    counter[k] = 0\n",
    "                counter[k] += sub_counter[k]\n",
    "        # print(counter)\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq, vocab_type=vocab_type)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_smiles_from_file(file_path, start, end, vocab_type):\n",
    "        # print(\"start\")\n",
    "        smiles = open(file_path, \"r\")\n",
    "        smiles.readline()\n",
    "        sub_counter = Counter()\n",
    "        for i, smi in enumerate(smiles):\n",
    "            if i < start:\n",
    "                continue\n",
    "            if i >= end:\n",
    "                break\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if vocab_type == 'atom':\n",
    "                for atom in mol.GetAtoms():\n",
    "                    v = atom_to_vocab(mol, atom)\n",
    "                    sub_counter[v] += 1\n",
    "            else:\n",
    "                for bond in mol.GetBonds():\n",
    "                    v = bond_to_vocab(mol, bond)\n",
    "                    sub_counter[v] += 1\n",
    "        # print(\"end\")\n",
    "        return sub_counter\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'MolVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc1b68",
   "metadata": {},
   "source": [
    "## 실행코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fec76108",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--data_path', default=\"../../dataset/grover_new_dataset/druglike_merged_refine2.csv\", type=str)\n",
    "parser.add_argument('--vocab_save_folder', default=\"../../dataset/grover_new_dataset\", type=str)\n",
    "parser.add_argument('--dataset_name', type=str, default=None,\n",
    "                    help=\"Will be the first part of the vocab file name. If it is None,\"\n",
    "                         \"the vocab files will be: atom_vocab.pkl and bond_vocab.pkl\")\n",
    "parser.add_argument('--vocab_max_size', type=int, default=None)\n",
    "parser.add_argument('--vocab_min_freq', type=int, default=1)\n",
    "args = parser.parse_args(['--data_path','data/CO2.csv','--vocab_save_folder', 'data/CO2', '--dataset_name','CO2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69c9ec40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building atom vocab from file: data/CO2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "50000it [00:00, 77882.49it/s]                                                                                           \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom vocab size 4\n",
      "Building bond vocab from file: data/CO2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "50000it [00:00, 84081.55it/s]                                                                                           \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bond vocab size 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for vocab_type in ['atom', 'bond']:\n",
    "    vocab_file = f\"{vocab_type}_vocab.pkl\"\n",
    "    if args.dataset_name is not None:\n",
    "        vocab_file = args.dataset_name + '_' + vocab_file\n",
    "    vocab_save_path = os.path.join(args.vocab_save_folder, vocab_file)\n",
    "\n",
    "    os.makedirs(os.path.dirname(vocab_save_path), exist_ok=True)\n",
    "    vocab = MolVocab(file_path=args.data_path,\n",
    "                     max_size=args.vocab_max_size,\n",
    "                     min_freq=args.vocab_min_freq,\n",
    "                     num_workers=100,\n",
    "                     vocab_type=vocab_type)\n",
    "    print(f\"{vocab_type} vocab size\", len(vocab))\n",
    "    vocab.save_vocab(vocab_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb825cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3346a31e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. pretrain_model()\n",
    "- pretrain.py파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a70363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from grover.data.dist_sampler import DistributedSampler\n",
    "from grover.data.groverdataset import get_data, split_data, GroverCollator, BatchMolDataset\n",
    "from grover.data.torchvocab import MolVocab\n",
    "from grover.model.models import GROVEREmbedding\n",
    "from grover.util.multi_gpu_wrapper import MultiGpuWrapper as mgw\n",
    "from grover.util.nn_utils import param_count\n",
    "from grover.util.utils import build_optimizer, build_lr_scheduler\n",
    "from task.grovertrainer import GROVERTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb46634",
   "metadata": {},
   "source": [
    "## 2-0. pre_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9b67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a800ee",
   "metadata": {},
   "source": [
    "### 2-0-1. DistributedSampler\n",
    "- 부분집합 만드는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "627ae206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedSampler(Sampler):\n",
    "    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n",
    "\n",
    "    It is especially useful in conjunction with\n",
    "    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n",
    "    process can pass a DistributedSampler instance as a DataLoader sampler,\n",
    "    and load a subset of the original dataset that is exclusive to it.\n",
    "\n",
    "    .. note::\n",
    "        Dataset is assumed to be of constant size.\n",
    "\n",
    "    Arguments:\n",
    "        dataset: Dataset used for sampling.\n",
    "        num_replicas (optional): Number of processes participating in\n",
    "            distributed training.\n",
    "        rank (optional): Rank of the current process within num_replicas.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True, sample_per_file=None):\n",
    "        if num_replicas is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            num_replicas = dist.get_world_size()\n",
    "        if rank is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            rank = dist.get_rank()\n",
    "        self.dataset = dataset\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.epoch = 0\n",
    "        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n",
    "        self.total_size = self.num_samples * self.num_replicas\n",
    "        self.sample_per_file = sample_per_file\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def get_indices(self):\n",
    "\n",
    "        indices = list(range(len(self.dataset)))\n",
    "\n",
    "        if self.sample_per_file is not None:\n",
    "            indices = self.sub_indices_of_rank(indices)\n",
    "        else:\n",
    "            # add extra samples to make it evenly divisible\n",
    "            indices += indices[:(self.total_size - len(indices))]\n",
    "            assert len(indices) == self.total_size\n",
    "            # subsample\n",
    "            s = self.rank * self.num_samples\n",
    "            e = min((self.rank + 1) * self.num_samples, len(indices))\n",
    "\n",
    "            # indices = indices[self.rank:self.total_size:self.num_replicas]\n",
    "            indices = indices[s:e]\n",
    "\n",
    "        if self.shuffle:\n",
    "            g = torch.Generator()\n",
    "            # the seed need to be considered.\n",
    "            g.manual_seed((self.epoch + 1) * (self.rank + 1) * time.time())\n",
    "            idx = torch.randperm(len(indices), generator=g).tolist()\n",
    "            indices = [indices[i] for i in idx]\n",
    "\n",
    "        # disable this since sub_indices_of_rank.\n",
    "        # assert len(indices) == self.num_samples\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def sub_indices_of_rank(self, indices):\n",
    "\n",
    "        # fix generator for each epoch\n",
    "        g = torch.Generator()\n",
    "        # All data should be loaded in each epoch.\n",
    "        g.manual_seed((self.epoch + 1) * 2 + 3)\n",
    "\n",
    "        # the fake file indices to cache\n",
    "        f_indices = list(range(int(math.ceil(len(indices) * 1.0 / self.sample_per_file))))\n",
    "        idx = torch.randperm(len(f_indices), generator=g).tolist()\n",
    "        f_indices = [f_indices[i] for i in idx]\n",
    "\n",
    "        file_per_rank = int(math.ceil(len(f_indices) * 1.0 / self.num_replicas))\n",
    "        # add extra fake file to make it evenly divisible\n",
    "        f_indices += f_indices[:(file_per_rank * self.num_replicas - len(f_indices))]\n",
    "\n",
    "        # divide index by rank\n",
    "        rank_s = self.rank * file_per_rank\n",
    "        rank_e = min((self.rank + 1) * file_per_rank, len(f_indices))\n",
    "\n",
    "        # get file index for this rank\n",
    "        f_indices = f_indices[rank_s:rank_e]\n",
    "        # print(\"f_indices\")\n",
    "        # print(f_indices)\n",
    "        res_indices = []\n",
    "        for fi in f_indices:\n",
    "            # get real indices for this rank\n",
    "            si = fi * self.sample_per_file\n",
    "            ei = min((fi + 1) * self.sample_per_file, len(indices))\n",
    "            cur_idx = [indices[i] for i in range(si, ei)]\n",
    "            res_indices += cur_idx\n",
    "\n",
    "        self.num_samples = len(res_indices)\n",
    "        return res_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.get_indices())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "232f53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_load_data(dataset: BatchMolDataset, rank: int, num_replicas: int, sample_per_file: int = None, epoch: int = 0):\n",
    "    \"\"\"\n",
    "    Pre-load data at the beginning of each epoch.\n",
    "    :param dataset: the training dataset.\n",
    "    :param rank: the rank of the current worker.\n",
    "    :param num_replicas: the replicas.\n",
    "    :param sample_per_file: the number of the data points in each file. When sample_per_file is None, all data will be\n",
    "    loaded. It implies the testing phase. (TODO: bad design here.)\n",
    "    :param epoch: the epoch number.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mock_sampler = DistributedSampler(dataset, num_replicas=num_replicas, rank=rank, shuffle=False,\n",
    "                                      sample_per_file=sample_per_file)\n",
    "    mock_sampler.set_epoch(epoch)\n",
    "    pre_indices = mock_sampler.get_indices()\n",
    "    for i in pre_indices:\n",
    "        dataset.load_data(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ae900",
   "metadata": {},
   "source": [
    "## 2-1. run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d17c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from grover.data.dist_sampler import DistributedSampler\n",
    "from grover.data.groverdataset import get_data, split_data, GroverCollator, BatchMolDataset\n",
    "from grover.data.torchvocab import MolVocab\n",
    "from grover.model.models import GROVEREmbedding\n",
    "from grover.util.multi_gpu_wrapper import MultiGpuWrapper as mgw\n",
    "from grover.util.nn_utils import param_count\n",
    "from grover.util.utils import build_optimizer, build_lr_scheduler\n",
    "from task.grovertrainer import GROVERTrainer\n",
    "\n",
    "from grover.topology.mol_tree import Motif_Vocab\n",
    "from grover.topology.motif_generation import Motif_Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b6cd3",
   "metadata": {},
   "source": [
    "### 2-1-1. get_data()\n",
    "- summary 예 : n_files:60, n_samples:5970, sample_per_file:100\n",
    "- graph : smiles를 잘게 나눈 csv들이 있는 곳\n",
    "- feature : 분자 안에 어떤 motif가 포함되어 있는가를 feature라고 표현하네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02291500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import csv\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from rdkit import Chem\n",
    "\n",
    "import grover.util.utils as feautils\n",
    "from grover.data import mol2graph\n",
    "from grover.data.moldataset import MoleculeDatapoint\n",
    "from grover.data.task_labels import atom_to_vocab, bond_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9a84538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDatapoint:\n",
    "    def __init__(self,\n",
    "                 smiles_file,\n",
    "                 feature_file,\n",
    "                 n_samples,\n",
    "                 ):\n",
    "        self.smiles_file = smiles_file\n",
    "        self.feature_file = feature_file\n",
    "        # deal with the last batch graph numbers.\n",
    "        self.n_samples = n_samples\n",
    "        self.datapoints = None\n",
    "\n",
    "    def load_datapoints(self):\n",
    "        features = self.load_feature()\n",
    "        self.datapoints = []\n",
    "\n",
    "        with open(self.smiles_file) as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for i, line in enumerate(reader):\n",
    "                # line = line[0]\n",
    "                d = MoleculeDatapoint(line=line,\n",
    "                                      features=features[i])\n",
    "                self.datapoints.append(d)\n",
    "\n",
    "        assert len(self.datapoints) == self.n_samples\n",
    "\n",
    "    def load_feature(self):\n",
    "        return feautils.load_features(self.feature_file)\n",
    "\n",
    "    def shuffle(self):\n",
    "        pass\n",
    "\n",
    "    def clean_cache(self):\n",
    "        del self.datapoints\n",
    "        self.datapoints = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert self.datapoints is not None\n",
    "        return self.datapoints[idx]\n",
    "\n",
    "    def is_loaded(self):\n",
    "        return self.datapoints is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac7e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMolDataset(Dataset):\n",
    "    def __init__(self, data: List[BatchDatapoint],\n",
    "                 graph_per_file=None):\n",
    "        self.data = data\n",
    "\n",
    "        self.len = 0\n",
    "        for d in self.data:\n",
    "            self.len += len(d)\n",
    "        if graph_per_file is not None:\n",
    "            self.sample_per_file = graph_per_file\n",
    "        else:\n",
    "            self.sample_per_file = len(self.data[0]) if len(self.data) != 0 else None\n",
    "\n",
    "    def shuffle(self, seed: int = None):\n",
    "        pass\n",
    "\n",
    "    def clean_cache(self):\n",
    "        for d in self.data:\n",
    "            d.clean_cache()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx) -> Union[MoleculeDatapoint, List[MoleculeDatapoint]]:\n",
    "        # print(idx)\n",
    "        dp_idx = int(idx / self.sample_per_file)\n",
    "        real_idx = idx % self.sample_per_file\n",
    "        return self.data[dp_idx][real_idx]\n",
    "\n",
    "    def load_data(self, idx):\n",
    "        dp_idx = int(idx / self.sample_per_file)\n",
    "        if not self.data[dp_idx].is_loaded():\n",
    "            self.data[dp_idx].load_datapoints()\n",
    "\n",
    "    def count_loaded_datapoints(self):\n",
    "        res = 0\n",
    "        for d in self.data:\n",
    "            if d.is_loaded():\n",
    "                res += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e08727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path, logger=None):\n",
    "    \"\"\"\n",
    "    Load data from the data_path.\n",
    "    :param data_path: the data_path.\n",
    "    :param logger: the logger.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    debug = logger.debug if logger is not None else print\n",
    "    summary_path = os.path.join(data_path, \"summary.txt\")\n",
    "    smiles_path = os.path.join(data_path, \"graph\")\n",
    "    feature_path = os.path.join(data_path, \"feature\")\n",
    "\n",
    "    fin = open(summary_path)\n",
    "    n_files = int(fin.readline().strip().split(\":\")[-1])\n",
    "    n_samples = int(fin.readline().strip().split(\":\")[-1])\n",
    "    sample_per_file = int(fin.readline().strip().split(\":\")[-1])\n",
    "    debug(\"Loading data:\")\n",
    "    debug(\"Number of files: %d\" % n_files)\n",
    "    debug(\"Number of samples: %d\" % n_samples)\n",
    "    debug(\"Samples/file: %d\" % sample_per_file)\n",
    "\n",
    "    datapoints = []\n",
    "    for i in range(n_files):\n",
    "        smiles_path_i = os.path.join(smiles_path, str(i) + \".csv\")\n",
    "        feature_path_i = os.path.join(feature_path, str(i) + \".npz\")\n",
    "        n_samples_i = sample_per_file if i != (n_files - 1) else n_samples % sample_per_file\n",
    "        datapoints.append(BatchDatapoint(smiles_path_i, feature_path_i, n_samples_i))\n",
    "    return BatchMolDataset(datapoints), sample_per_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36cd2f",
   "metadata": {},
   "source": [
    "### 2-1-2. GroverCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcd6ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-1-2-1. mol2graph(finetune과 같음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afd6e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "MAX_ATOMIC_NUM = 100\n",
    "\n",
    "\n",
    "ATOM_FEATURES = {\n",
    "    'atomic_num': list(range(MAX_ATOMIC_NUM)),\n",
    "    'degree': [0, 1, 2, 3, 4, 5],\n",
    "    'formal_charge': [-1, -2, 1, 2, 0],\n",
    "    'chiral_tag': [0, 1, 2, 3],\n",
    "    'num_Hs': [0, 1, 2, 3, 4],\n",
    "    'hybridization': [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2\n",
    "    ],\n",
    "}\n",
    "\n",
    "# len(choices) + 1 to include room for uncommon values; + 2 at end for IsAromatic and mass\n",
    "ATOM_FDIM = sum(len(choices) + 1 for choices in ATOM_FEATURES.values()) + 2    # 참고로 이거 133이다... 왜지?\n",
    "BOND_FDIM = 14\n",
    "BOND_FDIM_3D = 15\n",
    "\n",
    "\n",
    "def get_atom_fdim() -> int:\n",
    "    \"\"\"\n",
    "    Gets the dimensionality of atom features.\n",
    "\n",
    "    :param: Arguments.\n",
    "    \"\"\"\n",
    "    return ATOM_FDIM + 18\n",
    "\n",
    "\n",
    "def get_bond_fdim() -> int:\n",
    "    \"\"\"\n",
    "    Gets the dimensionality of bond features.\n",
    "\n",
    "    :param: Arguments.\n",
    "    \"\"\"\n",
    "    return BOND_FDIM\n",
    "\n",
    "\n",
    "def onek_encoding_unk(value: int, choices: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Creates a one-hot encoding.\n",
    "\n",
    "    :param value: The value for which the encoding should be one.\n",
    "    :param choices: A list of possible values.\n",
    "    :return: A one-hot encoding of the value in a list of length len(choices) + 1.\n",
    "    If value is not in the list of choices, then the final element in the encoding is 1.\n",
    "    \"\"\"\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    if min(choices) < 0:\n",
    "        index = value\n",
    "    else:\n",
    "        index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd250a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolGraph:\n",
    "    \"\"\"\n",
    "    A MolGraph represents the graph structure and featurization of a single molecule.\n",
    "\n",
    "    A MolGraph computes the following attributes:\n",
    "    - smiles: Smiles string.\n",
    "    - n_atoms: The number of atoms in the molecule.\n",
    "    - n_bonds: The number of bonds in the molecule.\n",
    "    - f_atoms: A mapping from an atom index to a list atom features.\n",
    "    - f_bonds: A mapping from a bond index to a list of bond features.\n",
    "    - a2b: A mapping from an atom index to a list of incoming bond indices.\n",
    "    - b2a: A mapping from a bond index to the index of the atom the bond originates from.\n",
    "    - b2revb: A mapping from a bond index to the index of the reverse bond.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smiles: str,  args: Namespace):\n",
    "        \"\"\"\n",
    "        Computes the graph structure and featurization of a molecule.\n",
    "\n",
    "        :param smiles: A smiles string.\n",
    "        :param args: Arguments.\n",
    "        \"\"\"\n",
    "        self.smiles = smiles\n",
    "        self.args = args\n",
    "        self.n_atoms = 0  # number of atoms\n",
    "        self.n_bonds = 0  # number of bonds\n",
    "        self.f_atoms = []  # mapping from atom index to atom features\n",
    "        self.f_bonds = []  # mapping from bond index to concat(in_atom, bond) features\n",
    "        self.a2b = []  # mapping from atom index to incoming bond indices\n",
    "        self.b2a = []  # mapping from bond index to the index of the atom the bond is coming from\n",
    "        self.b2revb = []  # mapping from bond index to the index of the reverse bond\n",
    "\n",
    "        # Convert smiles to molecule\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        self.hydrogen_donor = Chem.MolFromSmarts(\"[$([N;!H0;v3,v4&+1]),$([O,S;H1;+0]),n&H1&+0]\")\n",
    "        self.hydrogen_acceptor = Chem.MolFromSmarts(\n",
    "            \"[$([O,S;H1;v2;!$(*-*=[O,N,P,S])]),$([O,S;H0;v2]),$([O,S;-]),$([N;v3;!$(N-*=[O,N,P,S])]),\"\n",
    "            \"n&H0&+0,$([o,s;+0;!$([o,s]:n);!$([o,s]:c:n)])]\")\n",
    "        self.acidic = Chem.MolFromSmarts(\"[$([C,S](=[O,S,P])-[O;H1,-1])]\")\n",
    "        self.basic = Chem.MolFromSmarts(\n",
    "            \"[#7;+,$([N;H2&+0][$([C,a]);!$([C,a](=O))]),$([N;H1&+0]([$([C,a]);!$([C,a](=O))])[$([C,a]);\"\n",
    "            \"!$([C,a](=O))]),$([N;H0&+0]([C;!$(C(=O))])([C;!$(C(=O))])[C;!$(C(=O))])]\")\n",
    "\n",
    "        self.hydrogen_donor_match = sum(mol.GetSubstructMatches(self.hydrogen_donor), ())\n",
    "        self.hydrogen_acceptor_match = sum(mol.GetSubstructMatches(self.hydrogen_acceptor), ())\n",
    "        self.acidic_match = sum(mol.GetSubstructMatches(self.acidic), ())\n",
    "        self.basic_match = sum(mol.GetSubstructMatches(self.basic), ())\n",
    "        self.ring_info = mol.GetRingInfo()\n",
    "\n",
    "\n",
    "        # fake the number of \"atoms\" if we are collapsing substructures\n",
    "        self.n_atoms = mol.GetNumAtoms()\n",
    "\n",
    "        # Get atom features\n",
    "        for _, atom in enumerate(mol.GetAtoms()):\n",
    "            self.f_atoms.append(self.atom_features(atom))\n",
    "        self.f_atoms = [self.f_atoms[i] for i in range(self.n_atoms)]\n",
    "\n",
    "        for _ in range(self.n_atoms):\n",
    "            self.a2b.append([])\n",
    "\n",
    "        # Get bond features\n",
    "        for a1 in range(self.n_atoms):\n",
    "            for a2 in range(a1 + 1, self.n_atoms):\n",
    "                bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                if bond is None:\n",
    "                    continue\n",
    "\n",
    "                if args.bond_drop_rate > 0:\n",
    "                    if np.random.binomial(1, args.bond_drop_rate):\n",
    "                        continue\n",
    "\n",
    "                f_bond = self.bond_features(bond)\n",
    "\n",
    "                # Always treat the bond as directed.\n",
    "                self.f_bonds.append(self.f_atoms[a1] + f_bond)\n",
    "                self.f_bonds.append(self.f_atoms[a2] + f_bond)\n",
    "\n",
    "                # Update index mappings\n",
    "                b1 = self.n_bonds\n",
    "                b2 = b1 + 1\n",
    "                self.a2b[a2].append(b1)  # b1 = a1 --> a2\n",
    "                self.b2a.append(a1)\n",
    "                self.a2b[a1].append(b2)  # b2 = a2 --> a1\n",
    "                self.b2a.append(a2)\n",
    "                self.b2revb.append(b2)\n",
    "                self.b2revb.append(b1)\n",
    "                self.n_bonds += 2\n",
    "\n",
    "    def atom_features(self, atom: Chem.rdchem.Atom) -> List[Union[bool, int, float]]:\n",
    "        \"\"\"\n",
    "        Builds a feature vector for an atom.\n",
    "\n",
    "        :param atom: An RDKit atom.\n",
    "        :param functional_groups: A k-hot vector indicating the functional groups the atom belongs to.\n",
    "        :return: A list containing the atom features.\n",
    "        \"\"\"\n",
    "        features = onek_encoding_unk(atom.GetAtomicNum() - 1, ATOM_FEATURES['atomic_num']) + \\\n",
    "                   onek_encoding_unk(atom.GetTotalDegree(), ATOM_FEATURES['degree']) + \\\n",
    "                   onek_encoding_unk(atom.GetFormalCharge(), ATOM_FEATURES['formal_charge']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetChiralTag()), ATOM_FEATURES['chiral_tag']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetTotalNumHs()), ATOM_FEATURES['num_Hs']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetHybridization()), ATOM_FEATURES['hybridization']) + \\\n",
    "                   [1 if atom.GetIsAromatic() else 0] + \\\n",
    "                   [atom.GetMass() * 0.01]\n",
    "        atom_idx = atom.GetIdx()\n",
    "        features = features + \\\n",
    "                   onek_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6]) + \\\n",
    "                   [atom_idx in self.hydrogen_acceptor_match] + \\\n",
    "                   [atom_idx in self.hydrogen_donor_match] + \\\n",
    "                   [atom_idx in self.acidic_match] + \\\n",
    "                   [atom_idx in self.basic_match] + \\\n",
    "                   [self.ring_info.IsAtomInRingOfSize(atom_idx, 3),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 4),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 5),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 6),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 7),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 8)]\n",
    "        return features\n",
    "\n",
    "    def bond_features(self, bond: Chem.rdchem.Bond\n",
    "                      ) -> List[Union[bool, int, float]]:\n",
    "        \"\"\"\n",
    "        Builds a feature vector for a bond.\n",
    "\n",
    "        :param bond: A RDKit bond.\n",
    "        :return: A list containing the bond features.\n",
    "        \"\"\"\n",
    "\n",
    "        if bond is None:\n",
    "            fbond = [1] + [0] * (BOND_FDIM - 1)\n",
    "        else:\n",
    "            bt = bond.GetBondType()\n",
    "            fbond = [\n",
    "                0,  # bond is not None\n",
    "                bt == Chem.rdchem.BondType.SINGLE,\n",
    "                bt == Chem.rdchem.BondType.DOUBLE,\n",
    "                bt == Chem.rdchem.BondType.TRIPLE,\n",
    "                bt == Chem.rdchem.BondType.AROMATIC,\n",
    "                (bond.GetIsConjugated() if bt is not None else 0),\n",
    "                (bond.IsInRing() if bt is not None else 0)\n",
    "            ]\n",
    "            fbond += onek_encoding_unk(int(bond.GetStereo()), list(range(6)))\n",
    "        return fbond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c2d3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMolGraph:\n",
    "    \"\"\"\n",
    "    A BatchMolGraph represents the graph structure and featurization of a batch of molecules.\n",
    "\n",
    "    A BatchMolGraph contains the attributes of a MolGraph plus:\n",
    "    - smiles_batch: A list of smiles strings.\n",
    "    - n_mols: The number of molecules in the batch.\n",
    "    - atom_fdim: The dimensionality of the atom features.\n",
    "    - bond_fdim: The dimensionality of the bond features (technically the combined atom/bond features).\n",
    "    - a_scope: A list of tuples indicating the start and end atom indices for each molecule.\n",
    "    - b_scope: A list of tuples indicating the start and end bond indices for each molecule.\n",
    "    - max_num_bonds: The maximum number of bonds neighboring an atom in this batch.\n",
    "    - b2b: (Optional) A mapping from a bond index to incoming bond indices.\n",
    "    - a2a: (Optional): A mapping from an atom index to neighboring atom indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mol_graphs: List[MolGraph], args: Namespace):\n",
    "        self.smiles_batch = [mol_graph.smiles for mol_graph in mol_graphs]\n",
    "        self.n_mols = len(self.smiles_batch)\n",
    "\n",
    "        self.atom_fdim = get_atom_fdim()\n",
    "        self.bond_fdim = get_bond_fdim() + self.atom_fdim\n",
    "\n",
    "        # Start n_atoms and n_bonds at 1 b/c zero padding\n",
    "        self.n_atoms = 1  # number of atoms (start at 1 b/c need index 0 as padding)\n",
    "        self.n_bonds = 1  # number of bonds (start at 1 b/c need index 0 as padding)\n",
    "        self.a_scope = []  # list of tuples indicating (start_atom_index, num_atoms) for each molecule\n",
    "        self.b_scope = []  # list of tuples indicating (start_bond_index, num_bonds) for each molecule\n",
    "\n",
    "        # All start with zero padding so that indexing with zero padding returns zeros\n",
    "        f_atoms = [[0] * self.atom_fdim]  # atom features\n",
    "        f_bonds = [[0] * self.bond_fdim]  # combined atom/bond features\n",
    "        a2b = [[]]  # mapping from atom index to incoming bond indices\n",
    "        b2a = [0]  # mapping from bond index to the index of the atom the bond is coming from\n",
    "        b2revb = [0]  # mapping from bond index to the index of the reverse bond\n",
    "\n",
    "        for mol_graph in mol_graphs:\n",
    "            f_atoms.extend(mol_graph.f_atoms)\n",
    "            f_bonds.extend(mol_graph.f_bonds)\n",
    "\n",
    "            for a in range(mol_graph.n_atoms):\n",
    "                a2b.append([b + self.n_bonds for b in mol_graph.a2b[a]])\n",
    "\n",
    "            for b in range(mol_graph.n_bonds):\n",
    "                b2a.append(self.n_atoms + mol_graph.b2a[b])\n",
    "                b2revb.append(self.n_bonds + mol_graph.b2revb[b])\n",
    "\n",
    "            self.a_scope.append((self.n_atoms, mol_graph.n_atoms))\n",
    "            self.b_scope.append((self.n_bonds, mol_graph.n_bonds))\n",
    "            self.n_atoms += mol_graph.n_atoms\n",
    "            self.n_bonds += mol_graph.n_bonds\n",
    "\n",
    "        # max with 1 to fix a crash in rare case of all single-heavy-atom mols\n",
    "        self.max_num_bonds = max(1, max(len(in_bonds) for in_bonds in a2b))\n",
    "\n",
    "        self.f_atoms = torch.FloatTensor(f_atoms)\n",
    "        self.f_bonds = torch.FloatTensor(f_bonds)\n",
    "        self.a2b = torch.LongTensor([a2b[a] + [0] * (self.max_num_bonds - len(a2b[a])) for a in range(self.n_atoms)])\n",
    "        self.b2a = torch.LongTensor(b2a)\n",
    "        self.b2revb = torch.LongTensor(b2revb)\n",
    "        self.b2b = None  # try to avoid computing b2b b/c O(n_atoms^3)\n",
    "        self.a2a = self.b2a[self.a2b]  # only needed if using atom messages\n",
    "        self.a_scope = torch.LongTensor(self.a_scope)\n",
    "        self.b_scope = torch.LongTensor(self.b_scope)\n",
    "\n",
    "    def set_new_atom_feature(self, f_atoms):\n",
    "        \"\"\"\n",
    "        Set the new atom feature. Do not update bond feature.\n",
    "        :param f_atoms:\n",
    "        \"\"\"\n",
    "        self.f_atoms = f_atoms\n",
    "\n",
    "    def get_components(self) -> Tuple[torch.FloatTensor, torch.FloatTensor,\n",
    "                                      torch.LongTensor, torch.LongTensor, torch.LongTensor,\n",
    "                                      List[Tuple[int, int]], List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Returns the components of the BatchMolGraph.\n",
    "\n",
    "        :return: A tuple containing PyTorch tensors with the atom features, bond features, and graph structure\n",
    "        and two lists indicating the scope of the atoms and bonds (i.e. which molecules they belong to).\n",
    "        \"\"\"\n",
    "        return self.f_atoms, self.f_bonds, self.a2b, self.b2a, self.b2revb, self.a_scope, self.b_scope, self.a2a\n",
    "\n",
    "    def get_b2b(self) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Computes (if necessary) and returns a mapping from each bond index to all the incoming bond indices.\n",
    "\n",
    "        :return: A PyTorch tensor containing the mapping from each bond index to all the incoming bond indices.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.b2b is None:\n",
    "            b2b = self.a2b[self.b2a]  # num_bonds x max_num_bonds\n",
    "            # b2b includes reverse edge for each bond so need to mask out\n",
    "            revmask = (b2b != self.b2revb.unsqueeze(1).repeat(1, b2b.size(1))).long()  # num_bonds x max_num_bonds\n",
    "            self.b2b = b2b * revmask\n",
    "\n",
    "        return self.b2b\n",
    "\n",
    "    def get_a2a(self) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Computes (if necessary) and returns a mapping from each atom index to all neighboring atom indices.\n",
    "\n",
    "        :return: A PyTorch tensor containing the mapping from each bond index to all the incodming bond indices.\n",
    "        \"\"\"\n",
    "        if self.a2a is None:\n",
    "            # b = a1 --> a2\n",
    "            # a2b maps a2 to all incoming bonds b\n",
    "            # b2a maps each bond b to the atom it comes from a1\n",
    "            # thus b2a[a2b] maps atom a2 to neighboring atoms a1\n",
    "            self.a2a = self.b2a[self.a2b]  # num_atoms x max_num_bonds\n",
    "\n",
    "        return self.a2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e922782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol2graph(smiles_batch: List[str], shared_dict,\n",
    "              args: Namespace) -> BatchMolGraph:\n",
    "    \"\"\"\n",
    "    Converts a list of SMILES strings to a BatchMolGraph containing the batch of molecular graphs.\n",
    "\n",
    "    :param smiles_batch: A list of SMILES strings.\n",
    "    :param args: Arguments.\n",
    "    :return: A BatchMolGraph containing the combined molecular graph for the molecules\n",
    "    \"\"\"\n",
    "    mol_graphs = []\n",
    "    for smiles in smiles_batch:\n",
    "        if smiles in shared_dict:\n",
    "            mol_graph = shared_dict[smiles]\n",
    "        else:\n",
    "            mol_graph = MolGraph(smiles, args)\n",
    "            if not args.no_cache:\n",
    "                shared_dict[smiles] = mol_graph\n",
    "        mol_graphs.append(mol_graph)\n",
    "\n",
    "    return BatchMolGraph(mol_graphs, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2a440",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-1-2-2. Collator()함수\n",
    "- 여기의 percent를 수정하면 몇퍼센트를 알아맞출지를 결정한다.\n",
    "- 15%를 가린다는 의미인데, 실제로는 15%만 타겟으로 선정하여 맞추는 형식,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42a583b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroverCollator(object):\n",
    "    def __init__(self, shared_dict, atom_vocab, bond_vocab, args):\n",
    "        self.args = args\n",
    "        self.shared_dict = shared_dict\n",
    "        self.atom_vocab = atom_vocab\n",
    "        self.bond_vocab = bond_vocab\n",
    "\n",
    "    def atom_random_mask(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Perform the random mask operation on atoms.\n",
    "        :param smiles_batch:\n",
    "        :return: The corresponding atom labels.\n",
    "        \"\"\"\n",
    "        # There is a zero padding.\n",
    "        vocab_label = [0]\n",
    "        percent = 0.15\n",
    "        for smi in smiles_batch:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            mlabel = [0] * mol.GetNumAtoms()\n",
    "            n_mask = math.ceil(mol.GetNumAtoms() * percent)\n",
    "            perm = np.random.permutation(mol.GetNumAtoms())[:n_mask]\n",
    "            for p in perm:\n",
    "                atom = mol.GetAtomWithIdx(int(p))\n",
    "                mlabel[p] = self.atom_vocab.stoi.get(atom_to_vocab(mol, atom), self.atom_vocab.other_index)\n",
    "\n",
    "            vocab_label.extend(mlabel)\n",
    "        return vocab_label\n",
    "\n",
    "    def bond_random_mask(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Perform the random mask operaiion on bonds.\n",
    "        :param smiles_batch:\n",
    "        :return: The corresponding bond labels.\n",
    "        \"\"\"\n",
    "        # There is a zero padding.\n",
    "        vocab_label = [0]\n",
    "        percent = 0.15\n",
    "        for smi in smiles_batch:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            nm_atoms = mol.GetNumAtoms()\n",
    "            nm_bonds = mol.GetNumBonds()\n",
    "            mlabel = []\n",
    "            n_mask = math.ceil(nm_bonds * percent)\n",
    "            perm = np.random.permutation(nm_bonds)[:n_mask]\n",
    "            virtual_bond_id = 0\n",
    "            for a1 in range(nm_atoms):\n",
    "                for a2 in range(a1 + 1, nm_atoms):\n",
    "                    bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                    if bond is None:\n",
    "                        continue\n",
    "                    if virtual_bond_id in perm:\n",
    "                        label = self.bond_vocab.stoi.get(bond_to_vocab(mol, bond), self.bond_vocab.other_index)\n",
    "                        mlabel.extend([label])\n",
    "                    else:\n",
    "                        mlabel.extend([0])\n",
    "\n",
    "                    virtual_bond_id += 1\n",
    "            # todo: might need to consider bond_drop_rate\n",
    "            # todo: double check reverse bond\n",
    "            vocab_label.extend(mlabel)\n",
    "        return vocab_label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles_batch = [d.smiles for d in batch]\n",
    "        batchgraph = mol2graph(smiles_batch, self.shared_dict, self.args).get_components()\n",
    "\n",
    "        atom_vocab_label = torch.Tensor(self.atom_random_mask(smiles_batch)).long()\n",
    "        bond_vocab_label = torch.Tensor(self.bond_random_mask(smiles_batch)).long()\n",
    "        fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
    "        # may be some mask here\n",
    "        res = {\"graph_input\": batchgraph,\n",
    "               \"targets\": {\"av_task\": atom_vocab_label,\n",
    "                           \"bv_task\": bond_vocab_label,\n",
    "                           \"fg_task\": fgroup_label}\n",
    "               }\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297f3b8",
   "metadata": {},
   "source": [
    "### 2-1-3. GROVEREmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73796c7b",
   "metadata": {},
   "source": [
    "#### 2-1-3-1. MPNEncoder\n",
    "- 먼저 input feature(해당 블락의 feature 뿐만 아니라 - a2a, a2b, b2a, b2arevb 등 다양한 정보를 활용한다.)를 여기서 사용할 feature크기로 Linear(Dense)를 통과시키고, 활성화 함수를 적용시킨다.\n",
    "- 자신을 제외한 node, edge들의 메시지를 임의의 hop만큼 종합하여 Linear(Dense) 1개를 통과시켜서 Message를 구한다.\n",
    "- dyMPN은 truncated normal로 3을 기준으로 +-3에서 하는게 기본이나 0~6사이의 uniform분포에서 hop수를 지정할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b50701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b280febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNEncoder(nn.Module):\n",
    "    \"\"\"A message passing neural network for encoding a molecule.\"\"\"\n",
    "\n",
    "    def __init__(self, args: Namespace,\n",
    "                 atom_messages: bool,\n",
    "                 init_message_dim: int,\n",
    "                 attached_fea_fdim: int,\n",
    "                 hidden_size: int,\n",
    "                 bias: bool,\n",
    "                 depth: int,\n",
    "                 dropout: float,\n",
    "                 undirected: bool,\n",
    "                 dense: bool,\n",
    "                 aggregate_to_atom: bool,\n",
    "                 attach_fea: bool,\n",
    "                 input_layer=\"fc\",\n",
    "                 dynamic_depth='none'\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initializes the MPNEncoder.\n",
    "        :param args: the arguments.\n",
    "        :param atom_messages: enables atom_messages or not.\n",
    "        :param init_message_dim:  the initial input message dimension.\n",
    "        :param attached_fea_fdim:  the attached feature dimension.\n",
    "        :param hidden_size: the output message dimension during message passing.\n",
    "        :param bias: the bias in the message passing.\n",
    "        :param depth: the message passing depth.\n",
    "        :param dropout: the dropout rate.\n",
    "        :param undirected: the message passing is undirected or not.\n",
    "        :param dense: enables the dense connections.\n",
    "        :param attach_fea: enables the feature attachment during the message passing process.\n",
    "        :param dynamic_depth: enables the dynamic depth. Possible choices: \"none\", \"uniform\" and \"truncnorm\"\n",
    "        \"\"\"\n",
    "        super(MPNEncoder, self).__init__()\n",
    "        self.init_message_dim = init_message_dim\n",
    "        self.attached_fea_fdim = attached_fea_fdim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.depth = depth\n",
    "        self.dropout = dropout\n",
    "        self.input_layer = input_layer\n",
    "        self.layers_per_message = 1\n",
    "        self.undirected = undirected\n",
    "        self.atom_messages = atom_messages\n",
    "        self.dense = dense\n",
    "        self.aggreate_to_atom = aggregate_to_atom\n",
    "        self.attached_fea = attach_fea\n",
    "        self.dynamic_depth = dynamic_depth\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Activation\n",
    "        self.act_func = get_activation_function(args.activation)\n",
    "\n",
    "        # Input\n",
    "        if self.input_layer == \"fc\":\n",
    "            input_dim = self.init_message_dim\n",
    "            self.W_i = nn.Linear(input_dim, self.hidden_size, bias=self.bias)\n",
    "\n",
    "        if self.attached_fea:\n",
    "            w_h_input_size = self.hidden_size + self.attached_fea_fdim\n",
    "        else:\n",
    "            w_h_input_size = self.hidden_size\n",
    "\n",
    "        # Shared weight matrix across depths (default)\n",
    "        self.W_h = nn.Linear(w_h_input_size, self.hidden_size, bias=self.bias)\n",
    "\n",
    "    def forward(self,\n",
    "                init_messages,\n",
    "                init_attached_features,\n",
    "                a2nei,\n",
    "                a2attached,\n",
    "                b2a=None,\n",
    "                b2revb=None,\n",
    "                adjs=None\n",
    "                ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        The forward function.\n",
    "        :param init_messages:  initial massages, can be atom features or bond features.\n",
    "        :param init_attached_features: initial attached_features.\n",
    "        :param a2nei: the relation of item to its neighbors. For the atom message passing, a2nei = a2a. For bond\n",
    "        messages a2nei = a2b\n",
    "        :param a2attached: the relation of item to the attached features during message passing. For the atom message\n",
    "        passing, a2attached = a2b. For the bond message passing a2attached = a2a\n",
    "        :param b2a: remove the reversed bond in bond message passing\n",
    "        :param b2revb: remove the revered atom in bond message passing\n",
    "        :return: if aggreate_to_atom or self.atom_messages, return num_atoms x hidden.\n",
    "        Otherwise, return num_bonds x hidden\n",
    "        \"\"\"\n",
    "\n",
    "        # Input\n",
    "        if self.input_layer == 'fc':\n",
    "            input = self.W_i(init_messages)  # num_bonds x hidden_size # f_bond\n",
    "            message = self.act_func(input)  # num_bonds x hidden_size\n",
    "        elif self.input_layer == 'none':\n",
    "            input = init_messages\n",
    "            message = input\n",
    "\n",
    "        attached_fea = init_attached_features  # f_atom / f_bond\n",
    "\n",
    "        # dynamic depth\n",
    "        # uniform sampling from depth - 1 to depth + 1\n",
    "        # only works in training.\n",
    "        if self.training and self.dynamic_depth != \"none\":\n",
    "            if self.dynamic_depth == \"uniform\":\n",
    "                # uniform sampling\n",
    "                ndepth = numpy.random.randint(self.depth - 3, self.depth + 3)\n",
    "            else:\n",
    "                # truncnorm\n",
    "                mu = self.depth\n",
    "                sigma = 1\n",
    "                lower = mu - 3 * sigma\n",
    "                upper = mu + 3 * sigma\n",
    "                X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "                ndepth = int(X.rvs(1))\n",
    "        else:\n",
    "            ndepth = self.depth\n",
    "\n",
    "        # Message passing\n",
    "        for _ in range(ndepth - 1):\n",
    "            if self.undirected:\n",
    "                # two directions should be the same\n",
    "                message = (message + message[b2revb]) / 2\n",
    "\n",
    "            nei_message = select_neighbor_and_aggregate(message, a2nei)\n",
    "            a_message = nei_message\n",
    "            if self.attached_fea:\n",
    "                attached_nei_fea = select_neighbor_and_aggregate(attached_fea, a2attached)\n",
    "                a_message = torch.cat((nei_message, attached_nei_fea), dim=1)\n",
    "\n",
    "            if not self.atom_messages:\n",
    "                rev_message = message[b2revb]\n",
    "                if self.attached_fea:\n",
    "                    atom_rev_message = attached_fea[b2a[b2revb]]\n",
    "                    rev_message = torch.cat((rev_message, atom_rev_message), dim=1)\n",
    "                # Except reverse bond its-self(w) ! \\sum_{k\\in N(u) \\ w}\n",
    "                message = a_message[b2a] - rev_message  # num_bonds x hidden\n",
    "            else:\n",
    "                message = a_message\n",
    "\n",
    "            message = self.W_h(message)\n",
    "\n",
    "            # BUG here, by default MPNEncoder use the dense connection in the message passing step.\n",
    "            # The correct form should if not self.dense\n",
    "            if self.dense:\n",
    "                message = self.act_func(message)  # num_bonds x hidden_size\n",
    "            else:\n",
    "                message = self.act_func(input + message)\n",
    "            message = self.dropout_layer(message)  # num_bonds x hidden\n",
    "\n",
    "        output = message\n",
    "        print(output)\n",
    "\n",
    "        return output  # num_atoms x hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44b041",
   "metadata": {},
   "source": [
    "#### 2-1-3-2. 멀티헤드어텐션\n",
    "- 위의 MPN을 Q,K,V로 3개에 대해 Head수만큼 만든다(4 또는 8)\n",
    "- 그리고 각각의 Head수만큼 Self-Attention을 점곱하여 계산해낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f16c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product SelfAttention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        :param query:\n",
    "        :param key:\n",
    "        :param value:\n",
    "        :param mask:\n",
    "        :param dropout:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5f96c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    The multi-head attention module. Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1, bias=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param h:\n",
    "        :param d_model:\n",
    "        :param dropout:\n",
    "        :param bias:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        print(f'd_model is {d_model} and h is {h}')\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h  # number of heads\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])  # why 3: query, key, value\n",
    "        self.output_linear = nn.Linear(d_model, d_model, bias)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param query:\n",
    "        :param key:\n",
    "        :param value:\n",
    "        :param mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, _ = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "711840cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One head for multi-headed attention.\n",
    "    :return: (query, key, value)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, hidden_size, atom_messages=False):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        :param args: The argument.\n",
    "        :param hidden_size: the dimension of hidden layer in Head.\n",
    "        :param atom_messages: the MPNEncoder type.\n",
    "        \"\"\"\n",
    "        super(Head, self).__init__()\n",
    "        atom_fdim = hidden_size\n",
    "        bond_fdim = hidden_size\n",
    "        hidden_size = hidden_size\n",
    "        self.atom_messages = atom_messages\n",
    "        if self.atom_messages:\n",
    "            init_message_dim = atom_fdim\n",
    "            attached_fea_dim = bond_fdim\n",
    "        else:\n",
    "            init_message_dim = bond_fdim\n",
    "            attached_fea_dim = atom_fdim\n",
    "\n",
    "        # Here we use the message passing network as query, key and value.\n",
    "        self.mpn_q = MPNEncoder(args=args,\n",
    "                                atom_messages=atom_messages,\n",
    "                                init_message_dim=init_message_dim,\n",
    "                                attached_fea_fdim=attached_fea_dim,\n",
    "                                hidden_size=hidden_size,\n",
    "                                bias=args.bias,\n",
    "                                depth=args.depth,\n",
    "                                dropout=args.dropout,\n",
    "                                undirected=args.undirected,\n",
    "                                dense=args.dense,\n",
    "                                aggregate_to_atom=False,\n",
    "                                attach_fea=False,\n",
    "                                input_layer=\"none\",\n",
    "                                dynamic_depth=\"truncnorm\")\n",
    "        self.mpn_k = MPNEncoder(args=args,\n",
    "                                atom_messages=atom_messages,\n",
    "                                init_message_dim=init_message_dim,\n",
    "                                attached_fea_fdim=attached_fea_dim,\n",
    "                                hidden_size=hidden_size,\n",
    "                                bias=args.bias,\n",
    "                                depth=args.depth,\n",
    "                                dropout=args.dropout,\n",
    "                                undirected=args.undirected,\n",
    "                                dense=args.dense,\n",
    "                                aggregate_to_atom=False,\n",
    "                                attach_fea=False,\n",
    "                                input_layer=\"none\",\n",
    "                                dynamic_depth=\"truncnorm\")\n",
    "        self.mpn_v = MPNEncoder(args=args,\n",
    "                                atom_messages=atom_messages,\n",
    "                                init_message_dim=init_message_dim,\n",
    "                                attached_fea_fdim=attached_fea_dim,\n",
    "                                hidden_size=hidden_size,\n",
    "                                bias=args.bias,\n",
    "                                depth=args.depth,\n",
    "                                dropout=args.dropout,\n",
    "                                undirected=args.undirected,\n",
    "                                dense=args.dense,\n",
    "                                aggregate_to_atom=False,\n",
    "                                attach_fea=False,\n",
    "                                input_layer=\"none\",\n",
    "                                dynamic_depth=\"truncnorm\")\n",
    "\n",
    "    def forward(self, f_atoms, f_bonds, a2b, a2a, b2a, b2revb):\n",
    "        \"\"\"\n",
    "        The forward function.\n",
    "        :param f_atoms: the atom features, num_atoms * atom_dim\n",
    "        :param f_bonds: the bond features, num_bonds * bond_dim\n",
    "        :param a2b: mapping from atom index to incoming bond indices.\n",
    "        :param a2a: mapping from atom index to its neighbors. num_atoms * max_num_bonds\n",
    "        :param b2a: mapping from bond index to the index of the atom the bond is coming from.\n",
    "        :param b2revb: mapping from bond index to the index of the reverse bond.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.atom_messages:\n",
    "            init_messages = f_atoms\n",
    "            init_attached_features = f_bonds\n",
    "            a2nei = a2a\n",
    "            a2attached = a2b\n",
    "            b2a = b2a\n",
    "            b2revb = b2revb\n",
    "        else:\n",
    "            init_messages = f_bonds\n",
    "            init_attached_features = f_atoms\n",
    "            a2nei = a2b\n",
    "            a2attached = a2a\n",
    "            b2a = b2a\n",
    "            b2revb = b2revb\n",
    "\n",
    "        q = self.mpn_q(init_messages=init_messages,\n",
    "                       init_attached_features=init_attached_features,\n",
    "                       a2nei=a2nei,\n",
    "                       a2attached=a2attached,\n",
    "                       b2a=b2a,\n",
    "                       b2revb=b2revb)\n",
    "        k = self.mpn_k(init_messages=init_messages,\n",
    "                       init_attached_features=init_attached_features,\n",
    "                       a2nei=a2nei,\n",
    "                       a2attached=a2attached,\n",
    "                       b2a=b2a,\n",
    "                       b2revb=b2revb)\n",
    "        v = self.mpn_v(init_messages=init_messages,\n",
    "                       init_attached_features=init_attached_features,\n",
    "                       a2nei=a2nei,\n",
    "                       a2attached=a2attached,\n",
    "                       b2a=b2a,\n",
    "                       b2revb=b2revb)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d01a26",
   "metadata": {},
   "source": [
    "#### 2-1-3-3 MTBLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b94fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.nn_utils import get_activation_function, select_neighbor_and_aggregate\n",
    "from torch.nn import LayerNorm, functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f3d2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The Multi-headed attention block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 num_attn_head,\n",
    "                 input_dim,\n",
    "                 hidden_size,\n",
    "                 activation=\"ReLU\",\n",
    "                 dropout=0.0,\n",
    "                 bias=True,\n",
    "                 atom_messages=False,\n",
    "                 cuda=True,\n",
    "                 res_connection=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args: the arguments.\n",
    "        :param num_attn_head: the number of attention head.\n",
    "        :param input_dim: the input dimension.\n",
    "        :param hidden_size: the hidden size of the model.\n",
    "        :param activation: the activation function.\n",
    "        :param dropout: the dropout ratio\n",
    "        :param bias: if true: all linear layer contains bias term.\n",
    "        :param atom_messages: the MPNEncoder type\n",
    "        :param cuda: if true, the model run with GPU.\n",
    "        :param res_connection: enables the skip-connection in MTBlock.\n",
    "        \"\"\"\n",
    "        super(MTBlock, self).__init__()\n",
    "        # self.args = args\n",
    "        self.atom_messages = atom_messages\n",
    "        self.hidden_size = hidden_size\n",
    "        self.heads = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.cuda = cuda\n",
    "        self.res_connection = res_connection\n",
    "        self.act_func = get_activation_function(activation)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        # Note: elementwise_affine has to be consistent with the pre-training phase\n",
    "        self.layernorm = nn.LayerNorm(self.hidden_size, elementwise_affine=True)\n",
    "\n",
    "        self.W_i = nn.Linear(self.input_dim, self.hidden_size, bias=bias)\n",
    "        self.attn = MultiHeadedAttention(h=num_attn_head,\n",
    "                                         d_model=self.hidden_size,\n",
    "                                         bias=bias,\n",
    "                                         dropout=dropout)\n",
    "        self.W_o = nn.Linear(self.hidden_size * num_attn_head, self.hidden_size, bias=bias)\n",
    "        self.sublayer = SublayerConnection(self.hidden_size, dropout)\n",
    "        for _ in range(num_attn_head):\n",
    "            self.heads.append(Head(args, hidden_size=hidden_size, atom_messages=atom_messages))\n",
    "\n",
    "    def forward(self, batch, features_batch=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param batch: the graph batch generated by GroverCollator.\n",
    "        :param features_batch: the additional features of molecules. (deprecated)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = batch\n",
    "\n",
    "        if self.atom_messages:\n",
    "            # Only add linear transformation in the input feature.\n",
    "            if f_atoms.shape[1] != self.hidden_size:\n",
    "                f_atoms = self.W_i(f_atoms)\n",
    "                f_atoms = self.dropout_layer(self.layernorm(self.act_func(f_atoms)))\n",
    "\n",
    "        else:  # bond messages\n",
    "            if f_bonds.shape[1] != self.hidden_size:\n",
    "                f_bonds = self.W_i(f_bonds)\n",
    "                f_bonds = self.dropout_layer(self.layernorm(self.act_func(f_bonds)))\n",
    "\n",
    "        queries = []\n",
    "        keys = []\n",
    "        values = []\n",
    "        for head in self.heads:\n",
    "            q, k, v = head(f_atoms, f_bonds, a2b, a2a, b2a, b2revb)\n",
    "            queries.append(q.unsqueeze(1))\n",
    "            keys.append(k.unsqueeze(1))\n",
    "            values.append(v.unsqueeze(1))\n",
    "        queries = torch.cat(queries, dim=1)\n",
    "        keys = torch.cat(keys, dim=1)\n",
    "        values = torch.cat(values, dim=1)\n",
    "\n",
    "        x_out = self.attn(queries, keys, values)  # multi-headed attention\n",
    "        x_out = x_out.view(x_out.shape[0], -1)\n",
    "        x_out = self.W_o(x_out)\n",
    "\n",
    "        x_in = None\n",
    "        # support no residual connection in MTBlock.\n",
    "        if self.res_connection:\n",
    "            if self.atom_messages:\n",
    "                x_in = f_atoms\n",
    "            else:\n",
    "                x_in = f_bonds\n",
    "\n",
    "        if self.atom_messages:\n",
    "            f_atoms = self.sublayer(x_in, x_out)\n",
    "        else:\n",
    "            f_bonds = self.sublayer(x_in, x_out)\n",
    "\n",
    "        batch = f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a\n",
    "        features_batch = features_batch\n",
    "        return batch, features_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73c6bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Implements FFN equation.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, activation=\"PReLU\", dropout=0.1, d_out=None):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        :param d_model: the input dimension.\n",
    "        :param d_ff: the hidden dimension.\n",
    "        :param activation: the activation function.\n",
    "        :param dropout: the dropout rate.\n",
    "        :param d_out: the output dimension, the default value is equal to d_model.\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        if d_out is None:\n",
    "            d_out = d_model\n",
    "        # By default, bias is on.\n",
    "        self.W_1 = nn.Linear(d_model, d_ff)\n",
    "        self.W_2 = nn.Linear(d_ff, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act_func = get_activation_function(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function\n",
    "        :param x: input tensor.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.W_2(self.dropout(self.act_func(self.W_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "362e7a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        :param size: the input dimension.\n",
    "        :param dropout: the dropout ratio.\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, outputs):\n",
    "        \"\"\"Apply residual connection to any sublayer with the same size.\"\"\"\n",
    "        # return x + self.dropout(self.norm(x))\n",
    "        if inputs is None:\n",
    "            return self.dropout(self.norm(outputs))\n",
    "        return inputs + self.dropout(self.norm(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d92bfd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_select_nd(source: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Selects the message features from source corresponding to the atom or bond indices in index.\n",
    "\n",
    "    :param source: A tensor of shape (num_bonds, hidden_size) containing message features.\n",
    "    :param index: A tensor of shape (num_atoms/num_bonds, max_num_bonds) containing the atom or bond\n",
    "    indices to select from source.\n",
    "    :return: A tensor of shape (num_atoms/num_bonds, max_num_bonds, hidden_size) containing the message\n",
    "    features corresponding to the atoms/bonds specified in index.\n",
    "    \"\"\"\n",
    "    index_size = index.size()  # (num_atoms/num_bonds, max_num_bonds)\n",
    "    suffix_dim = source.size()[1:]  # (hidden_size,)\n",
    "    final_size = index_size + suffix_dim  # (num_atoms/num_bonds, max_num_bonds, hidden_size)\n",
    "\n",
    "    target = source.index_select(dim=0, index=index.view(-1))  # (num_atoms/num_bonds * max_num_bonds, hidden_size)\n",
    "    target = target.view(final_size)  # (num_atoms/num_bonds, max_num_bonds, hidden_size)\n",
    "\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008344bf",
   "metadata": {},
   "source": [
    "#### 2-1-4-4. GTransEncoder and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e68f5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e86cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTransEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 hidden_size,\n",
    "                 edge_fdim,\n",
    "                 node_fdim,\n",
    "                 dropout=0.0,\n",
    "                 activation=\"ReLU\",\n",
    "                 num_mt_block=1,\n",
    "                 num_attn_head=4,\n",
    "                 atom_emb_output: Union[bool, str] = False,  # options: True, False, None, \"atom\", \"bond\", \"both\"\n",
    "                 bias=False,\n",
    "                 cuda=True,\n",
    "                 res_connection=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args: the arguments.\n",
    "        :param hidden_size: the hidden size of the model.\n",
    "        :param edge_fdim: the dimension of additional feature for edge/bond.\n",
    "        :param node_fdim: the dimension of additional feature for node/atom.\n",
    "        :param dropout: the dropout ratio\n",
    "        :param activation: the activation function\n",
    "        :param num_mt_block: the number of mt block.\n",
    "        :param num_attn_head: the number of attention head.\n",
    "        :param atom_emb_output:  enable the output aggregation after message passing.\n",
    "                                              atom_messages:      True                      False\n",
    "        -False: no aggregating to atom. output size:     (num_atoms, hidden_size)    (num_bonds, hidden_size)\n",
    "        -True:  aggregating to atom.    output size:     (num_atoms, hidden_size)    (num_atoms, hidden_size)\n",
    "        -None:                         same as False\n",
    "        -\"atom\":                       same as True\n",
    "        -\"bond\": aggragating to bond.   output size:     (num_bonds, hidden_size)    (num_bonds, hidden_size)\n",
    "        -\"both\": aggregating to atom&bond. output size:  (num_atoms, hidden_size)    (num_bonds, hidden_size)\n",
    "                                                         (num_bonds, hidden_size)    (num_atoms, hidden_size)\n",
    "        :param bias: enable bias term in all linear layers.\n",
    "        :param cuda: run with cuda.\n",
    "        :param res_connection: enables the skip-connection in MTBlock.\n",
    "        \"\"\"\n",
    "        super(GTransEncoder, self).__init__()\n",
    "\n",
    "        # For the compatibility issue.\n",
    "        if atom_emb_output is False:\n",
    "            atom_emb_output = None\n",
    "        if atom_emb_output is True:\n",
    "            atom_emb_output = 'atom'\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.cuda = cuda\n",
    "        self.bias = bias\n",
    "        self.res_connection = res_connection\n",
    "        self.edge_blocks = nn.ModuleList()\n",
    "        self.node_blocks = nn.ModuleList()\n",
    "\n",
    "        edge_input_dim = edge_fdim\n",
    "        node_input_dim = node_fdim\n",
    "        edge_input_dim_i = edge_input_dim\n",
    "        node_input_dim_i = node_input_dim\n",
    "\n",
    "        for i in range(num_mt_block):\n",
    "            if i != 0:\n",
    "                edge_input_dim_i = self.hidden_size\n",
    "                node_input_dim_i = self.hidden_size\n",
    "            self.edge_blocks.append(MTBlock(args=args,\n",
    "                                            num_attn_head=num_attn_head,\n",
    "                                            input_dim=edge_input_dim_i,\n",
    "                                            hidden_size=self.hidden_size,\n",
    "                                            activation=activation,\n",
    "                                            dropout=dropout,\n",
    "                                            bias=self.bias,\n",
    "                                            atom_messages=False,\n",
    "                                            cuda=cuda))\n",
    "            self.node_blocks.append(MTBlock(args=args,\n",
    "                                            num_attn_head=num_attn_head,\n",
    "                                            input_dim=node_input_dim_i,\n",
    "                                            hidden_size=self.hidden_size,\n",
    "                                            activation=activation,\n",
    "                                            dropout=dropout,\n",
    "                                            bias=self.bias,\n",
    "                                            atom_messages=True,\n",
    "                                            cuda=cuda))\n",
    "\n",
    "        self.atom_emb_output = atom_emb_output\n",
    "\n",
    "        self.ffn_atom_from_atom = PositionwiseFeedForward(self.hidden_size + node_fdim,\n",
    "                                                          self.hidden_size * 4,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          d_out=self.hidden_size)\n",
    "\n",
    "        self.ffn_atom_from_bond = PositionwiseFeedForward(self.hidden_size + node_fdim,\n",
    "                                                          self.hidden_size * 4,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          d_out=self.hidden_size)\n",
    "\n",
    "        self.ffn_bond_from_atom = PositionwiseFeedForward(self.hidden_size + edge_fdim,\n",
    "                                                          self.hidden_size * 4,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          d_out=self.hidden_size)\n",
    "\n",
    "        self.ffn_bond_from_bond = PositionwiseFeedForward(self.hidden_size + edge_fdim,\n",
    "                                                          self.hidden_size * 4,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          d_out=self.hidden_size)\n",
    "\n",
    "        self.atom_from_atom_sublayer = SublayerConnection(size=self.hidden_size, dropout=self.dropout)\n",
    "        self.atom_from_bond_sublayer = SublayerConnection(size=self.hidden_size, dropout=self.dropout)\n",
    "        self.bond_from_atom_sublayer = SublayerConnection(size=self.hidden_size, dropout=self.dropout)\n",
    "        self.bond_from_bond_sublayer = SublayerConnection(size=self.hidden_size, dropout=self.dropout)\n",
    "\n",
    "        self.act_func_node = get_activation_function(self.activation)\n",
    "        self.act_func_edge = get_activation_function(self.activation)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=args.dropout)\n",
    "\n",
    "    def pointwise_feed_forward_to_atom_embedding(self, emb_output, atom_fea, index, ffn_layer):\n",
    "        \"\"\"\n",
    "        The point-wise feed forward and long-range residual connection for atom view.\n",
    "        aggregate to atom.\n",
    "        :param emb_output: the output embedding from the previous multi-head attentions.\n",
    "        :param atom_fea: the atom/node feature embedding.\n",
    "        :param index: the index of neighborhood relations.\n",
    "        :param ffn_layer: the feed forward layer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        aggr_output = select_neighbor_and_aggregate(emb_output, index)\n",
    "        aggr_outputx = torch.cat([atom_fea, aggr_output], dim=1)\n",
    "        return ffn_layer(aggr_outputx), aggr_output\n",
    "\n",
    "    def pointwise_feed_forward_to_bond_embedding(self, emb_output, bond_fea, a2nei, b2revb, ffn_layer):\n",
    "        \"\"\"\n",
    "        The point-wise feed forward and long-range residual connection for bond view.\n",
    "        aggregate to bond.\n",
    "        :param emb_output: the output embedding from the previous multi-head attentions.\n",
    "        :param bond_fea: the bond/edge feature embedding.\n",
    "        :param index: the index of neighborhood relations.\n",
    "        :param ffn_layer: the feed forward layer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        aggr_output = select_neighbor_and_aggregate(emb_output, a2nei)\n",
    "        # remove rev bond / atom --- need for bond view\n",
    "        aggr_output = self.remove_rev_bond_message(emb_output, aggr_output, b2revb)\n",
    "        aggr_outputx = torch.cat([bond_fea, aggr_output], dim=1)\n",
    "        return ffn_layer(aggr_outputx), aggr_output\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_rev_bond_message(orginal_message, aggr_message, b2revb):\n",
    "        \"\"\"\n",
    "\n",
    "        :param orginal_message:\n",
    "        :param aggr_message:\n",
    "        :param b2revb:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rev_message = orginal_message[b2revb]\n",
    "        return aggr_message - rev_message\n",
    "\n",
    "    def atom_bond_transform(self,\n",
    "                            to_atom=True,  # False: to bond\n",
    "                            atomwise_input=None,\n",
    "                            bondwise_input=None,\n",
    "                            original_f_atoms=None,\n",
    "                            original_f_bonds=None,\n",
    "                            a2a=None,\n",
    "                            a2b=None,\n",
    "                            b2a=None,\n",
    "                            b2revb=None\n",
    "                            ):\n",
    "        \"\"\"\n",
    "        Transfer the output of atom/bond multi-head attention to the final atom/bond output.\n",
    "        :param to_atom: if true, the output is atom emebedding, otherwise, the output is bond embedding.\n",
    "        :param atomwise_input: the input embedding of atom/node.\n",
    "        :param bondwise_input: the input embedding of bond/edge.\n",
    "        :param original_f_atoms: the initial atom features.\n",
    "        :param original_f_bonds: the initial bond features.\n",
    "        :param a2a: mapping from atom index to its neighbors. num_atoms * max_num_bonds\n",
    "        :param a2b: mapping from atom index to incoming bond indices.\n",
    "        :param b2a: mapping from bond index to the index of the atom the bond is coming from.\n",
    "        :param b2revb: mapping from bond index to the index of the reverse bond.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if to_atom:\n",
    "            # atom input to atom output\n",
    "            atomwise_input, _ = self.pointwise_feed_forward_to_atom_embedding(atomwise_input, original_f_atoms, a2a,\n",
    "                                                                              self.ffn_atom_from_atom)\n",
    "            atom_in_atom_out = self.atom_from_atom_sublayer(None, atomwise_input)\n",
    "            # bond to atom\n",
    "            bondwise_input, _ = self.pointwise_feed_forward_to_atom_embedding(bondwise_input, original_f_atoms, a2b,\n",
    "                                                                              self.ffn_atom_from_bond)\n",
    "            bond_in_atom_out = self.atom_from_bond_sublayer(None, bondwise_input)\n",
    "            return atom_in_atom_out, bond_in_atom_out\n",
    "        else:  # to bond embeddings\n",
    "\n",
    "            # atom input to bond output\n",
    "            atom_list_for_bond = torch.cat([b2a.unsqueeze(dim=1), a2a[b2a]], dim=1)\n",
    "            atomwise_input, _ = self.pointwise_feed_forward_to_bond_embedding(atomwise_input, original_f_bonds,\n",
    "                                                                              atom_list_for_bond,\n",
    "                                                                              b2a[b2revb], self.ffn_bond_from_atom)\n",
    "            atom_in_bond_out = self.bond_from_atom_sublayer(None, atomwise_input)\n",
    "            # bond input to bond output\n",
    "            bond_list_for_bond = a2b[b2a]\n",
    "            bondwise_input, _ = self.pointwise_feed_forward_to_bond_embedding(bondwise_input, original_f_bonds,\n",
    "                                                                              bond_list_for_bond,\n",
    "                                                                              b2revb, self.ffn_bond_from_bond)\n",
    "            bond_in_bond_out = self.bond_from_bond_sublayer(None, bondwise_input)\n",
    "            return atom_in_bond_out, bond_in_bond_out\n",
    "\n",
    "    def forward(self, batch, features_batch = None):\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = batch\n",
    "        if self.cuda or next(self.parameters()).is_cuda:\n",
    "            f_atoms, f_bonds, a2b, b2a, b2revb = f_atoms.cuda(), f_bonds.cuda(), a2b.cuda(), b2a.cuda(), b2revb.cuda()\n",
    "            a2a = a2a.cuda()\n",
    "\n",
    "        node_batch = f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a\n",
    "        edge_batch = f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a\n",
    "\n",
    "        # opt pointwise_feed_forward\n",
    "        original_f_atoms, original_f_bonds = f_atoms, f_bonds\n",
    "\n",
    "        # Note: features_batch is not used here.\n",
    "        for nb in self.node_blocks:  # atom messages. Multi-headed attention\n",
    "            node_batch, features_batch = nb(node_batch, features_batch)\n",
    "        for eb in self.edge_blocks:  # bond messages. Multi-headed attention\n",
    "            edge_batch, features_batch = eb(edge_batch, features_batch)\n",
    "\n",
    "        atom_output, _, _, _, _, _, _, _ = node_batch  # atom hidden states\n",
    "        _, bond_output, _, _, _, _, _, _ = edge_batch  # bond hidden states\n",
    "\n",
    "        if self.atom_emb_output is None:\n",
    "            # output the embedding from multi-head attention directly.\n",
    "            return atom_output, bond_output\n",
    "\n",
    "        if self.atom_emb_output == 'atom':\n",
    "            return self.atom_bond_transform(to_atom=True,  # False: to bond\n",
    "                                            atomwise_input=atom_output,\n",
    "                                            bondwise_input=bond_output,\n",
    "                                            original_f_atoms=original_f_atoms,\n",
    "                                            original_f_bonds=original_f_bonds,\n",
    "                                            a2a=a2a,\n",
    "                                            a2b=a2b,\n",
    "                                            b2a=b2a,\n",
    "                                            b2revb=b2revb)\n",
    "        elif self.atom_emb_output == 'bond':\n",
    "            return self.atom_bond_transform(to_atom=False,  # False: to bond\n",
    "                                            atomwise_input=atom_output,\n",
    "                                            bondwise_input=bond_output,\n",
    "                                            original_f_atoms=original_f_atoms,\n",
    "                                            original_f_bonds=original_f_bonds,\n",
    "                                            a2a=a2a,\n",
    "                                            a2b=a2b,\n",
    "                                            b2a=b2a,\n",
    "                                            b2revb=b2revb)\n",
    "        else:  # 'both'\n",
    "            atom_embeddings = self.atom_bond_transform(to_atom=True,  # False: to bond\n",
    "                                                       atomwise_input=atom_output,\n",
    "                                                       bondwise_input=bond_output,\n",
    "                                                       original_f_atoms=original_f_atoms,\n",
    "                                                       original_f_bonds=original_f_bonds,\n",
    "                                                       a2a=a2a,\n",
    "                                                       a2b=a2b,\n",
    "                                                       b2a=b2a,\n",
    "                                                       b2revb=b2revb)\n",
    "\n",
    "            bond_embeddings = self.atom_bond_transform(to_atom=False,  # False: to bond\n",
    "                                                       atomwise_input=atom_output,\n",
    "                                                       bondwise_input=bond_output,\n",
    "                                                       original_f_atoms=original_f_atoms,\n",
    "                                                       original_f_bonds=original_f_bonds,\n",
    "                                                       a2a=a2a,\n",
    "                                                       a2b=a2b,\n",
    "                                                       b2a=b2a,\n",
    "                                                       b2revb=b2revb)\n",
    "            # Notice: need to be consistent with output format of DualMPNN encoder\n",
    "            return ((atom_embeddings[0], bond_embeddings[0]),\n",
    "                    (atom_embeddings[1], bond_embeddings[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80217a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GROVEREmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    The GROVER Embedding class. It contains the GTransEncoder.\n",
    "    This GTransEncoder can be replaced by any validate encoders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args: Namespace):\n",
    "        \"\"\"\n",
    "        Initialize the GROVEREmbedding class.\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(GROVEREmbedding, self).__init__()\n",
    "        self.embedding_output_type = args.embedding_output_type\n",
    "        edge_dim = get_bond_fdim() + get_atom_fdim()  # fdim에 대한건 4-3-4-1. mol2graph()참조\n",
    "        node_dim = get_atom_fdim()\n",
    "        if not hasattr(args, \"backbone\"):\n",
    "            print(\"No backbone specified in args, use gtrans backbone.\")\n",
    "            args.backbone = \"gtrans\"\n",
    "        if args.backbone == \"gtrans\" or args.backbone == \"dualtrans\":\n",
    "            # dualtrans is the old name.\n",
    "            self.encoders = GTransEncoder(args,\n",
    "                                          hidden_size=args.hidden_size,\n",
    "                                          edge_fdim=edge_dim,\n",
    "                                          node_fdim=node_dim,\n",
    "                                          dropout=args.dropout,\n",
    "                                          activation=args.activation,\n",
    "                                          num_mt_block=args.num_mt_block,\n",
    "                                          num_attn_head=args.num_attn_head,\n",
    "                                          atom_emb_output=self.embedding_output_type,\n",
    "                                          bias=args.bias,\n",
    "                                          cuda=args.cuda)\n",
    "\n",
    "    def forward(self, graph_batch: List) -> Dict:\n",
    "        \"\"\"\n",
    "        The forward function takes graph_batch as input and output a dict. The content of the dict is decided by\n",
    "        self.embedding_output_type.\n",
    "\n",
    "        :param graph_batch: the input graph batch generated by MolCollator.\n",
    "        :return: a dict containing the embedding results.\n",
    "        \"\"\"\n",
    "        output = self.encoders(graph_batch)\n",
    "        if self.embedding_output_type == 'atom':\n",
    "            return {\"atom_from_atom\": output[0], \"atom_from_bond\": output[1],\n",
    "                    \"bond_from_atom\": None, \"bond_from_bond\": None}  # atom_from_atom, atom_from_bond\n",
    "        elif self.embedding_output_type == 'bond':\n",
    "            return {\"atom_from_atom\": None, \"atom_from_bond\": None,\n",
    "                    \"bond_from_atom\": output[0], \"bond_from_bond\": output[1]}  # bond_from_atom, bond_from_bond\n",
    "        elif self.embedding_output_type == \"both\":\n",
    "            return {\"atom_from_atom\": output[0][0], \"bond_from_atom\": output[0][1],\n",
    "                    \"atom_from_bond\": output[1][0], \"bond_from_bond\": output[1][1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcda1d6",
   "metadata": {},
   "source": [
    "### 2-1-4. GROVERTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8ee3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from logging import Logger\n",
    "from typing import List, Tuple\n",
    "from collections.abc import Callable\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from grover.model.models import GroverTask\n",
    "from grover.util.multi_gpu_wrapper import MultiGpuWrapper as mgw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45c597",
   "metadata": {},
   "source": [
    "#### 2-2-1-1. GroverTask() *중요! Loss함수코드\n",
    "- 순서 : batch의 graph를 embedding_model=grover_model=GROVEREmbedding(args)를 통과시킨다. 이건 finetune의 인코더와 똑같다.\n",
    "\n",
    "- 이때 embedding은 both로 5개의 결과가 나온다. atom/bond from bond/atom과 fg_task_all\n",
    "\n",
    "- 손실함수\n",
    "  - av_loss, bv_loss : NLLLoss(pred, target)\n",
    "  - fg_atom, bond : BinaryCrossEntropyWithLogitsLoss(pred, target)\n",
    "  - dist_loss : MSELoss(atom or bond from atom/bond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77e32bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "\n",
    "from grover.data import get_atom_fdim, get_bond_fdim\n",
    "from grover.model.layers import Readout, GTransEncoder\n",
    "from grover.util.nn_utils import get_activation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bd95e2",
   "metadata": {},
   "source": [
    "##### 2-2-1-1-1. Atom, Bond, FG predict\n",
    "- 바로 예측을 하네.\n",
    "- 그리고 예측값에 logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de94d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomVocabPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    The atom-wise vocabulary prediction task. The atom vocabulary is constructed by the context.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, vocab_size, hidden_size=None):\n",
    "        \"\"\"\n",
    "        :param args: the argument.\n",
    "        :param vocab_size: the size of atom vocabulary.\n",
    "        \"\"\"\n",
    "        super(AtomVocabPrediction, self).__init__()\n",
    "        if not hidden_size:\n",
    "            hidden_size = args.hidden_size\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        If embeddings is None: do not go through forward pass.\n",
    "        :param embeddings: the atom embeddings, num_atom X fea_dim.\n",
    "        :return: the prediction for each atom, num_atom X vocab_size.\n",
    "        \"\"\"\n",
    "        if embeddings is None:\n",
    "            return None\n",
    "        return self.logsoftmax(self.linear(embeddings))\n",
    "    \n",
    "class BondVocabPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    The bond-wise vocabulary prediction task. The bond vocabulary is constructed by the context.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, vocab_size, hidden_size=None):\n",
    "        \"\"\"\n",
    "        Might need to use different architecture for bond vocab prediction.\n",
    "        :param args:\n",
    "        :param vocab_size: size of bond vocab.\n",
    "        :param hidden_size: hidden size\n",
    "        \"\"\"\n",
    "        super(BondVocabPrediction, self).__init__()\n",
    "        if not hidden_size:\n",
    "            hidden_size = args.hidden_size\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # ad-hoc here\n",
    "        # If TWO_FC_4_BOND_VOCAB, we will use two distinct fc layer to deal with the bond and rev bond.\n",
    "        self.TWO_FC_4_BOND_VOCAB = True\n",
    "        if self.TWO_FC_4_BOND_VOCAB:\n",
    "            self.linear_rev = nn.Linear(hidden_size, vocab_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        If embeddings is None: do not go through forward pass.\n",
    "        :param embeddings: the atom embeddings, num_bond X fea_dim.\n",
    "        :return: the prediction for each atom, num_bond X vocab_size.\n",
    "        \"\"\"\n",
    "        if embeddings is None:\n",
    "            return None\n",
    "        nm_bonds = embeddings.shape[0]  # must be an odd number\n",
    "        # The bond and rev bond have odd and even ids respectively. See definition in molgraph.\n",
    "        ids1 = [0] + list(range(1, nm_bonds, 2))\n",
    "        ids2 = list(range(0, nm_bonds, 2))\n",
    "        if self.TWO_FC_4_BOND_VOCAB:\n",
    "            logits = self.linear(embeddings[ids1]) + self.linear_rev(embeddings[ids2])\n",
    "        else:\n",
    "            logits = self.linear(embeddings[ids1] + embeddings[ids2])\n",
    "\n",
    "        return self.logsoftmax(logits)\n",
    "\n",
    "\n",
    "class FunctionalGroupPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    The functional group (semantic motifs) prediction task. This is a graph-level task.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, fg_size):\n",
    "        \"\"\"\n",
    "        :param args: The arguments.\n",
    "        :param fg_size: The size of semantic motifs.\n",
    "        \"\"\"\n",
    "        super(FunctionalGroupPrediction, self).__init__()\n",
    "        first_linear_dim = args.hidden_size\n",
    "        hidden_size = args.hidden_size\n",
    "\n",
    "        # In order to retain maximal information in the encoder, we use a simple readout function here.\n",
    "        self.readout = Readout(rtype=\"mean\", hidden_size=hidden_size)\n",
    "        # We have four branches here. But the input with less than four branch is OK.\n",
    "        # Since we use BCEWithLogitsLoss as the loss function, we only need to output logits here.\n",
    "        self.linear_atom_from_atom = nn.Linear(first_linear_dim, fg_size)\n",
    "        self.linear_atom_from_bond = nn.Linear(first_linear_dim, fg_size)\n",
    "        self.linear_bond_from_atom = nn.Linear(first_linear_dim, fg_size)\n",
    "        self.linear_bond_from_bond = nn.Linear(first_linear_dim, fg_size)\n",
    "\n",
    "    def forward(self, embeddings: Dict, ascope: List, bscope: List) -> Dict:\n",
    "        \"\"\"\n",
    "        The forward function of semantic motif prediction. It takes the node/bond embeddings, and the corresponding\n",
    "        atom/bond scope as input and produce the prediction logits for different branches.\n",
    "        :param embeddings: The input embeddings are organized as dict. The output of GROVEREmbedding.\n",
    "        :param ascope: The scope for bonds. Please refer BatchMolGraph for more details.\n",
    "        :param bscope: The scope for aotms. Please refer BatchMolGraph for more details.\n",
    "        :return: a dict contains the predicted logits.\n",
    "        \"\"\"\n",
    "\n",
    "        preds_atom_from_atom, preds_atom_from_bond, preds_bond_from_atom, preds_bond_from_bond = \\\n",
    "            None, None, None, None\n",
    "\n",
    "        if embeddings[\"bond_from_atom\"] is not None:\n",
    "            preds_bond_from_atom = self.linear_bond_from_atom(self.readout(embeddings[\"bond_from_atom\"], bscope))\n",
    "        if embeddings[\"bond_from_bond\"] is not None:\n",
    "            preds_bond_from_bond = self.linear_bond_from_bond(self.readout(embeddings[\"bond_from_bond\"], bscope))\n",
    "\n",
    "        if embeddings[\"atom_from_atom\"] is not None:\n",
    "            preds_atom_from_atom = self.linear_atom_from_atom(self.readout(embeddings[\"atom_from_atom\"], ascope))\n",
    "        if embeddings[\"atom_from_bond\"] is not None:\n",
    "            preds_atom_from_bond = self.linear_atom_from_bond(self.readout(embeddings[\"atom_from_bond\"], ascope))\n",
    "\n",
    "        return {\"atom_from_atom\": preds_atom_from_atom, \"atom_from_bond\": preds_atom_from_bond,\n",
    "                \"bond_from_atom\": preds_bond_from_atom, \"bond_from_bond\": preds_bond_from_bond}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "763adbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroverTask(nn.Module):\n",
    "    \"\"\"\n",
    "    The pretrain module.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, grover, atom_vocab_size, bond_vocab_size, fg_size):\n",
    "        super(GroverTask, self).__init__()\n",
    "        self.grover = grover\n",
    "        self.av_task_atom = AtomVocabPrediction(args, atom_vocab_size)\n",
    "        self.av_task_bond = AtomVocabPrediction(args, atom_vocab_size)\n",
    "        self.bv_task_atom = BondVocabPrediction(args, bond_vocab_size)\n",
    "        self.bv_task_bond = BondVocabPrediction(args, bond_vocab_size)\n",
    "\n",
    "        self.fg_task_all = FunctionalGroupPrediction(args, fg_size)\n",
    "\n",
    "        self.embedding_output_type = args.embedding_output_type\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_func(args: Namespace) -> Callable:\n",
    "        \"\"\"\n",
    "        The loss function generator.\n",
    "        :param args: the arguments.\n",
    "        :return: the loss fucntion for GroverTask.\n",
    "        \"\"\"\n",
    "        def loss_func(preds, targets, dist_coff=args.dist_coff):\n",
    "            \"\"\"\n",
    "            The loss function for GroverTask.\n",
    "            :param preds: the predictions.\n",
    "            :param targets: the targets.\n",
    "            :param dist_coff: the default disagreement coefficient for the distances between different branches.\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            av_task_loss = nn.NLLLoss(ignore_index=0, reduction=\"mean\")  # same for av and bv\n",
    "\n",
    "            fg_task_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            # av_task_dist_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "            av_task_dist_loss = nn.MSELoss(reduction=\"mean\")\n",
    "            fg_task_dist_loss = nn.MSELoss(reduction=\"mean\")\n",
    "            sigmoid = nn.Sigmoid()\n",
    "\n",
    "            av_atom_loss, av_bond_loss, av_dist_loss = 0.0, 0.0, 0.0\n",
    "            fg_atom_from_atom_loss, fg_atom_from_bond_loss, fg_atom_dist_loss = 0.0, 0.0, 0.0\n",
    "            bv_atom_loss, bv_bond_loss, bv_dist_loss = 0.0, 0.0, 0.0\n",
    "            fg_bond_from_atom_loss, fg_bond_from_bond_loss, fg_bond_dist_loss = 0.0, 0.0, 0.0\n",
    "\n",
    "            if preds[\"av_task\"][0] is not None:\n",
    "                av_atom_loss = av_task_loss(preds['av_task'][0], targets[\"av_task\"])\n",
    "                fg_atom_from_atom_loss = fg_task_loss(preds[\"fg_task\"][\"atom_from_atom\"], targets[\"fg_task\"])\n",
    "\n",
    "            if preds[\"av_task\"][1] is not None:\n",
    "                av_bond_loss = av_task_loss(preds['av_task'][1], targets[\"av_task\"])\n",
    "                fg_atom_from_bond_loss = fg_task_loss(preds[\"fg_task\"][\"atom_from_bond\"], targets[\"fg_task\"])\n",
    "\n",
    "            if preds[\"bv_task\"][0] is not None:\n",
    "                bv_atom_loss = av_task_loss(preds['bv_task'][0], targets[\"bv_task\"])\n",
    "                fg_bond_from_atom_loss = fg_task_loss(preds[\"fg_task\"][\"bond_from_atom\"], targets[\"fg_task\"])\n",
    "\n",
    "            if preds[\"bv_task\"][1] is not None:\n",
    "                bv_bond_loss = av_task_loss(preds['bv_task'][1], targets[\"bv_task\"])\n",
    "                fg_bond_from_bond_loss = fg_task_loss(preds[\"fg_task\"][\"bond_from_bond\"], targets[\"fg_task\"])\n",
    "\n",
    "            if preds[\"av_task\"][0] is not None and preds[\"av_task\"][1] is not None:\n",
    "                av_dist_loss = av_task_dist_loss(preds['av_task'][0], preds['av_task'][1])\n",
    "                fg_atom_dist_loss = fg_task_dist_loss(sigmoid(preds[\"fg_task\"][\"atom_from_atom\"]),\n",
    "                                                      sigmoid(preds[\"fg_task\"][\"atom_from_bond\"]))\n",
    "\n",
    "            if preds[\"bv_task\"][0] is not None and preds[\"bv_task\"][1] is not None:\n",
    "                bv_dist_loss = av_task_dist_loss(preds['bv_task'][0], preds['bv_task'][1])\n",
    "                fg_bond_dist_loss = fg_task_dist_loss(sigmoid(preds[\"fg_task\"][\"bond_from_atom\"]),\n",
    "                                                      sigmoid(preds[\"fg_task\"][\"bond_from_bond\"]))\n",
    "\n",
    "            av_loss = av_atom_loss + av_bond_loss\n",
    "            bv_loss = bv_atom_loss + bv_bond_loss\n",
    "            fg_atom_loss = fg_atom_from_atom_loss + fg_atom_from_bond_loss\n",
    "            fg_bond_loss = fg_bond_from_atom_loss + fg_bond_from_bond_loss\n",
    "\n",
    "            fg_loss = fg_atom_loss + fg_bond_loss\n",
    "            fg_dist_loss = fg_atom_dist_loss + fg_bond_dist_loss\n",
    "\n",
    "            # dist_loss = av_dist_loss + bv_dist_loss + fg_dist_loss\n",
    "            # print(\"%.4f %.4f %.4f %.4f %.4f %.4f\"%(av_atom_loss,\n",
    "            #                                       av_bond_loss,\n",
    "            #                                       fg_atom_loss,\n",
    "            #                                       fg_bond_loss,\n",
    "            #                                       av_dist_loss,\n",
    "            #                                       fg_dist_loss))\n",
    "            # return av_loss + fg_loss + dist_coff * dist_loss\n",
    "            overall_loss = av_loss + bv_loss + fg_loss + dist_coff * av_dist_loss + \\\n",
    "                           dist_coff * bv_dist_loss + fg_dist_loss\n",
    "\n",
    "            return overall_loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss\n",
    "\n",
    "        return loss_func\n",
    "\n",
    "    def forward(self, graph_batch: List):\n",
    "        \"\"\"\n",
    "        The forward function.\n",
    "        :param graph_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        _, _, _, _, _, a_scope, b_scope, _ = graph_batch\n",
    "        a_scope = a_scope.data.cpu().numpy().tolist()\n",
    "\n",
    "        embeddings = self.grover(graph_batch)\n",
    "\n",
    "        av_task_pred_atom = self.av_task_atom(\n",
    "            embeddings[\"atom_from_atom\"])  # if None: means not go through this fowward\n",
    "        av_task_pred_bond = self.av_task_bond(embeddings[\"atom_from_bond\"])\n",
    "\n",
    "        bv_task_pred_atom = self.bv_task_atom(embeddings[\"bond_from_atom\"])\n",
    "        bv_task_pred_bond = self.bv_task_bond(embeddings[\"bond_from_bond\"])\n",
    "\n",
    "        fg_task_pred_all = self.fg_task_all(embeddings, a_scope, b_scope)\n",
    "\n",
    "        return {\"av_task\": (av_task_pred_atom, av_task_pred_bond),\n",
    "                \"bv_task\": (bv_task_pred_atom, bv_task_pred_bond),\n",
    "                \"fg_task\": fg_task_pred_all}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479a64d",
   "metadata": {},
   "source": [
    "#### 2-2-1 GroverTrainer 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6296a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GROVERTrainer:\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 embedding_model: Module,\n",
    "                 atom_vocab_size: int,  # atom vocab size\n",
    "                 bond_vocab_size: int,\n",
    "                 fg_szie: int,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 test_dataloader: DataLoader,\n",
    "                 optimizer_builder: Callable,\n",
    "                 scheduler_builder: Callable,\n",
    "                 logger: Logger = None,\n",
    "                 with_cuda: bool = False,\n",
    "                 enable_multi_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        The init function of GROVERTrainer\n",
    "        :param args: the input arguments.\n",
    "        :param embedding_model: the model to generate atom/bond embeddings.\n",
    "        :param atom_vocab_size: the vocabulary size of atoms.\n",
    "        :param bond_vocab_size: the vocabulary size of bonds.\n",
    "        :param fg_szie: the size of semantic motifs (functional groups)\n",
    "        :param train_dataloader: the data loader of train data.\n",
    "        :param test_dataloader: the data loader of validation data.\n",
    "        :param optimizer_builder: the function of building the optimizer.\n",
    "        :param scheduler_builder: the function of building the scheduler.\n",
    "        :param logger: the logger\n",
    "        :param with_cuda: enable gpu training.\n",
    "        :param enable_multi_gpu: enable multi_gpu traning.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.with_cuda = with_cuda\n",
    "        self.grover = embedding_model\n",
    "        self.model = GroverTask(args, embedding_model, atom_vocab_size, bond_vocab_size, fg_szie)\n",
    "        self.loss_func = self.model.get_loss_func(args)\n",
    "        self.enable_multi_gpu = enable_multi_gpu\n",
    "\n",
    "        self.atom_vocab_size = atom_vocab_size\n",
    "        self.bond_vocab_size = bond_vocab_size\n",
    "        self.debug = logger.debug if logger is not None else print\n",
    "\n",
    "        if self.with_cuda:\n",
    "            # print(\"Using %d GPUs for training.\" % (torch.cuda.device_count()))\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        self.optimizer = optimizer_builder(self.model, self.args)\n",
    "        self.scheduler = scheduler_builder(self.optimizer, self.args)\n",
    "        if self.enable_multi_gpu:\n",
    "            self.optimizer = mgw.DistributedOptimizer(self.optimizer,\n",
    "                                                      named_parameters=self.model.named_parameters())\n",
    "        self.args = args\n",
    "        self.n_iter = 0\n",
    "\n",
    "    def broadcast_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Broadcast parameters before training.\n",
    "        :return: no return.\n",
    "        \"\"\"\n",
    "        if self.enable_multi_gpu:\n",
    "            # broadcast parameters & optimizer state.\n",
    "            mgw.broadcast_parameters(self.model.state_dict(), root_rank=0)\n",
    "            mgw.broadcast_optimizer_state(self.optimizer, root_rank=0)\n",
    "\n",
    "    def train(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The training iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return: the loss terms of current epoch.\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.train_data, train=True)\n",
    "        return self.iter(epoch, self.train_data, train=True)\n",
    "\n",
    "    def test(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The test/validaiion iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return:  the loss terms as a list\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.test_data, train=False)\n",
    "        return self.iter(epoch, self.test_data, train=False)\n",
    "\n",
    "    def mock_iter(self, epoch: int, data_loader: DataLoader, train: bool = True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a mock iteration. For test only.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        for _, _ in enumerate(data_loader):\n",
    "            self.scheduler.step()\n",
    "        cum_loss_sum = 0.0\n",
    "        self.n_iter += self.args.batch_size\n",
    "        return self.n_iter, cum_loss_sum, (0, 0, 0, 0, 0, 0)\n",
    "\n",
    "    def iter(self, epoch, data_loader, train=True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a training / validation iteration.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        loss_sum, iter_count = 0, 0\n",
    "        cum_loss_sum, cum_iter_count = 0, 0\n",
    "        av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum, bv_dist_loss_sum, fg_dist_loss_sum = 0, 0, 0, 0, 0, 0\n",
    "        # loss_func = self.model.get_loss_func(self.args)\n",
    "\n",
    "        for _, item in enumerate(data_loader):\n",
    "            batch_graph = item[\"graph_input\"]\n",
    "            targets = item[\"targets\"]\n",
    "\n",
    "            if next(self.model.parameters()).is_cuda:\n",
    "                targets[\"av_task\"] = targets[\"av_task\"].cuda()\n",
    "                targets[\"bv_task\"] = targets[\"bv_task\"].cuda()\n",
    "                targets[\"fg_task\"] = targets[\"fg_task\"].cuda()\n",
    "\n",
    "            preds = self.model(batch_graph)\n",
    "\n",
    "            # # ad-hoc code, for visualizing a model, comment this block when it is not needed\n",
    "            # import dglt.contrib.grover.vis_model as vis_model\n",
    "            # for task in ['av_task', 'bv_task', 'fg_task']:\n",
    "            #     vis_graph = vis_model.make_dot(self.model(batch_graph)[task],\n",
    "            #                                    params=dict(self.model.named_parameters()))\n",
    "            #     # vis_graph.view()\n",
    "            #     vis_graph.render(f\"{self.args.backbone}_model_{task}_vis.png\", format=\"png\")\n",
    "            # exit()\n",
    "\n",
    "            loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss = self.loss_func(preds, targets)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            iter_count += self.args.batch_size\n",
    "\n",
    "            if train:\n",
    "                cum_loss_sum += loss.item()\n",
    "                # Run model\n",
    "                self.model.zero_grad()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                # For eval model, only consider the loss of three task.\n",
    "                cum_loss_sum += av_loss.item()\n",
    "                cum_loss_sum += bv_loss.item()\n",
    "                cum_loss_sum += fg_loss.item()\n",
    "\n",
    "            av_loss_sum += av_loss.item()\n",
    "            bv_loss_sum += bv_loss.item()\n",
    "            fg_loss_sum += fg_loss.item()\n",
    "            av_dist_loss_sum += av_dist_loss.item() if type(av_dist_loss) != float else av_dist_loss\n",
    "            bv_dist_loss_sum += bv_dist_loss.item() if type(bv_dist_loss) != float else bv_dist_loss\n",
    "            fg_dist_loss_sum += fg_dist_loss.item() if type(fg_dist_loss) != float else fg_dist_loss\n",
    "\n",
    "            cum_iter_count += 1\n",
    "            self.n_iter += self.args.batch_size\n",
    "\n",
    "            # Debug only.\n",
    "            # if i % 50 == 0:\n",
    "            #     print(f\"epoch: {epoch}, batch_id: {i}, av_loss: {av_loss}, bv_loss: {bv_loss}, \"\n",
    "            #           f\"fg_loss: {fg_loss}, av_dist_loss: {av_dist_loss}, bv_dist_loss: {bv_dist_loss}, \"\n",
    "            #           f\"fg_dist_loss: {fg_dist_loss}\")\n",
    "\n",
    "        cum_loss_sum /= cum_iter_count\n",
    "        av_loss_sum /= cum_iter_count\n",
    "        bv_loss_sum /= cum_iter_count\n",
    "        fg_loss_sum /= cum_iter_count\n",
    "        av_dist_loss_sum /= cum_iter_count\n",
    "        bv_dist_loss_sum /= cum_iter_count\n",
    "        fg_dist_loss_sum /= cum_iter_count\n",
    "\n",
    "        return self.n_iter, cum_loss_sum, (av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum,\n",
    "                                           bv_dist_loss_sum, fg_dist_loss_sum)\n",
    "\n",
    "    def save(self, epoch, file_path, name=None) -> str:\n",
    "        \"\"\"\n",
    "        Save the intermediate models during training.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to save the model.\n",
    "        :return: the output path.\n",
    "        \"\"\"\n",
    "        # add specific time in model fine name, in order to distinguish different saved models\n",
    "        now = time.localtime()\n",
    "        if name is None:\n",
    "            name = \"_%04d_%02d_%02d_%02d_%02d_%02d\" % (\n",
    "                now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "        output_path = file_path + name + \".ep%d\" % epoch\n",
    "        scaler = None\n",
    "        features_scaler = None\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch,\n",
    "            'data_scaler': {\n",
    "                'means': scaler.means,\n",
    "                'stds': scaler.stds\n",
    "            } if scaler is not None else None,\n",
    "            'features_scaler': {\n",
    "                'means': features_scaler.means,\n",
    "                'stds': features_scaler.stds\n",
    "            } if features_scaler is not None else None\n",
    "        }\n",
    "        torch.save(state, output_path)\n",
    "\n",
    "        # Is this necessary?\n",
    "        # if self.with_cuda:\n",
    "        #    self.model = self.model.cuda()\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "\n",
    "    def save_tmp(self, epoch, file_path, rank=0):\n",
    "        \"\"\"\n",
    "        Save the models for auto-restore during training.\n",
    "        The model are stored in file_path/tmp folder and will replaced on each epoch.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        store_path = os.path.join(file_path, \"tmp\")\n",
    "        if not os.path.exists(store_path):\n",
    "            os.makedirs(store_path, exist_ok=True)\n",
    "        store_path = os.path.join(store_path, \"model.%d\" % rank)\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        torch.save(state, store_path)\n",
    "\n",
    "    def restore(self, file_path, rank=0) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Restore the training state saved by save_tmp.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return: the restored epoch number and the scheduler_step in scheduler.\n",
    "        \"\"\"\n",
    "        cpt_path = os.path.join(file_path, \"tmp\", \"model.%d\" % rank)\n",
    "        if not os.path.exists(cpt_path):\n",
    "            print(\"No checkpoint found %d\")\n",
    "            return 0, 0\n",
    "        cpt = torch.load(cpt_path)\n",
    "        self.model.load_state_dict(cpt[\"state_dict\"])\n",
    "        self.optimizer.load_state_dict(cpt[\"optimizer\"])\n",
    "        epoch = cpt[\"epoch\"]\n",
    "        scheduler_step = cpt[\"scheduler_step\"]\n",
    "        self.scheduler.current_step = scheduler_step\n",
    "        print(\"Restore checkpoint, current epoch: %d\" % (epoch))\n",
    "        return epoch, scheduler_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ef3e9",
   "metadata": {},
   "source": [
    "### 2-1-5. run_training 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d2278c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(args, logger):\n",
    "    \"\"\"\n",
    "    Run the pretrain task.\n",
    "    :param args:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # initalize the logger.\n",
    "    if logger is not None:\n",
    "        debug, _ = logger.debug, logger.info\n",
    "    else:\n",
    "        debug = print\n",
    "\n",
    "    # initialize the horovod library\n",
    "    if args.enable_multi_gpu:\n",
    "        mgw.init()\n",
    "\n",
    "    # binding training to GPUs.\n",
    "    master_worker = (mgw.rank() == 0) if args.enable_multi_gpu else True\n",
    "    # pin GPU to local rank. By default, we use gpu:0 for training.\n",
    "    local_gpu_idx = mgw.local_rank() if args.enable_multi_gpu else 0\n",
    "    with_cuda = args.cuda\n",
    "    if with_cuda:\n",
    "        torch.cuda.set_device(local_gpu_idx)\n",
    "\n",
    "    # get rank an  number of workers\n",
    "    rank = mgw.rank() if args.enable_multi_gpu else 0\n",
    "    num_replicas = mgw.size() if args.enable_multi_gpu else 1\n",
    "    # print(\"Rank: %d Rep: %d\" % (rank, num_replicas))\n",
    "\n",
    "    # load file paths of the data.\n",
    "    if master_worker:\n",
    "        print(args)\n",
    "        if args.enable_multi_gpu:\n",
    "            debug(\"Total workers: %d\" % (mgw.size()))\n",
    "        debug('Loading data')\n",
    "    data, sample_per_file = get_data(data_path=args.data_path)\n",
    "\n",
    "    # data splitting\n",
    "    if master_worker:\n",
    "        debug(f'Splitting data with seed 0.')\n",
    "    train_data, test_data, _ = split_data(data=data, sizes=(0.9, 0.1, 0.0), seed=0, logger=logger)\n",
    "\n",
    "    # Here the true train data size is the train_data divided by #GPUs\n",
    "    if args.enable_multi_gpu:\n",
    "        args.train_data_size = len(train_data) // mgw.size()\n",
    "    else:\n",
    "        args.train_data_size = len(train_data)\n",
    "    if master_worker:\n",
    "        debug(f'Total size = {len(data):,} | '\n",
    "              f'train size = {len(train_data):,} | val size = {len(test_data):,}')\n",
    "\n",
    "    # load atom and bond vocabulary and the semantic motif labels.\n",
    "    atom_vocab = MolVocab.load_vocab(args.atom_vocab_path)\n",
    "    bond_vocab = MolVocab.load_vocab(args.bond_vocab_path)\n",
    "    atom_vocab_size, bond_vocab_size = len(atom_vocab), len(bond_vocab)\n",
    "\n",
    "    # Load motif vocabulary for pretrain\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if args.parser_name == 'pretrain':\n",
    "        motif_vocab = [x.strip(\"\\r\\n \") for x in open(args.motif_vocab_path)]\n",
    "        motif_vocab = Motif_Vocab(motif_vocab)\n",
    "        #see below motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order).to(device)\n",
    "        \n",
    "        \n",
    "    # Hard coding here, since we haven't load any data yet!\n",
    "    fg_size = 85\n",
    "    shared_dict = {}\n",
    "    mol_collator = GroverCollator(shared_dict=shared_dict, atom_vocab=atom_vocab, bond_vocab=bond_vocab, args=args)\n",
    "    if master_worker:\n",
    "        debug(\"atom vocab size: %d, bond vocab size: %d, Number of FG tasks: %d\" % (atom_vocab_size,\n",
    "                                                                                    bond_vocab_size, fg_size))\n",
    "\n",
    "    # Define the distributed sampler. If using the single card, the sampler will be None.\n",
    "    train_sampler = None\n",
    "    test_sampler = None\n",
    "    shuffle = True\n",
    "    if args.enable_multi_gpu:\n",
    "        # If not shuffle, the performance may decayed.\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=True, sample_per_file=sample_per_file)\n",
    "        # Here sample_per_file in test_sampler is None, indicating the test sampler would not divide the test samples by\n",
    "        # rank. (TODO: bad design here.)\n",
    "        test_sampler = DistributedSampler(\n",
    "            test_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=False)\n",
    "        train_sampler.set_epoch(args.epochs)\n",
    "        test_sampler.set_epoch(1)\n",
    "        # if we enables multi_gpu training. shuffle should be disabled.\n",
    "        shuffle = False\n",
    "\n",
    "    # Pre load data. (Maybe unnecessary. )\n",
    "    pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
    "    pre_load_data(test_data, rank, num_replicas)\n",
    "    if master_worker:\n",
    "        # print(\"Pre-loaded training data: %d\" % train_data.count_loaded_datapoints())\n",
    "        print(\"Pre-loaded test data: %d\" % test_data.count_loaded_datapoints())\n",
    "\n",
    "    # Build dataloader\n",
    "    train_data_dl = DataLoader(train_data,\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=shuffle,\n",
    "                               num_workers=12,\n",
    "                               sampler=train_sampler,\n",
    "                               collate_fn=mol_collator)\n",
    "    test_data_dl = DataLoader(test_data,\n",
    "                              batch_size=args.batch_size,\n",
    "                              shuffle=shuffle,\n",
    "                              num_workers=10,\n",
    "                              sampler=test_sampler,\n",
    "                              collate_fn=mol_collator)\n",
    "\n",
    "    # Build the embedding model.\n",
    "    grover_model = GROVEREmbedding(args)\n",
    "\n",
    "    #  Build the trainer.\n",
    "    trainer = GROVERTrainer(args=args,\n",
    "                            embedding_model=grover_model,\n",
    "                            atom_vocab_size=atom_vocab_size,\n",
    "                            bond_vocab_size=bond_vocab_size,\n",
    "                            fg_szie=fg_size,\n",
    "                            train_dataloader=train_data_dl,\n",
    "                            test_dataloader=test_data_dl,\n",
    "                            optimizer_builder=build_optimizer,\n",
    "                            scheduler_builder=build_lr_scheduler,\n",
    "                            logger=logger,\n",
    "                            with_cuda=with_cuda,\n",
    "                            enable_multi_gpu=args.enable_multi_gpu)\n",
    "\n",
    "    # Restore the interrupted training.\n",
    "    model_dir = os.path.join(args.save_dir, \"model\")\n",
    "    resume_from_epoch = 0\n",
    "    resume_scheduler_step = 0\n",
    "    if master_worker:\n",
    "        resume_from_epoch, resume_scheduler_step = trainer.restore(model_dir)\n",
    "    if args.enable_multi_gpu:\n",
    "        resume_from_epoch = mgw.broadcast(torch.tensor(resume_from_epoch), root_rank=0, name=\"resume_from_epoch\").item()\n",
    "        resume_scheduler_step = mgw.broadcast(torch.tensor(resume_scheduler_step),\n",
    "                                              root_rank=0, name=\"resume_scheduler_step\").item()\n",
    "        trainer.scheduler.current_step = resume_scheduler_step\n",
    "        print(\"Restored epoch: %d Restored scheduler step: %d\" % (resume_from_epoch, trainer.scheduler.current_step))\n",
    "    trainer.broadcast_parameters()\n",
    "\n",
    "    # Print model details.\n",
    "    if master_worker:\n",
    "        # Change order here.\n",
    "        print(grover_model)\n",
    "        print(\"Total parameters: %d\" % param_count(trainer.grover))\n",
    "\n",
    "    # Perform training.\n",
    "    for epoch in range(resume_from_epoch + 1, args.epochs):\n",
    "        s_time = time.time()\n",
    "\n",
    "        # Data pre-loading.\n",
    "        if args.enable_multi_gpu:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "            train_data.clean_cache()\n",
    "            idxs = train_sampler.get_indices()\n",
    "            for local_gpu_idx in idxs:\n",
    "                train_data.load_data(local_gpu_idx)\n",
    "        d_time = time.time() - s_time\n",
    "\n",
    "        # perform training and validation.\n",
    "        s_time = time.time()\n",
    "        _, train_loss, _ = trainer.train(epoch)\n",
    "        t_time = time.time() - s_time\n",
    "        s_time = time.time()\n",
    "        _, val_loss, detailed_loss_val = trainer.test(epoch)\n",
    "        val_av_loss, val_bv_loss, val_fg_loss, _, _, _ = detailed_loss_val\n",
    "        v_time = time.time() - s_time\n",
    "\n",
    "        # print information.\n",
    "        if master_worker:\n",
    "            print('Epoch: {:04d}'.format(epoch),\n",
    "                  'loss_train: {:.6f}'.format(train_loss),\n",
    "                  'loss_val: {:.6f}'.format(val_loss),\n",
    "                  'loss_val_av: {:.6f}'.format(val_av_loss),\n",
    "                  'loss_val_bv: {:.6f}'.format(val_bv_loss),\n",
    "                  'loss_val_fg: {:.6f}'.format(val_fg_loss),\n",
    "                  'cur_lr: {:.5f}'.format(trainer.scheduler.get_lr()[0]),\n",
    "                  't_time: {:.4f}s'.format(t_time),\n",
    "                  'v_time: {:.4f}s'.format(v_time),\n",
    "                  'd_time: {:.4f}s'.format(d_time), flush=True)\n",
    "\n",
    "            if epoch % args.save_interval == 0:\n",
    "                trainer.save(epoch, model_dir)\n",
    "\n",
    "\n",
    "            trainer.save_tmp(epoch, model_dir, rank)\n",
    "\n",
    "    # Only save final version.\n",
    "    if master_worker:\n",
    "        trainer.save(args.epochs, model_dir, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d24bb-01f0-4392-89b8-1a38f25a8a5a",
   "metadata": {},
   "source": [
    "## 3. save_moltrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7e93876-29ff-4cf4-aa86-bf3960dc81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes and saves molecular features for a dataset.\n",
    "\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from multiprocessing import Pool\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n",
    "\n",
    "from grover.util.utils import get_data, makedirs, load_features, save_features\n",
    "from grover.data.molfeaturegenerator import get_available_features_generators, \\\n",
    "    get_features_generator\n",
    "from grover.data.task_labels import rdkit_functional_group_label_features_generator\n",
    "\n",
    "import pickle\n",
    "from grover.topology.mol_tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66dd976e-25a5-47b4-a99a-3e9a2e810f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temp(temp_dir: str) -> Tuple[List[List[float]], int]:\n",
    "    \"\"\"\n",
    "    Loads all features saved as .npz files in load_dir.\n",
    "\n",
    "    Assumes temporary files are named in order 0.npz, 1.npz, ...\n",
    "\n",
    "    :param temp_dir: Directory in which temporary .npz files containing features are stored.\n",
    "    :return: A tuple with a list of molecule features, where each molecule's features is a list of floats,\n",
    "    and the number of temporary files.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    temp_num = 0\n",
    "    temp_path = os.path.join(temp_dir, f'{temp_num}.npz')\n",
    "\n",
    "    while os.path.exists(temp_path):\n",
    "        features.extend(load_features(temp_path))\n",
    "        temp_num += 1\n",
    "        temp_path = os.path.join(temp_dir, f'{temp_num}.npz')\n",
    "\n",
    "    return features, temp_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c622e2f1-b738-46ff-801c-0e5147ffbed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_moltree(smiles):\n",
    "    mol_tree = MolTree(smiles)\n",
    "    mol_tree.recover()\n",
    "    mol_tree.assemble()\n",
    "    return mol_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d90115bf-d5f1-48bb-8a96-6b2e85382a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_moltrees(args: Namespace):\n",
    "    \"\"\"\n",
    "    Computes and saves features for a dataset of molecules as a 2D array in a .npz file.\n",
    "\n",
    "    :param args: Arguments.\n",
    "    \"\"\"\n",
    "    # Create directory for save_path\n",
    "    makedirs(args.save_path, isfile=True)\n",
    "\n",
    "    # Get data and features function\n",
    "    data = get_data(path=args.data_path, max_data_size=None)\n",
    "    temp_save_dir = args.save_path + '_temp'\n",
    "\n",
    "    # Load partially complete data\n",
    "    if args.restart:\n",
    "        if os.path.exists(args.save_path):\n",
    "            os.remove(args.save_path)\n",
    "        if os.path.exists(temp_save_dir):\n",
    "            shutil.rmtree(temp_save_dir)\n",
    "    else:\n",
    "        if os.path.exists(args.save_path):\n",
    "            raise ValueError(f'\"{args.save_path}\" already exists and args.restart is False.')\n",
    "\n",
    "        if os.path.exists(temp_save_dir):\n",
    "            moltrees, temp_num = load_temp(temp_save_dir)\n",
    "\n",
    "    if not os.path.exists(temp_save_dir):\n",
    "        makedirs(temp_save_dir)\n",
    "        moltrees, temp_num = [], 0\n",
    "\n",
    "    # Build features map function\n",
    "    data = data[len(moltrees):]  # restrict to data for which features have not been computed yet\n",
    "    mols = (d.smiles for d in data)\n",
    "    \n",
    "    if args.sequential:\n",
    "        moltrees_map = map(make_moltree, mols)\n",
    "    else:\n",
    "        moltrees_map = Pool(30).imap(make_moltree, mols)\n",
    "        \n",
    "    # Get features\n",
    "    temp_moltrees = []\n",
    "    for i, moltree in tqdm(enumerate(moltrees_map), total=len(data)):\n",
    "        temp_moltrees.append(moltree)\n",
    "\n",
    "        # Save temporary features every save_frequency\n",
    "        if (i > 0 and (i + 1) % args.save_frequency == 0) or i == len(data) - 1:\n",
    "            #save_features(os.path.join(temp_save_dir, f'{temp_num}.npz'), temp_moltrees)\n",
    "            moltrees.extend(temp_moltrees)\n",
    "            temp_moltrees = []\n",
    "            temp_num += 1\n",
    "\n",
    "    try:\n",
    "        # Save all features\n",
    "        with open('mgssl_moltree.p', 'wb') as file: \n",
    "            pickle.dump(moltrees, file)\n",
    "\n",
    "        # Remove temporary features\n",
    "        shutil.rmtree(temp_save_dir)\n",
    "    except OverflowError:\n",
    "        print('moltree object is too large to save as a single file. Instead keeping features as a directory of files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4514e4b-eb77-454c-9c05-37e450c55eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--data_path', type=str, required=True,\n",
    "                    help='Path to data CSV')\n",
    "parser.add_argument('--features_generator', type=str, required=True,\n",
    "                    choices=get_available_features_generators(),\n",
    "                    help='Type of features to generate')\n",
    "parser.add_argument('--save_path', type=str, default=None,\n",
    "                    help='Path to .npz file where features will be saved as a compressed numpy archive')\n",
    "parser.add_argument('--save_frequency', type=int, default=10000,\n",
    "                    help='Frequency with which to save the features')\n",
    "parser.add_argument('--restart', action='store_true', default=False,\n",
    "                    help='Whether to not load partially complete featurization and instead start from scratch')\n",
    "parser.add_argument('--max_data_size', type=int,\n",
    "                    help='Maximum number of data points to load')\n",
    "parser.add_argument('--sequential', action='store_true', default=False,\n",
    "                    help='Whether to task sequentially rather than in parallel')\n",
    "args = parser.parse_args(['--data_path', 'data/mgssl.csv', '--features_generator','fgtasklabel', '--save_path', 'data/test/mgssl2'])\n",
    "if args.save_path is None:\n",
    "    args.save_path = args.data_path.split('csv')[0] + 'npz'\n",
    "#generate_and_save_features(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76adebc2-ca07-45da-9f97-b45913ae23df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='data/mgssl.csv', features_generator='fgtasklabel', max_data_size=None, restart=False, save_frequency=10000, save_path='data/test/mgssl2', sequential=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de398dd4-7f02-4657-aada-c33addda8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(args.save_path, isfile=True)\n",
    "\n",
    "# Get data and features function\n",
    "data = get_data(path=args.data_path, max_data_size=None)\n",
    "temp_save_dir = args.save_path + '_temp'\n",
    "\n",
    "# Load partially complete data\n",
    "if args.restart:\n",
    "    if os.path.exists(args.save_path):\n",
    "        os.remove(args.save_path)\n",
    "    if os.path.exists(temp_save_dir):\n",
    "        shutil.rmtree(temp_save_dir)\n",
    "else:\n",
    "    if os.path.exists(args.save_path):\n",
    "        raise ValueError(f'\"{args.save_path}\" already exists and args.restart is False.')\n",
    "\n",
    "    if os.path.exists(temp_save_dir):\n",
    "        moltrees, temp_num = load_temp(temp_save_dir)\n",
    "\n",
    "if not os.path.exists(temp_save_dir):\n",
    "    makedirs(temp_save_dir)\n",
    "    moltrees, temp_num = [], 0\n",
    "\n",
    "# Build features map function\n",
    "data = data[len(moltrees):]  # restrict to data for which features have not been computed yet\n",
    "mols = (d.smiles for d in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df41c19e-29a5-4567-a6d8-b70f60606b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.sequential:\n",
    "    moltrees_map = map(make_moltree, mols)\n",
    "else:\n",
    "    moltrees_map = Pool(30).imap(make_moltree, mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cfb6339-2efc-4bef-b503-39c88b9d669a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 293/293 [00:07<00:00, 40.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get features\n",
    "temp_moltrees = []\n",
    "for i, moltree in tqdm(enumerate(moltrees_map), total=len(data)):\n",
    "    temp_moltrees.append(moltree)\n",
    "\n",
    "    # Save temporary features every save_frequency\n",
    "    if (i > 0 and (i + 1) % args.save_frequency == 0) or i == len(data) - 1:\n",
    "        #save_features(os.path.join(temp_save_dir, f'{temp_num}.npz'), temp_moltrees)\n",
    "        moltrees.extend(temp_moltrees)\n",
    "        temp_moltrees = []\n",
    "        temp_num += 1\n",
    "\n",
    "try:\n",
    "    # Save all features\n",
    "    #save_features(args.save_path, moltrees)\n",
    "\n",
    "    # Remove temporary features\n",
    "    shutil.rmtree(temp_save_dir)\n",
    "except OverflowError:\n",
    "    print('moltree object is too large to save as a single file. Instead keeping features as a directory of files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4da30bd-facb-48d5-97e6-1943efc123a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<grover.topology.mol_tree.MolTree at 0x7f70ee7f0550>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6b6ed0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70f02ea310>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee610dd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6a1650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ad091e50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee502f10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6e2450>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4f2510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee505b50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee68b690>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4fe850>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5eb490>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6d1f90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee67a450>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5eb410>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee874bd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee658510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee502e10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4edfd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb5cdd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4f67d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee77fe50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70f0310650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee898910>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee77f890>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee623fd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6b6190>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee8526d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70eeab6dd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6dcb90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee61ea50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5e3410>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee512d10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee820590>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee512c10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6911d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee68b6d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6b6110>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee746310>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee67ae10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6f6bd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee71cbd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70eea68750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee658090>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6a1590>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee534c50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70eea13510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee830050>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7c5490>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee9dbc10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6dcb50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4e6710>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee87d650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee8dbd10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6be8d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6d15d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6c5cd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecbd7c50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70eea36e10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee802290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6c8a50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6cb290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4f7790>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ad028810>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4e68d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecba1ad0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6c4950>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee512710>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7d0410>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7e8ad0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6cbfd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4f1b50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee704a50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6c4090>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5eedd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4f16d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7f9590>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4e6910>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee561990>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee807dd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6cb350>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee820990>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb6c350>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee68eb10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4fd650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7e8910>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4dfc90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7f9610>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5088d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee687050>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee8043d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7e8ed0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4e9690>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4e9650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4dfa90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee606210>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee784590>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6fdd90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70e6a99550>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6e2b50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee55ca90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee704510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7e8f50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4ec0d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6f3550>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7e8950>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7a6ad0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4ea590>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee797050>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee73cfd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4eac90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb57490>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc31290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6e2510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6ed0d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6f3310>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee684650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee66e750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee66e090>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee667850>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee702390>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee684cd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee68e690>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7349d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee797410>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee73cf90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6875d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee687210>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee674710>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee736f50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee802050>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee796a10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7e87d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee704f50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7253d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7bcd90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee796e10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee63bed0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee610110>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee73c850>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee73bfd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee796310>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5d90d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee606fd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5d9090>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6477d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5b8290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee746f10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7368d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee745210>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5dfb10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee745050>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5e7790>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5fe750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee737050>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5e7850>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5fe110>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5e7890>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee78df90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6239d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5d5150>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7939d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee780490>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee793a90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee736ad0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4d2a90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4e6d10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee60f710>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee793810>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee55ca50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5a3410>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee61ec50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecbff150>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee765ad0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee7751d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee577090>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee784fd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee64b510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee780350>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee620c50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee78d350>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb4f750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee75d8d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee64b4d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4ab450>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee653b10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5d05d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee641390>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee62c150>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee62cfd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee636550>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee6198d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee58f450>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee62c650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee629d50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee63bd10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4edf50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee55f290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee63bc90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee64b0d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee784550>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5c7cd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee63bc50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee551c10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5a76d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee56b390>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5aee50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee56b3d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee55fd50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee55fc90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5a7790>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee54eb90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5770d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee58dc50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee56b110>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee54f710>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb7ab90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee551c50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4b8b90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4c8f50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc468d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee5423d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee55cb90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee874390>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee551090>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc85d90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee542510>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4ce6d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee54f6d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4cb3d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee51fe50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee54eb50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc65690>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee54eb10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee522250>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee551bd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee577050>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4b8c50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee524310>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee497210>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4c8550>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee53a690>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee629e10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc55450>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc55490>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4d2ad0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4b8610>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4cb9d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb16b10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb573d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4c8f10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70e6aaf710>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecbd7e90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc5bed0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc1efd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb94590>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc5bf50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecbb1f10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc85a50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc76f10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc7ab10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee64b650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee64b610>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc74150>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc85b10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecbf6cd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc81dd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4c8650>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70e6b04bd0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee4aba10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc15550>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb57410>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecba0e50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc1f1d0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc1f750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc1ef50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc36290>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc2fe50>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc36ed0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc46d90>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc4a450>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ee784690>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc7aad0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70e6aaf750>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc46810>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc2f610>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb6c490>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc01ed0>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecb96610>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc46950>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecc01f10>,\n",
       " <grover.topology.mol_tree.MolTree at 0x7f70ecba7b10>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moltrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc5e0d01-1273-4ebd-93a6-45aac4ce3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#mol_tree = MolTree(smiles)\n",
    "#mol_tree.recover()\n",
    "#mol_tree.assemble()\n",
    "with open('mgssl_moltree.p', 'wb') as file: \n",
    "    pickle.dump(moltrees, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "193a35fa-9114-43f7-a7ab-55618cbb966e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"mgssl_moltree.p\", 'rb') as f:\n",
    "    moltreedata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b3fab",
   "metadata": {},
   "source": [
    "## 4. Grover_motiftrain코드 나눠서 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12aa99",
   "metadata": {},
   "source": [
    "### 3-0. parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52f0ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.parsing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "393414c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args() -> Namespace:\n",
    "    \"\"\"\n",
    "    Parses arguments for training and testing (includes modifying/validating arguments).\n",
    "\n",
    "    :return: A Namespace containing the parsed, modified, and validated args.\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "                                      dest=\"parser_name\",\n",
    "                                      help=\"Subcommands for fintune, prediction, and fingerprint.\")\n",
    "    parser_finetune = subparser.add_parser('finetune', help=\"Fine tune the pre-trained model.\")\n",
    "    add_finetune_args(parser_finetune)\n",
    "    parser_eval = subparser.add_parser('eval', help=\"Evaluate the results of the pre-trained model.\")\n",
    "    add_finetune_args(parser_eval)\n",
    "    parser_predict = subparser.add_parser('predict', help=\"Predict results from fine tuned model.\")\n",
    "    add_predict_args(parser_predict)\n",
    "    parser_fp = subparser.add_parser('fingerprint', help=\"Get the fingerprints of SMILES.\")\n",
    "    add_fingerprint_args(parser_fp)\n",
    "    parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "    add_pretrain_args(parser_pretrain)\n",
    "\n",
    "    args = parser.parse_args(['pretrain','--data_path','data/zinc10M','--save_dir','model/zinc10M','--atom_vocab_path','data/zinc10M/zinc10M_atom_vocab.pkl','--bond_vocab_path','data/zinc10M/zinc10M_bond_vocab.pkl',\n",
    "                              '--batch_size','100','--dropout','0.1','--depth','3','--num_attn_head','4','--hidden_size','1200','--epochs','20','--activation','PReLU','--backbone','gtrans','--embedding_output_type','both',\n",
    "                              '--save_interval','5','--init_lr', '0.0002', '--max_lr', '0.0004', '--final_lr', '0.0001', '--weight_decay', '0.0000001', \n",
    "                              '--topology','--motif_vocab_path','data/zinc10M/clique.txt','--motif_hidden_size','1200','--motif_latent_size','56','--motif_order','dfs'])\n",
    "    \n",
    "    if args.parser_name == 'finetune' or args.parser_name == 'eval':\n",
    "        modify_train_args(args)\n",
    "    elif args.parser_name == \"pretrain\":\n",
    "        modify_pretrain_args(args)\n",
    "    elif args.parser_name == 'predict':\n",
    "        modify_predict_args(args)\n",
    "    elif args.parser_name == 'fingerprint':\n",
    "        modify_fingerprint_args(args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd2aec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "args\n",
    "logger = create_logger(name='pretrain', save_dir=args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f1fd8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(activation='PReLU', atom_vocab_path='data/zinc10M/zinc10M_atom_vocab.pkl', backbone='gtrans', batch_size=100, bias=False, bond_drop_rate=0, bond_vocab_path='data/zinc10M/zinc10M_bond_vocab.pkl', cuda=True, data_path='data/zinc10M', dense=False, depth=3, dist_coff=0.1, dropout=0.1, each_epochs=5, embedding_output_type='both', enable_multi_gpu=False, epochs=20, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=1200, init_lr=0.0002, max_lr=0.0004, motif_hidden_size=1200, motif_latent_size=56, motif_order='dfs', motif_vocab_path='data/zinc10M/clique.txt', no_cache=True, num_attn_head=4, num_mt_block=1, parser_name='pretrain', save_dir='model/zinc10M', save_interval=5, subset_learning=False, topology=True, undirected=False, wandb=False, wandb_name='pretrain', warmup_epochs=2.0, weight_decay=1e-07)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b85a46c",
   "metadata": {},
   "source": [
    "python main.py pretrain \\\n",
    "               --data_path exampledata/pretrain/tryout \\\n",
    "               --save_dir model/tryout \\\n",
    "               --atom_vocab_path exampledata/pretrain/tryout_atom_vocab.pkl \\\n",
    "               --bond_vocab_path exampledata/pretrain/tryout_bond_vocab.pkl \\\n",
    "               --batch_size 32 \\\n",
    "               --dropout 0.1 \\\n",
    "               --depth 5 \\\n",
    "               --num_attn_head 1 \\\n",
    "               --hidden_size 100 \\\n",
    "               --epochs 3 \\\n",
    "               --init_lr 0.0002 \\\n",
    "               --max_lr 0.0004 \\\n",
    "               --final_lr 0.0001 \\\n",
    "               --weight_decay 0.0000001 \\\n",
    "               --activation PReLU \\\n",
    "               --backbone gtrans \\\n",
    "               --embedding_output_type both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68ba76c-eea0-467b-b797-94a0e51afe4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-1. MolTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5210b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.topology.chemutils import *\n",
    "from grover.topology.mol_tree import *\n",
    "from grover.topology.motif_generation import *\n",
    "from grover.topology.dfs import *\n",
    "from grover.topology.bfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b0d40b0-0729-440c-8e9f-0ebab490be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "import rdkit.Chem as Chem\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from grover.topology.chemutils import get_clique_mol, tree_decomp, brics_decomp, get_mol, get_smiles, set_atommap, enum_assemble, decode_stereo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfba68f2-4770-4db2-8717-de5994a51589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slots(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return [(atom.GetSymbol(), atom.GetFormalCharge(), atom.GetTotalNumHs()) for atom in mol.GetAtoms()]\n",
    "\n",
    "class Motif_Vocab(object):\n",
    "\n",
    "    def __init__(self, smiles_list):\n",
    "        self.vocab = smiles_list\n",
    "        self.vmap = {x:i for i,x in enumerate(self.vocab)}\n",
    "        self.slots = [get_slots(smiles) for smiles in self.vocab]\n",
    "        \n",
    "    def get_index(self, smiles):\n",
    "        return self.vmap[smiles]\n",
    "    \n",
    "\n",
    "    def get_smiles(self, idx):\n",
    "        return self.vocab[idx]\n",
    "\n",
    "    def get_slots(self, idx):\n",
    "        return copy.deepcopy(self.slots[idx])\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def add_motif(self, smiles):\n",
    "        self.vocab.append(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50f331ff-26e5-4727-8565-a4a6e237c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolTreeNode(object):\n",
    "\n",
    "    def __init__(self, smiles, clique=[]):\n",
    "        self.smiles = smiles\n",
    "        self.mol = get_mol(self.smiles)\n",
    "        #self.mol = cmol\n",
    "\n",
    "        self.clique = [x for x in clique] #copy\n",
    "        self.neighbors = []\n",
    "        \n",
    "    def add_neighbor(self, nei_node):\n",
    "        self.neighbors.append(nei_node)\n",
    "\n",
    "    def recover(self, original_mol):\n",
    "        clique = []\n",
    "        clique.extend(self.clique)\n",
    "        if not self.is_leaf:\n",
    "            for cidx in self.clique:\n",
    "                original_mol.GetAtomWithIdx(cidx).SetAtomMapNum(self.nid)\n",
    "\n",
    "        for nei_node in self.neighbors:\n",
    "            clique.extend(nei_node.clique)\n",
    "            if nei_node.is_leaf: #Leaf node, no need to mark \n",
    "                continue\n",
    "            for cidx in nei_node.clique:\n",
    "                #allow singleton node override the atom mapping\n",
    "                if cidx not in self.clique or len(nei_node.clique) == 1:\n",
    "                    atom = original_mol.GetAtomWithIdx(cidx)\n",
    "                    atom.SetAtomMapNum(nei_node.nid)\n",
    "\n",
    "        clique = list(set(clique))\n",
    "        label_mol = get_clique_mol(original_mol, clique)\n",
    "        self.label = Chem.MolToSmiles(Chem.MolFromSmiles(get_smiles(label_mol)))\n",
    "        self.label_mol = get_mol(self.label)\n",
    "\n",
    "        for cidx in clique:\n",
    "            original_mol.GetAtomWithIdx(cidx).SetAtomMapNum(0)\n",
    "\n",
    "        return self.label\n",
    "    \n",
    "    def assemble(self):\n",
    "        neighbors = [nei for nei in self.neighbors if nei.mol.GetNumAtoms() > 1]\n",
    "        neighbors = sorted(neighbors, key=lambda x:x.mol.GetNumAtoms(), reverse=True)\n",
    "        singletons = [nei for nei in self.neighbors if nei.mol.GetNumAtoms() == 1]\n",
    "        neighbors = singletons + neighbors\n",
    "        \n",
    "        cands = enum_assemble(self, neighbors)\n",
    "        \n",
    "        if len(cands) > 0:\n",
    "            self.cands, self.cand_mols, _ = zip(*cands)\n",
    "            self.cands = list(self.cands)\n",
    "            self.cand_mols = list(self.cand_mols)\n",
    "        else:\n",
    "            self.cands = []\n",
    "            self.cand_mols = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5d8e254-45c8-42dc-abb3-245047777bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolTree(object):\n",
    "\n",
    "    def __init__(self, smiles):\n",
    "        self.smiles = smiles\n",
    "        self.mol = get_mol(smiles)\n",
    "\n",
    "        '''\n",
    "        #Stereo Generation\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        self.smiles3D = Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "        self.smiles2D = Chem.MolToSmiles(mol)\n",
    "        self.stereo_cands = decode_stereo(self.smiles2D)\n",
    "        '''\n",
    "\n",
    "        cliques, edges = brics_decomp(self.mol)\n",
    "        if len(edges) <= 1:\n",
    "            cliques, edges = tree_decomp(self.mol)\n",
    "        self.nodes = []\n",
    "        root = 0\n",
    "        for i,c in enumerate(cliques):\n",
    "            cmol = get_clique_mol(self.mol, c)\n",
    "            node = MolTreeNode(get_smiles(cmol), c)\n",
    "            self.nodes.append(node)\n",
    "            if min(c) == 0:\n",
    "                root = i\n",
    "\n",
    "        for x,y in edges:\n",
    "            self.nodes[x].add_neighbor(self.nodes[y])\n",
    "            self.nodes[y].add_neighbor(self.nodes[x])\n",
    "        \n",
    "        if root > 0:\n",
    "            self.nodes[0],self.nodes[root] = self.nodes[root],self.nodes[0]\n",
    "\n",
    "        for i,node in enumerate(self.nodes):\n",
    "            node.nid = i + 1\n",
    "            if len(node.neighbors) > 1: #Leaf node mol is not marked\n",
    "                set_atommap(node.mol, node.nid)\n",
    "            node.is_leaf = (len(node.neighbors) == 1)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.nodes)\n",
    "\n",
    "    def recover(self):\n",
    "        for node in self.nodes:\n",
    "            node.recover(self.mol)\n",
    "\n",
    "    def assemble(self):\n",
    "        for node in self.nodes:\n",
    "            node.assemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a1cae-6ab3-41b0-be7a-1359b306f2fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-2. get_motif_data():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3365b4d8-4ed8-4ca3-85c4-5c1a7ce3dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "342a2f03-6a12-4367-9037-33e50e078687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_moltrees(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loads features saved in a variety of formats.\n",
    "\n",
    "    Supported formats:\n",
    "    - .npz compressed (assumes features are saved with name \"features\")\n",
    "\n",
    "    All formats assume that the SMILES strings loaded elsewhere in the code are in the same\n",
    "    order as the features loaded here.\n",
    "\n",
    "    :param path: Path to a file containing features.\n",
    "    :return: A 2D numpy array of size (num_molecules, features_size) containing the features.\n",
    "    \"\"\"\n",
    "    extension = os.path.splitext(path)[1]\n",
    "\n",
    "    if extension == '.p':\n",
    "        with open(path, 'rb') as f:\n",
    "            moltrees = pickle.load(f)\n",
    "    else:\n",
    "        raise ValueError(f'Features path extension {extension} not supported.')\n",
    "\n",
    "    return moltrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3f663-9343-4e01-a608-716d55154200",
   "metadata": {},
   "source": [
    "#### 3-2-1 MoleculeDataPoint_motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "456f4f60-eaa9-416a-be8c-cfc26cad6bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDatapoint_motif:\n",
    "    \"\"\"A MoleculeDatapoint contains a single molecule and its associated features and targets.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 line: List[str],\n",
    "                 args: Namespace = None,\n",
    "                 features: np.ndarray = None,\n",
    "                 moltrees: object = None,\n",
    "                 use_compound_names: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes a MoleculeDatapoint, which contains a single molecule.\n",
    "\n",
    "        :param line: A list of strings generated by separating a line in a data CSV file by comma.\n",
    "        :param args: Arguments.\n",
    "        :param features: A numpy array containing additional features (ex. Morgan fingerprint).\n",
    "        :param use_compound_names: Whether the data CSV includes the compound name on each line.\n",
    "        \"\"\"\n",
    "        self.features_generator = None\n",
    "        self.args = None\n",
    "        if args is not None:\n",
    "            if hasattr(args, \"features_generator\"):\n",
    "                self.features_generator = args.features_generator\n",
    "            self.args = args\n",
    "\n",
    "        if features is not None and self.features_generator is not None:\n",
    "            raise ValueError('Currently cannot provide both loaded features and a features generator.')\n",
    "\n",
    "        self.features = features\n",
    "        self.moltrees = moltrees\n",
    "\n",
    "        if use_compound_names:\n",
    "            self.compound_name = line[0]  # str\n",
    "            line = line[1:]\n",
    "        else:\n",
    "            self.compound_name = None\n",
    "\n",
    "        self.smiles = line[0]  # str\n",
    "\n",
    "\n",
    "        # Generate additional features if given a generator\n",
    "        if self.features_generator is not None:\n",
    "            self.features = []\n",
    "            mol = Chem.MolFromSmiles(self.smiles)\n",
    "            for fg in self.features_generator:\n",
    "                features_generator = get_features_generator(fg)\n",
    "                if mol is not None and mol.GetNumHeavyAtoms() > 0:\n",
    "                    if fg in ['morgan', 'morgan_count']:\n",
    "                        self.features.extend(features_generator(mol, num_bits=args.num_bits))\n",
    "                    else:\n",
    "                        self.features.extend(features_generator(mol))\n",
    "\n",
    "            self.features = np.array(self.features)\n",
    "\n",
    "        # Fix nans in features\n",
    "        if self.features is not None:\n",
    "            replace_token = 0\n",
    "            self.features = np.where(np.isnan(self.features), replace_token, self.features)\n",
    "\n",
    "        # Create targets\n",
    "        self.targets = [float(x) if x != '' else None for x in line[1:]]\n",
    "\n",
    "    def set_features(self, features: np.ndarray):\n",
    "        \"\"\"\n",
    "        Sets the features of the molecule.\n",
    "\n",
    "        :param features: A 1-D numpy array of features for the molecule.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        \n",
    "    def set_moltrees(self, moltrees: list):\n",
    "        \"\"\"\n",
    "        Sets the features of the molecule.\n",
    "\n",
    "        :param features: A 1-D numpy array of features for the molecule.\n",
    "        \"\"\"\n",
    "        self.moltrees = moltrees\n",
    "\n",
    "    def num_tasks(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of prediction tasks.\n",
    "\n",
    "        :return: The number of tasks.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def set_targets(self, targets: List[float]):\n",
    "        \"\"\"\n",
    "        Sets the targets of a molecule.\n",
    "\n",
    "        :param targets: A list of floats containing the targets.\n",
    "        \"\"\"\n",
    "        self.targets = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5bd01e3-41a3-4979-9c28-6100580ec107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDatapoint_motif:\n",
    "    def __init__(self,\n",
    "                 smiles_file,\n",
    "                 feature_file,\n",
    "                 moltree_file,\n",
    "                 n_samples,\n",
    "                 ):\n",
    "        self.smiles_file = smiles_file\n",
    "        self.feature_file = feature_file\n",
    "        self.moltree_file = moltree_file\n",
    "        # deal with the last batch graph numbers.\n",
    "        self.n_samples = n_samples\n",
    "        self.datapoints = None\n",
    "\n",
    "    def load_datapoints(self):\n",
    "        features = self.load_feature()\n",
    "        moltrees = self.load_moltree()\n",
    "        self.datapoints = []\n",
    "\n",
    "        with open(self.smiles_file) as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for i, line in enumerate(reader):\n",
    "                # line = line[0]\n",
    "                d = MoleculeDatapoint_motif(line=line,\n",
    "                                      features=features[i],\n",
    "                                      moltrees=moltrees[i])\n",
    "                self.datapoints.append(d)\n",
    "\n",
    "        assert len(self.datapoints) == self.n_samples\n",
    "\n",
    "    def load_feature(self):\n",
    "        return feautils.load_features(self.feature_file)\n",
    "    \n",
    "    def load_moltree(self):\n",
    "        return feautils.load_moltrees(self.moltree_file)\n",
    "\n",
    "    def shuffle(self):\n",
    "        pass\n",
    "\n",
    "    def clean_cache(self):\n",
    "        del self.datapoints\n",
    "        self.datapoints = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert self.datapoints is not None\n",
    "        return self.datapoints[idx]\n",
    "\n",
    "    def is_loaded(self):\n",
    "        return self.datapoints is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c87cd8a-aed7-442d-b21d-3320509ac7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMolDataset_motif(Dataset):\n",
    "    def __init__(self, data: List[BatchDatapoint_motif],\n",
    "                 graph_per_file=None):\n",
    "        self.data = data\n",
    "\n",
    "        self.len = 0\n",
    "        for d in self.data:\n",
    "            self.len += len(d)\n",
    "        if graph_per_file is not None:\n",
    "            self.sample_per_file = graph_per_file\n",
    "        else:\n",
    "            self.sample_per_file = len(self.data[0]) if len(self.data) != 0 else None\n",
    "\n",
    "    def shuffle(self, seed: int = None):\n",
    "        pass\n",
    "\n",
    "    def clean_cache(self):\n",
    "        for d in self.data:\n",
    "            d.clean_cache()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx) -> Union[MoleculeDatapoint_motif, List[MoleculeDatapoint_motif]]:\n",
    "        # print(idx)\n",
    "        dp_idx = int(idx / self.sample_per_file)\n",
    "        real_idx = idx % self.sample_per_file\n",
    "        return self.data[dp_idx][real_idx]\n",
    "\n",
    "    def load_data(self, idx):\n",
    "        dp_idx = int(idx / self.sample_per_file)\n",
    "        if not self.data[dp_idx].is_loaded():\n",
    "            self.data[dp_idx].load_datapoints()\n",
    "\n",
    "    def count_loaded_datapoints(self):\n",
    "        res = 0\n",
    "        for d in self.data:\n",
    "            if d.is_loaded():\n",
    "                res += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e57e9421-5071-4b05-a008-785ccb72e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_data(data_path, logger=None):\n",
    "    \"\"\"\n",
    "    Load data from the data_path.\n",
    "    :param data_path: the data_path.\n",
    "    :param logger: the logger.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    debug = logger.debug if logger is not None else print\n",
    "    summary_path = os.path.join(data_path, \"summary.txt\")\n",
    "    smiles_path = os.path.join(data_path, \"graph\")\n",
    "    feature_path = os.path.join(data_path, \"feature\")\n",
    "    moltree_path = os.path.join(data_path, \"moltrees\")\n",
    "\n",
    "    fin = open(summary_path)\n",
    "    n_files = int(fin.readline().strip().split(\":\")[-1])\n",
    "    n_samples = int(fin.readline().strip().split(\":\")[-1])\n",
    "    sample_per_file = int(fin.readline().strip().split(\":\")[-1])\n",
    "    debug(\"Loading data:\")\n",
    "    debug(\"Number of files: %d\" % n_files)\n",
    "    debug(\"Number of samples: %d\" % n_samples)\n",
    "    debug(\"Samples/file: %d\" % sample_per_file)\n",
    "\n",
    "    datapoints = []\n",
    "    for i in range(n_files):\n",
    "        smiles_path_i = os.path.join(smiles_path, str(i) + \".csv\")\n",
    "        feature_path_i = os.path.join(feature_path, str(i) + \".npz\")\n",
    "        moltree_path_i = os.path.join(moltree_path, str(i) + \".p\")\n",
    "        n_samples_i = sample_per_file if i != (n_files - 1) else n_samples % sample_per_file\n",
    "        datapoints.append(BatchDatapoint_motif(smiles_path_i, feature_path_i, moltree_path_i, n_samples_i))\n",
    "    return BatchMolDataset_motif(datapoints), sample_per_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d5689",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-2. Grover_MotifGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdea0c4-0e92-4b0c-b5e3-f82d0edd2a9f",
   "metadata": {},
   "source": [
    "#### 3-2-1. GroverMotifcollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6a9544e-5bd2-40e8-962e-7fe09c39563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroverMotifCollator(object):\n",
    "    def __init__(self, shared_dict, atom_vocab, bond_vocab, args):\n",
    "        self.args = args\n",
    "        self.shared_dict = shared_dict\n",
    "        self.atom_vocab = atom_vocab\n",
    "        self.bond_vocab = bond_vocab\n",
    "\n",
    "    def atom_random_mask(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Perform the random mask operation on atoms.\n",
    "        :param smiles_batch:\n",
    "        :return: The corresponding atom labels.\n",
    "        \"\"\"\n",
    "        # There is a zero padding.\n",
    "        vocab_label = [0]\n",
    "        percent = 0.15\n",
    "        for smi in smiles_batch:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            mlabel = [0] * mol.GetNumAtoms()\n",
    "            n_mask = math.ceil(mol.GetNumAtoms() * percent)\n",
    "            perm = np.random.permutation(mol.GetNumAtoms())[:n_mask]\n",
    "            for p in perm:\n",
    "                atom = mol.GetAtomWithIdx(int(p))\n",
    "                mlabel[p] = self.atom_vocab.stoi.get(atom_to_vocab(mol, atom), self.atom_vocab.other_index)\n",
    "\n",
    "            vocab_label.extend(mlabel)\n",
    "        return vocab_label\n",
    "\n",
    "    def bond_random_mask(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Perform the random mask operaiion on bonds.\n",
    "        :param smiles_batch:\n",
    "        :return: The corresponding bond labels.\n",
    "        \"\"\"\n",
    "        # There is a zero padding.\n",
    "        vocab_label = [0]\n",
    "        percent = 0.15\n",
    "        for smi in smiles_batch:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            nm_atoms = mol.GetNumAtoms()\n",
    "            nm_bonds = mol.GetNumBonds()\n",
    "            mlabel = []\n",
    "            n_mask = math.ceil(nm_bonds * percent)\n",
    "            perm = np.random.permutation(nm_bonds)[:n_mask]\n",
    "            virtual_bond_id = 0\n",
    "            for a1 in range(nm_atoms):\n",
    "                for a2 in range(a1 + 1, nm_atoms):\n",
    "                    bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                    if bond is None:\n",
    "                        continue\n",
    "                    if virtual_bond_id in perm:\n",
    "                        label = self.bond_vocab.stoi.get(bond_to_vocab(mol, bond), self.bond_vocab.other_index)\n",
    "                        mlabel.extend([label])\n",
    "                    else:\n",
    "                        mlabel.extend([0])\n",
    "\n",
    "                    virtual_bond_id += 1\n",
    "            # todo: might need to consider bond_drop_rate\n",
    "            # todo: double check reverse bond\n",
    "            vocab_label.extend(mlabel)\n",
    "        return vocab_label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles_batch = [d.smiles for d in batch]\n",
    "        batchgraph = mol2graph(smiles_batch, self.shared_dict, self.args).get_components()\n",
    "\n",
    "        atom_vocab_label = torch.Tensor(self.atom_random_mask(smiles_batch)).long()\n",
    "        bond_vocab_label = torch.Tensor(self.bond_random_mask(smiles_batch)).long()\n",
    "        fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
    "        moltree_batch = [d.moltrees for d in batch]\n",
    "        \n",
    "        # may be some mask here\n",
    "        res = {\"graph_input\": batchgraph,\n",
    "               \"targets\": {\"av_task\": atom_vocab_label,\n",
    "                           \"bv_task\": bond_vocab_label,\n",
    "                           \"fg_task\": fgroup_label},\n",
    "               \"moltree\" : moltree_batch\n",
    "               }\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0a8f8",
   "metadata": {},
   "source": [
    "#### 3-2-2. dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc106b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from grover.topology.mol_tree import Motif_Vocab, MolTree, MolTreeNode\n",
    "from grover.topology.chemutils import enum_assemble\n",
    "# add this directly below (from nnutils import create_var, GRU)\n",
    "import copy\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "MAX_NB = 8\n",
    "MAX_DECODE_LEN = 100\n",
    "\n",
    "def create_var(tensor, requires_grad=None):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'   #\"cuda:\" + \"1\" 다중 처리 때문에 이렇게 했나봐 ㅡㅡ 원본 : torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if requires_grad is None:\n",
    "        return Variable(tensor).to(device)\n",
    "    else:\n",
    "        return Variable(tensor, requires_grad=requires_grad).to(device)\n",
    "\n",
    "def index_select_ND(source, dim, index):\n",
    "    index_size = index.size()\n",
    "    suffix_dim = source.size()[1:]\n",
    "    final_size = index_size + suffix_dim\n",
    "    target = source.index_select(dim, index.view(-1))\n",
    "    return target.view(final_size)\n",
    "\n",
    "def GRU(x, h_nei, W_z, W_r, U_r, W_h):\n",
    "    hidden_size = x.size()[-1]\n",
    "    sum_h = h_nei.sum(dim=1)\n",
    "    z_input = torch.cat([x,sum_h], dim=1)\n",
    "    z = nn.Sigmoid()(W_z(z_input))\n",
    "\n",
    "    r_1 = W_r(x).view(-1,1,hidden_size)\n",
    "    r_2 = U_r(h_nei)\n",
    "    r = nn.Sigmoid()(r_1 + r_2)\n",
    "    \n",
    "    gated_h = r * h_nei\n",
    "    sum_gated_h = gated_h.sum(dim=1)\n",
    "    h_input = torch.cat([x,sum_gated_h], dim=1)\n",
    "    pre_h = nn.Tanh()(W_h(h_input))\n",
    "    new_h = (1.0 - z) * sum_h + z * pre_h\n",
    "    return new_h\n",
    "\n",
    "def dfs(stack, x, fa):\n",
    "    for y in x.neighbors:\n",
    "        if y.idx == fa.idx:\n",
    "            continue\n",
    "        stack.append((x, y, 1))\n",
    "        dfs(stack, y, x)\n",
    "        stack.append((y, x, 0))\n",
    "\n",
    "\n",
    "def have_slots(fa_slots, ch_slots):\n",
    "    if len(fa_slots) > 2 and len(ch_slots) > 2:\n",
    "        return True\n",
    "    matches = []\n",
    "    for i, s1 in enumerate(fa_slots):\n",
    "        a1, c1, h1 = s1\n",
    "        for j, s2 in enumerate(ch_slots):\n",
    "            a2, c2, h2 = s2\n",
    "            if a1 == a2 and c1 == c2 and (a1 != \"C\" or h1 + h2 >= 4):\n",
    "                matches.append((i, j))\n",
    "\n",
    "    if len(matches) == 0: return False\n",
    "\n",
    "    fa_match, ch_match = zip(*matches)\n",
    "    if len(set(fa_match)) == 1 and 1 < len(fa_slots) <= 2:  # never remove atom from ring\n",
    "        fa_slots.pop(fa_match[0])\n",
    "    if len(set(ch_match)) == 1 and 1 < len(ch_slots) <= 2:  # never remove atom from ring\n",
    "        ch_slots.pop(ch_match[0])\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def can_assemble(node_x, node_y):\n",
    "    neis = node_x.neighbors + [node_y]\n",
    "    for i, nei in enumerate(neis):\n",
    "        nei.nid = i\n",
    "\n",
    "    neighbors = [nei for nei in neis if nei.mol.GetNumAtoms() > 1]\n",
    "    neighbors = sorted(neighbors, key=lambda x: x.mol.GetNumAtoms(), reverse=True)\n",
    "    singletons = [nei for nei in neis if nei.mol.GetNumAtoms() == 1]\n",
    "    neighbors = singletons + neighbors\n",
    "    cands = enum_assemble(node_x, neighbors)\n",
    "    return len(cands) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46e07d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Motif_Generation_dfs(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, hidden_size, device):       #각종 초기값들 설정해두기\n",
    "        super(Motif_Generation_dfs, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab.size()\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "\n",
    "        # GRU Weights\n",
    "        self.W_z = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.U_r = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_r = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_h = nn.Linear(2 * hidden_size, hidden_size)\n",
    "\n",
    "        # Feature Aggregate Weights\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U = nn.Linear(2 * hidden_size, hidden_size)\n",
    "\n",
    "        # Output Weights\n",
    "        self.W_o = nn.Linear(hidden_size, self.vocab_size)\n",
    "        self.U_s = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Loss Functions\n",
    "        self.pred_loss = nn.CrossEntropyLoss(size_average=False)\n",
    "        self.stop_loss = nn.BCEWithLogitsLoss(size_average=False)\n",
    "\n",
    "    def get_trace(self, node):\t\t#trace란것을 얻을건데, \n",
    "        super_root = MolTreeNode(\"\")\n",
    "        super_root.idx = -1\n",
    "        trace = []\n",
    "        dfs(trace, node, super_root)\n",
    "        return [(x.smiles, y.smiles, z) for x, y, z in trace]\n",
    "\n",
    "    def forward(self, mol_batch, node_rep):\n",
    "        super_root = MolTreeNode(\"\")\n",
    "        super_root.idx = -1\n",
    "\n",
    "        # Initialize\n",
    "        pred_hiddens, pred_targets = [], []\n",
    "        stop_hiddens, stop_targets = [], []\n",
    "        traces = []\n",
    "        for mol_tree in mol_batch:\n",
    "            s = []\n",
    "            dfs(s, mol_tree.nodes[0], super_root)\n",
    "            traces.append(s)\n",
    "            for node in mol_tree.nodes:\n",
    "                node.neighbors = []\n",
    "        '''\n",
    "        # Predict Root\n",
    "        pred_hiddens.append(create_var(torch.zeros(len(mol_batch), self.hidden_size)))\n",
    "        pred_targets.extend([mol_tree.nodes[0].wid for mol_tree in mol_batch])\n",
    "        pred_mol_vecs.append(mol_vec)\n",
    "        '''\n",
    "\n",
    "        max_iter = max([len(tr) for tr in traces])\n",
    "        padding = create_var(torch.zeros(self.hidden_size), False)\n",
    "        h = {}\n",
    "\n",
    "        for t in range(max_iter):\n",
    "            prop_list = []\n",
    "            batch_list = []\n",
    "            for i, plist in enumerate(traces):\n",
    "                if t < len(plist):\n",
    "                    prop_list.append(plist[t])\n",
    "                    batch_list.append(i)\n",
    "                else:\n",
    "                    prop_list.append(None)\n",
    "\n",
    "            em_list = []\n",
    "            cur_h_nei, cur_o_nei = [], []\n",
    "\n",
    "            for mol_index, prop in enumerate(prop_list):\n",
    "                if prop is None:\n",
    "                    continue\n",
    "                node_x, real_y, _ = prop\n",
    "                # Neighbors for message passing (target not included)\n",
    "                cur_nei = [h[(node_y.idx, node_x.idx)] for node_y in node_x.neighbors if node_y.idx != real_y.idx]\n",
    "                pad_len = MAX_NB - len(cur_nei)\n",
    "                if pad_len>= 0:\n",
    "                    cur_h_nei.extend(cur_nei)\n",
    "                    cur_h_nei.extend([padding] * pad_len)\n",
    "                else:\n",
    "                    cur_h_nei.extend(cur_nei[:MAX_NB])\n",
    "\n",
    "                # Neighbors for stop prediction (all neighbors)\n",
    "                cur_nei = [h[(node_y.idx, node_x.idx)] for node_y in node_x.neighbors]\n",
    "                pad_len = MAX_NB - len(cur_nei)\n",
    "                if pad_len >= 0:\n",
    "                    cur_o_nei.extend(cur_nei)\n",
    "                    cur_o_nei.extend([padding] * pad_len)\n",
    "                else:\n",
    "                    cur_o_nei.extend(cur_nei[:MAX_NB])\n",
    "\n",
    "\n",
    "                # Current clique embedding\n",
    "                em_list.append(torch.sum(node_rep[mol_index].index_select(0, torch.tensor(node_x.clique).to(self.device)), dim=0))\n",
    "\n",
    "            # Clique embedding\n",
    "            cur_x = torch.stack(em_list, dim=0)\n",
    "\n",
    "            # Message passing\n",
    "            cur_h_nei = torch.stack(cur_h_nei, dim=0).view(-1, MAX_NB, self.hidden_size)\n",
    "            new_h = GRU(cur_x, cur_h_nei, self.W_z, self.W_r, self.U_r, self.W_h)\n",
    "\n",
    "            # Node Aggregate\n",
    "            cur_o_nei = torch.stack(cur_o_nei, dim=0).view(-1, MAX_NB, self.hidden_size)\n",
    "            cur_o = cur_o_nei.sum(dim=1)\n",
    "\n",
    "            # Gather targets\n",
    "            pred_target, pred_list = [], []\n",
    "            stop_target = []\n",
    "            prop_list = [x for x in prop_list if x is not None]\n",
    "            for i, m in enumerate(prop_list):\n",
    "                node_x, node_y, direction = m\n",
    "                x, y = node_x.idx, node_y.idx\n",
    "                h[(x, y)] = new_h[i]\n",
    "                node_y.neighbors.append(node_x)\n",
    "                if direction == 1:\n",
    "                    pred_target.append(node_y.wid)\n",
    "                    pred_list.append(i)\n",
    "                stop_target.append(direction)\n",
    "\n",
    "            # Hidden states for stop prediction\n",
    "            stop_hidden = torch.cat([cur_x, cur_o], dim=1)\n",
    "            stop_hiddens.append(stop_hidden)\n",
    "            stop_targets.extend(stop_target)\n",
    "\n",
    "            # Hidden states for clique prediction\n",
    "            if len(pred_list) > 0:\n",
    "                #batch_list = [batch_list[i] for i in pred_list]\n",
    "                #cur_batch = create_var(torch.LongTensor(batch_list))\n",
    "                #pred_mol_vecs.append(mol_vec.index_select(0, cur_batch))\n",
    "\n",
    "                cur_pred = create_var(torch.LongTensor(pred_list))\n",
    "                pred_hiddens.append(new_h.index_select(0, cur_pred))\n",
    "                pred_targets.extend(pred_target)\n",
    "\n",
    "        # Last stop at root\n",
    "        em_list, cur_o_nei = [], []\n",
    "        for mol_index, mol_tree in enumerate(mol_batch):\n",
    "            node_x = mol_tree.nodes[0]\n",
    "            em_list.append(torch.sum(node_rep[mol_index].index_select(0, torch.tensor(node_x.clique).to(self.device)), dim=0))\n",
    "            cur_nei = [h[(node_y.idx, node_x.idx)] for node_y in node_x.neighbors]\n",
    "            pad_len = MAX_NB - len(cur_nei)\n",
    "            cur_o_nei.extend(cur_nei)\n",
    "            cur_o_nei.extend([padding] * pad_len)\n",
    "\n",
    "        cur_x = torch.stack(em_list, dim=0)\n",
    "        cur_o_nei = torch.stack(cur_o_nei, dim=0).view(-1, MAX_NB, self.hidden_size)\n",
    "        cur_o = cur_o_nei.sum(dim=1)\n",
    "\n",
    "        stop_hidden = torch.cat([cur_x, cur_o], dim=1)\n",
    "        stop_hiddens.append(stop_hidden)\n",
    "        stop_targets.extend([0] * len(mol_batch))\n",
    "\n",
    "        # Predict next clique\n",
    "        pred_hiddens = torch.cat(pred_hiddens, dim=0)\n",
    "        #pred_mol_vecs = torch.cat(pred_mol_vecs, dim=0)\n",
    "        #pred_vecs = torch.cat([pred_hiddens, pred_mol_vecs], dim=1)\n",
    "        pred_vecs = pred_hiddens\n",
    "        pred_vecs = nn.ReLU()(self.W(pred_vecs))\n",
    "        pred_scores = self.W_o(pred_vecs)\n",
    "        pred_targets = create_var(torch.LongTensor(pred_targets))\n",
    "\n",
    "        pred_loss = self.pred_loss(pred_scores, pred_targets) / len(mol_batch)\n",
    "        _, preds = torch.max(pred_scores, dim=1)\n",
    "        pred_acc = torch.eq(preds, pred_targets).float()\n",
    "        pred_acc = torch.sum(pred_acc) / pred_targets.nelement()\n",
    "\n",
    "        # Predict stop\n",
    "        stop_hiddens = torch.cat(stop_hiddens, dim=0)\n",
    "        stop_vecs = nn.ReLU()(self.U(stop_hiddens))\n",
    "        stop_scores = self.U_s(stop_vecs).squeeze()\n",
    "        stop_targets = create_var(torch.Tensor(stop_targets))\n",
    "\n",
    "        stop_loss = self.stop_loss(stop_scores, stop_targets) / len(mol_batch)\n",
    "        stops = torch.ge(stop_scores, 0).float()\n",
    "        stop_acc = torch.eq(stops, stop_targets).float()\n",
    "        stop_acc = torch.sum(stop_acc) / stop_targets.nelement()\n",
    "\n",
    "        return pred_loss, stop_loss, pred_acc.item(), stop_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17d1c5-67bd-4502-a506-d82639f772d6",
   "metadata": {},
   "source": [
    "#### 3-2-3. MOotif_Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e23665c3-459b-42a8-aaff-0c6fb7d64761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Motif_Generation(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, hidden_size, latent_size, depth, device, order):\n",
    "        super(Motif_Generation, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.device = device\n",
    "        if order == 'dfs':\n",
    "            self.decoder = Motif_Generation_dfs(vocab, hidden_size, self.device)\n",
    "        elif order == 'bfs':\n",
    "            self.decoder = Motif_Generation_bfs(vocab, hidden_size, self.device)\n",
    "\n",
    "    def forward(self, mol_batch, node_rep):\n",
    "        set_batch_nodeID(mol_batch, self.vocab)\n",
    "\n",
    "        word_loss, topo_loss, word_acc, topo_acc = self.decoder(mol_batch, node_rep)\n",
    "\n",
    "        return word_loss, topo_loss, word_acc, topo_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e6cde-6340-48c7-929d-216d96ccac99",
   "metadata": {},
   "source": [
    "#### 3-2-4. Grover_MotifTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "644c6fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroverMotifTask(nn.Module):\n",
    "    \"\"\"\n",
    "    The pretrain module.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, grover, atom_vocab_size, bond_vocab_size, fg_size):\n",
    "        super(GroverMotifTask, self).__init__()\n",
    "        self.grover = grover\n",
    "        self.av_task_atom = AtomVocabPrediction(args, atom_vocab_size)\n",
    "        self.av_task_bond = AtomVocabPrediction(args, atom_vocab_size)\n",
    "        self.bv_task_atom = BondVocabPrediction(args, bond_vocab_size)\n",
    "        self.bv_task_bond = BondVocabPrediction(args, bond_vocab_size)\n",
    "\n",
    "        self.fg_task_all = FunctionalGroupPrediction(args, fg_size)\n",
    "\n",
    "        self.embedding_output_type = args.embedding_output_type\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_func(args: Namespace) -> Callable:\n",
    "        \"\"\"\n",
    "        The loss function generator.\n",
    "        :param args: the arguments.\n",
    "        :return: the loss fucntion for GroverTask.\n",
    "        \"\"\"\n",
    "        def loss_func(preds, targets, dist_coff=args.dist_coff):\n",
    "            \"\"\"\n",
    "            The loss function for GroverTask.\n",
    "            :param preds: the predictions.\n",
    "            :param targets: the targets.\n",
    "            :param dist_coff: the default disagreement coefficient for the distances between different branches.\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            av_task_loss = nn.NLLLoss(ignore_index=0, reduction=\"mean\")  # same for av and bv\n",
    "\n",
    "            fg_task_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            # av_task_dist_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "            av_task_dist_loss = nn.MSELoss(reduction=\"mean\")\n",
    "            fg_task_dist_loss = nn.MSELoss(reduction=\"mean\")\n",
    "            sigmoid = nn.Sigmoid()\n",
    "\n",
    "            av_atom_loss, av_bond_loss, av_dist_loss = 0.0, 0.0, 0.0\n",
    "            fg_atom_from_atom_loss, fg_atom_from_bond_loss, fg_atom_dist_loss = 0.0, 0.0, 0.0\n",
    "            bv_atom_loss, bv_bond_loss, bv_dist_loss = 0.0, 0.0, 0.0\n",
    "            fg_bond_from_atom_loss, fg_bond_from_bond_loss, fg_bond_dist_loss = 0.0, 0.0, 0.0\n",
    "\n",
    "            if preds[\"av_task\"][0] is not None:\n",
    "                av_atom_loss = av_task_loss(preds['av_task'][0], targets[\"av_task\"])\n",
    "                fg_atom_from_atom_loss = fg_task_loss(preds[\"fg_task\"][\"atom_from_atom\"], targets[\"fg_task\"])\n",
    "\n",
    "            if preds[\"av_task\"][1] is not None:\n",
    "                av_bond_loss = av_task_loss(preds['av_task'][1], targets[\"av_task\"])\n",
    "                fg_atom_from_bond_loss = fg_task_loss(preds[\"fg_task\"][\"atom_from_bond\"], targets[\"fg_task\"])\n",
    "\n",
    "            if preds[\"bv_task\"][0] is not None:\n",
    "                bv_atom_loss = av_task_loss(preds['bv_task'][0], targets[\"bv_task\"])\n",
    "                fg_bond_from_atom_loss = fg_task_loss(preds[\"fg_task\"][\"bond_from_atom\"], targets[\"fg_task\"])\n",
    "\n",
    "            if preds[\"bv_task\"][1] is not None:\n",
    "                bv_bond_loss = av_task_loss(preds['bv_task'][1], targets[\"bv_task\"])\n",
    "                fg_bond_from_bond_loss = fg_task_loss(preds[\"fg_task\"][\"bond_from_bond\"], targets[\"fg_task\"])\n",
    "\n",
    "            if preds[\"av_task\"][0] is not None and preds[\"av_task\"][1] is not None:\n",
    "                av_dist_loss = av_task_dist_loss(preds['av_task'][0], preds['av_task'][1])\n",
    "                fg_atom_dist_loss = fg_task_dist_loss(sigmoid(preds[\"fg_task\"][\"atom_from_atom\"]),\n",
    "                                                      sigmoid(preds[\"fg_task\"][\"atom_from_bond\"]))\n",
    "\n",
    "            if preds[\"bv_task\"][0] is not None and preds[\"bv_task\"][1] is not None:\n",
    "                bv_dist_loss = av_task_dist_loss(preds['bv_task'][0], preds['bv_task'][1])\n",
    "                fg_bond_dist_loss = fg_task_dist_loss(sigmoid(preds[\"fg_task\"][\"bond_from_atom\"]),\n",
    "                                                      sigmoid(preds[\"fg_task\"][\"bond_from_bond\"]))\n",
    "                \n",
    "            #if \n",
    "\n",
    "            av_loss = av_atom_loss + av_bond_loss\n",
    "            bv_loss = bv_atom_loss + bv_bond_loss\n",
    "            fg_atom_loss = fg_atom_from_atom_loss + fg_atom_from_bond_loss\n",
    "            fg_bond_loss = fg_bond_from_atom_loss + fg_bond_from_bond_loss\n",
    "\n",
    "            fg_loss = fg_atom_loss + fg_bond_loss\n",
    "            fg_dist_loss = fg_atom_dist_loss + fg_bond_dist_loss\n",
    "\n",
    "            # dist_loss = av_dist_loss + bv_dist_loss + fg_dist_loss\n",
    "            # print(\"%.4f %.4f %.4f %.4f %.4f %.4f\"%(av_atom_loss,\n",
    "            #                                       av_bond_loss,\n",
    "            #                                       fg_atom_loss,\n",
    "            #                                       fg_bond_loss,\n",
    "            #                                       av_dist_loss,\n",
    "            #                                       fg_dist_loss))\n",
    "            # return av_loss + fg_loss + dist_coff * dist_loss\n",
    "            overall_loss = av_loss + bv_loss + fg_loss + dist_coff * av_dist_loss + \\\n",
    "                           dist_coff * bv_dist_loss + fg_dist_loss\n",
    "\n",
    "            return overall_loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss\n",
    "\n",
    "        return loss_func\n",
    "\n",
    "    def forward(self, graph_batch: List):\n",
    "        \"\"\"\n",
    "        The forward function.\n",
    "        :param graph_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        _, _, _, _, _, a_scope, b_scope, _ = graph_batch\n",
    "        a_scope = a_scope.data.cpu().numpy().tolist()\n",
    "\n",
    "        embeddings = self.grover(graph_batch)\n",
    "\n",
    "        av_task_pred_atom = self.av_task_atom(\n",
    "            embeddings[\"atom_from_atom\"])  # if None: means not go through this fowward\n",
    "        av_task_pred_bond = self.av_task_bond(embeddings[\"atom_from_bond\"])\n",
    "\n",
    "        bv_task_pred_atom = self.bv_task_atom(embeddings[\"bond_from_atom\"])\n",
    "        bv_task_pred_bond = self.bv_task_bond(embeddings[\"bond_from_bond\"])\n",
    "\n",
    "        fg_task_pred_all = self.fg_task_all(embeddings, a_scope, b_scope)\n",
    "\n",
    "        return {\"av_task\": (av_task_pred_atom, av_task_pred_bond),\n",
    "                \"bv_task\": (bv_task_pred_atom, bv_task_pred_bond),\n",
    "                \"fg_task\": fg_task_pred_all,\n",
    "                \"emb_vec\": embeddings}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55287704-9342-498c-b7a1-97a3477a5dd7",
   "metadata": {},
   "source": [
    "#### group_node_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fadd82c4-2ad5-4cbe-864c-fdad9c0ea0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_node_rep(moltree, node_rep, batch_graph):\n",
    "    group = []\n",
    "    count = 1\n",
    "    for i in range(len(moltree)):\n",
    "        num=batch_graph[5][i][1]\n",
    "        group.append(node_rep[count:count + num])\t\t# count += num번째 node의 표현을 그룹에 더해라\n",
    "        count += num\n",
    "    return group\t\t\t\t\t\t# 최종 그룹을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2244115-2b38-4701-a169-e21e66a4a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_edge_rep(moltree, edge_rep, batch_graph):\n",
    "    group = []\n",
    "    count = 1\n",
    "    for i in range(len(moltree)):\n",
    "        num=batch_graph[6][i][1]\n",
    "        group.append(edge_rep[count:count + num])\t\t# count += num번째 node의 표현을 그룹에 더해라\n",
    "        count += num\n",
    "    return group\t\t\t\t\t\t# 최종 그룹을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5e603",
   "metadata": {},
   "source": [
    "#### 3-2-5. Grover_motiftrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07eabfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GROVERMotifTrainer:\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 embedding_model: Module,\n",
    "                 topology_model: Module,\n",
    "                 atom_vocab_size: int,  # atom vocab size\n",
    "                 bond_vocab_size: int,\n",
    "                 fg_size: int,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 test_dataloader: DataLoader,\n",
    "                 optimizer_builder: Callable,\n",
    "                 scheduler_builder: Callable,\n",
    "                 logger: Logger = None,\n",
    "                 with_cuda: bool = False,\n",
    "                 enable_multi_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        The init function of GROVERTrainer\n",
    "        :param args: the input arguments.\n",
    "        :param embedding_model: the model to generate atom/bond embeddings.\n",
    "        :param topology_model : the model to predict topology of molecule from embeddings\n",
    "        :param atom_vocab_size: the vocabulary size of atoms.\n",
    "        :param bond_vocab_size: the vocabulary size of bonds.\n",
    "        :param fg_size: the size of semantic motifs (functional groups)\n",
    "        :param train_dataloader: the data loader of train data.\n",
    "        :param test_dataloader: the data loader of validation data.\n",
    "        :param optimizer_builder: the function of building the optimizer.\n",
    "        :param scheduler_builder: the function of building the scheduler.\n",
    "        :param logger: the logger\n",
    "        :param with_cuda: enable gpu training.\n",
    "        :param enable_multi_gpu: enable multi_gpu traning.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.with_cuda = with_cuda\n",
    "        self.grover = embedding_model\n",
    "        self.model = GroverMotifTask(args, embedding_model, atom_vocab_size, bond_vocab_size, fg_size)\n",
    "        self.motif_model = topology_model\n",
    "        self.loss_func = self.model.get_loss_func(args)\n",
    "        self.enable_multi_gpu = enable_multi_gpu\n",
    "\n",
    "        self.atom_vocab_size = atom_vocab_size\n",
    "        self.bond_vocab_size = bond_vocab_size\n",
    "        self.debug = logger.debug if logger is not None else print\n",
    "\n",
    "        if self.with_cuda:\n",
    "            # print(\"Using %d GPUs for training.\" % (torch.cuda.device_count()))\n",
    "            self.model = self.model.cuda()\n",
    "            self.motif_model = self.motif_model.cuda()\n",
    "\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        self.optimizer = optimizer_builder(self.model, self.args)\n",
    "        self.motif_optimizer = torch.optim.Adam(self.motif_model.parameters(), lr=args.init_lr, weight_decay=args.weight_decay)\n",
    "        self.scheduler = scheduler_builder(self.optimizer, self.args)\n",
    "        if self.enable_multi_gpu:\n",
    "            self.optimizer = mgw.DistributedOptimizer(self.optimizer,\n",
    "                                                      named_parameters=self.model.named_parameters())\n",
    "        self.args = args\n",
    "        self.n_iter = 0\n",
    "\n",
    "    def broadcast_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Broadcast parameters before training.\n",
    "        :return: no return.\n",
    "        \"\"\"\n",
    "        if self.enable_multi_gpu:\n",
    "            # broadcast parameters & optimizer state.\n",
    "            mgw.broadcast_parameters(self.model.state_dict(), root_rank=0)\n",
    "            mgw.broadcast_optimizer_state(self.optimizer, root_rank=0)\n",
    "\n",
    "    def train(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The training iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return: the loss terms of current epoch.\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.train_data, train=True)\n",
    "        return self.iter(epoch, self.train_data, train=True)\n",
    "\n",
    "    def test(self, epoch: int) -> List:\n",
    "        \"\"\"\n",
    "        The test/validaiion iteration\n",
    "        :param epoch: the current epoch number.\n",
    "        :return:  the loss terms as a list\n",
    "        \"\"\"\n",
    "        # return self.mock_iter(epoch, self.test_data, train=False)\n",
    "        return self.iter(epoch, self.test_data, train=False)\n",
    "\n",
    "    def mock_iter(self, epoch: int, data_loader: DataLoader, train: bool = True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a mock iteration. For test only.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        for _, _ in enumerate(data_loader):\n",
    "            self.scheduler.step()\n",
    "        cum_loss_sum = 0.0\n",
    "        self.n_iter += self.args.batch_size\n",
    "        return self.n_iter, cum_loss_sum, (0, 0, 0, 0, 0, 0)\n",
    "\n",
    "    def iter(self, epoch, data_loader, train=True) -> List:\n",
    "        \"\"\"\n",
    "        Perform a training / validation iteration.\n",
    "        :param epoch: the current epoch number.\n",
    "        :param data_loader: the data loader.\n",
    "        :param train: True: train model, False: validation model.\n",
    "        :return: the loss terms as a list\n",
    "        \"\"\"\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            self.motif_model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            self.motif_model.eval()\n",
    "            \n",
    "        time1 = time.time()\n",
    "        print(f'iter start')\n",
    "        loss_sum, iter_count = 0, 0\n",
    "        cum_loss_sum, cum_iter_count = 0, 0\n",
    "        av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum, bv_dist_loss_sum, fg_dist_loss_sum, topo_loss_sum, node_loss_sum = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        # loss_func = self.model.get_loss_func(self.args)\n",
    "\n",
    "        for _, item in enumerate(data_loader):\n",
    "            batch_graph = item[\"graph_input\"]\n",
    "            targets = item[\"targets\"]\n",
    "            \n",
    "            # add this for motif generation\n",
    "            moltree = item[\"moltree\"]\n",
    "            #list_graph = list(batch_graph)\n",
    "            #new_graph=list_graph[0][1:],list_graph[1][1:], list_graph[2][1:], list_graph[3][1:], list_graph[4][1:], list_graph[5][:], list_graph[6][:], list_graph[7][1:]\n",
    "            \n",
    "            time2 = time.time()\n",
    "            print(f'dataloader time is {time2-time1}')\n",
    "            \n",
    "            if next(self.model.parameters()).is_cuda:\n",
    "                targets[\"av_task\"] = targets[\"av_task\"].cuda()\n",
    "                targets[\"bv_task\"] = targets[\"bv_task\"].cuda()\n",
    "                targets[\"fg_task\"] = targets[\"fg_task\"].cuda()\n",
    "            \n",
    "            preds = self.model(batch_graph)\n",
    "            emb_vector = preds['emb_vec']\n",
    "            \n",
    "            time3 = time.time()\n",
    "            print(f'pred time is {time3-time2}')\n",
    "\n",
    "            # add this for motif generation\n",
    "            if self.args.embedding_output_type == 'atom':\n",
    "                emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "                emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "                \n",
    "                node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = self.motif_model(moltree, emb_afa_grouped)\n",
    "                node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = self.motif_model(moltree, emb_afb_grouped)\n",
    "                \n",
    "                node_loss = node_afa_loss + node_afb_loss\n",
    "                topo_loss = topo_afa_loss + topo_afb_loss\n",
    "                node_acc = (node_afa_acc + node_afb_acc)/2\n",
    "                topo_acc = (topo_afa_acc + topo_afb_acc)/2\n",
    "                \n",
    "            elif self.args.embedding_output_type == 'bond':\n",
    "                emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "                emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "                \n",
    "                node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = self.motif_model(moltree, emb_bfa_grouped)\n",
    "                node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = self.motif_model(moltree, emb_bfb_grouped)\n",
    "                \n",
    "                node_loss = node_bfa_loss + node_bfb_loss\n",
    "                topo_loss = topo_bfa_loss + topo_bfb_loss\n",
    "                node_acc = (node_bfa_acc + node_bfb_acc)/2\n",
    "                topo_acc = (topo_bfa_acc + topo_bfb_acc)/2\n",
    "                \n",
    "            elif self.args.embedding_output_type == \"both\":\n",
    "                emb_afa_grouped = group_node_rep(moltree, emb_vector['atom_from_atom'],batch_graph)\n",
    "                emb_afb_grouped = group_node_rep(moltree, emb_vector['atom_from_bond'],batch_graph)\n",
    "                emb_bfa_grouped = group_node_rep(moltree, emb_vector['bond_from_atom'],batch_graph)\n",
    "                emb_bfb_grouped = group_node_rep(moltree, emb_vector['bond_from_bond'],batch_graph)\n",
    "                \n",
    "                node_afa_loss, topo_afa_loss, node_afa_acc, topo_afa_acc = self.motif_model(moltree, emb_afa_grouped)\n",
    "                node_afb_loss, topo_afb_loss, node_afb_acc, topo_afb_acc = self.motif_model(moltree, emb_afb_grouped)\n",
    "                node_bfa_loss, topo_bfa_loss, node_bfa_acc, topo_bfa_acc = self.motif_model(moltree, emb_bfa_grouped)\n",
    "                node_bfb_loss, topo_bfb_loss, node_bfb_acc, topo_bfb_acc = self.motif_model(moltree, emb_bfb_grouped)\n",
    "                \n",
    "                node_loss = node_afa_loss + node_afb_loss + node_bfa_loss + node_bfb_loss\n",
    "                topo_loss = topo_afa_loss + topo_afb_loss + topo_bfa_loss + topo_bfb_loss\n",
    "                node_acc = (node_afa_acc + node_afb_acc + node_bfa_acc + node_bfb_acc)/4\n",
    "                topo_acc = (topo_afa_acc + topo_afb_acc + topo_bfa_acc + topo_bfb_acc)/4\n",
    "\n",
    "            # # ad-hoc code, for visualizing a model, comment this block when it is not needed\n",
    "            # import dglt.contrib.grover.vis_model as vis_model\n",
    "            # for task in ['av_task', 'bv_task', 'fg_task']:\n",
    "            #     vis_graph = vis_model.make_dot(self.model(batch_graph)[task],\n",
    "            #                                    params=dict(self.model.named_parameters()))\n",
    "            #     # vis_graph.view()\n",
    "            #     vis_graph.render(f\"{self.args.backbone}_model_{task}_vis.png\", format=\"png\")\n",
    "            # exit()\n",
    "            \n",
    "            time4 = time.time()\n",
    "            print(f'motif_model time is {time4-time3}')\n",
    "\n",
    "            loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss = self.loss_func(preds, targets)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            iter_count += self.args.batch_size\n",
    "            \n",
    "            # add for topology loss\n",
    "            loss += topo_loss.item()\n",
    "            loss += node_loss.item()\n",
    "            topo_loss_sum += topo_loss.item()\n",
    "            node_loss_sum += node_loss.item()\n",
    "\n",
    "            if train:\n",
    "                cum_loss_sum += loss.item()\n",
    "                # Run model\n",
    "                self.model.zero_grad()\n",
    "                self.motif_model.zero_grad()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                # For eval model, only consider the loss of three task.\n",
    "                cum_loss_sum += av_loss.item()\n",
    "                cum_loss_sum += bv_loss.item()\n",
    "                cum_loss_sum += fg_loss.item()\n",
    "\n",
    "            av_loss_sum += av_loss.item()\n",
    "            bv_loss_sum += bv_loss.item()\n",
    "            fg_loss_sum += fg_loss.item()\n",
    "            av_dist_loss_sum += av_dist_loss.item() if type(av_dist_loss) != float else av_dist_loss\n",
    "            bv_dist_loss_sum += bv_dist_loss.item() if type(bv_dist_loss) != float else bv_dist_loss\n",
    "            fg_dist_loss_sum += fg_dist_loss.item() if type(fg_dist_loss) != float else fg_dist_loss\n",
    "\n",
    "            cum_iter_count += 1\n",
    "            self.n_iter += self.args.batch_size\n",
    "\n",
    "            # Debug only.\n",
    "            # if i % 50 == 0:\n",
    "            #     print(f\"epoch: {epoch}, batch_id: {i}, av_loss: {av_loss}, bv_loss: {bv_loss}, \"\n",
    "            #           f\"fg_loss: {fg_loss}, av_dist_loss: {av_dist_loss}, bv_dist_loss: {bv_dist_loss}, \"\n",
    "            #           f\"fg_dist_loss: {fg_dist_loss}\")\n",
    "            \n",
    "            time1=time.time()\n",
    "            print(f'loss time is {time1-time4}')\n",
    "\n",
    "        cum_loss_sum /= cum_iter_count\n",
    "        av_loss_sum /= cum_iter_count\n",
    "        bv_loss_sum /= cum_iter_count\n",
    "        fg_loss_sum /= cum_iter_count\n",
    "        av_dist_loss_sum /= cum_iter_count\n",
    "        bv_dist_loss_sum /= cum_iter_count\n",
    "        fg_dist_loss_sum /= cum_iter_count\n",
    "        \n",
    "        topo_loss_sum /= cum_iter_count\n",
    "        node_loss_sum /= cum_iter_count\n",
    "\n",
    "        return self.n_iter, cum_loss_sum, (av_loss_sum, bv_loss_sum, fg_loss_sum, av_dist_loss_sum,\n",
    "                                           bv_dist_loss_sum, fg_dist_loss_sum, topo_loss_sum, node_loss_sum)\n",
    "\n",
    "    def save(self, epoch, file_path, name=None) -> str:\n",
    "        \"\"\"\n",
    "        Save the intermediate models during training.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to save the model.\n",
    "        :return: the output path.\n",
    "        \"\"\"\n",
    "        # add specific time in model fine name, in order to distinguish different saved models\n",
    "        now = time.localtime()\n",
    "        if name is None:\n",
    "            name = \"_%04d_%02d_%02d_%02d_%02d_%02d\" % (\n",
    "                now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "        output_path = file_path + name + \".ep%d\" % epoch\n",
    "        scaler = None\n",
    "        features_scaler = None\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch,\n",
    "            'data_scaler': {\n",
    "                'means': scaler.means,\n",
    "                'stds': scaler.stds\n",
    "            } if scaler is not None else None,\n",
    "            'features_scaler': {\n",
    "                'means': features_scaler.means,\n",
    "                'stds': features_scaler.stds\n",
    "            } if features_scaler is not None else None\n",
    "        }\n",
    "        torch.save(state, output_path)\n",
    "\n",
    "        # Is this necessary?\n",
    "        # if self.with_cuda:\n",
    "        #    self.model = self.model.cuda()\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "\n",
    "    def save_tmp(self, epoch, file_path, rank=0):\n",
    "        \"\"\"\n",
    "        Save the models for auto-restore during training.\n",
    "        The model are stored in file_path/tmp folder and will replaced on each epoch.\n",
    "        :param epoch: the epoch number.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        store_path = os.path.join(file_path, \"tmp\")\n",
    "        if not os.path.exists(store_path):\n",
    "            os.makedirs(store_path, exist_ok=True)\n",
    "        store_path = os.path.join(store_path, \"model.%d\" % rank)\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler_step': self.scheduler.current_step,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        torch.save(state, store_path)\n",
    "\n",
    "    def restore(self, file_path, rank=0) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Restore the training state saved by save_tmp.\n",
    "        :param file_path: the file_path to store the model.\n",
    "        :param rank: the current rank (decrypted).\n",
    "        :return: the restored epoch number and the scheduler_step in scheduler.\n",
    "        \"\"\"\n",
    "        cpt_path = os.path.join(file_path, \"tmp\", \"model.%d\" % rank)\n",
    "        if not os.path.exists(cpt_path):\n",
    "            print(\"No checkpoint found %d\")\n",
    "            return 0, 0\n",
    "        cpt = torch.load(cpt_path)\n",
    "        self.model.load_state_dict(cpt[\"state_dict\"])\n",
    "        self.optimizer.load_state_dict(cpt[\"optimizer\"])\n",
    "        epoch = cpt[\"epoch\"]\n",
    "        scheduler_step = cpt[\"scheduler_step\"]\n",
    "        self.scheduler.current_step = scheduler_step\n",
    "        print(\"Restore checkpoint, current epoch: %d\" % (epoch))\n",
    "        return epoch, scheduler_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cbc42-ae5d-4fd1-9f0e-c199391be63e",
   "metadata": {},
   "source": [
    "##### 3-2-3-1 trainer 세부 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58b9d5b0-fabd-4595-89e1-9de7e51ebb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slots(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return [(atom.GetSymbol(), atom.GetFormalCharge(), atom.GetTotalNumHs()) for atom in mol.GetAtoms()]\n",
    "        \n",
    "class Motif_Vocab(object):\n",
    "\n",
    "    def __init__(self, smiles_list):\n",
    "        self.vocab = smiles_list\n",
    "        self.vmap = {x:i for i,x in enumerate(self.vocab)}\n",
    "        self.slots = [get_slots(smiles) for smiles in self.vocab]\n",
    "        \n",
    "    def get_index(self, smiles):\n",
    "        try : return self.vmap[smiles]\n",
    "        except : return [0]\n",
    "    \n",
    "    def get_smiles(self, idx):\n",
    "        return self.vocab[idx]\n",
    "\n",
    "    def get_slots(self, idx):\n",
    "        return copy.deepcopy(self.slots[idx])\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def add_motif(self, smiles):\n",
    "        self.vocab.append(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fe41b85-9727-439e-80bb-ab9c9ac44150",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.parser_name == 'pretrain':\n",
    "    motif_vocab = [x.strip(\"\\r\\n \") for x in open(args.motif_vocab_path)]\n",
    "    motif_vocab = Motif_Vocab(motif_vocab)\n",
    "    #see below motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0de07e6-7d1b-4043-803f-ed0f99cc36df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GroverMotifTask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3945/878915818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_model_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_model_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroverMotifTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0membed_model_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroverMotifTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GroverMotifTask' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_model = GROVEREmbedding(args)\n",
    "embedding_model.cuda()\n",
    "embedding_model_test = GROVEREmbedding(args)\n",
    "embedding_model_test.cuda()\n",
    "embedding_model_test.eval()\n",
    "embed_model = GroverMotifTask(args, embedding_model, atom_vocab_size, bond_vocab_size, fg_size)\n",
    "embed_model.cuda()\n",
    "embed_model_test = GroverMotifTask(args, embedding_model, atom_vocab_size, bond_vocab_size, fg_size)\n",
    "embed_model_test.cuda()\n",
    "embed_model_test.eval()\n",
    "motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc858162-db3c-4ac6-83a4-819ad9c666fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.__getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933df0dd-4458-4dbc-8d55-a8dcfbe58aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_collator = GroverMotifCollator(shared_dict=shared_dict, atom_vocab=atom_vocab, bond_vocab=bond_vocab, args=args)\n",
    "train_data_motif_dl = DataLoader(train_data,\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=12,\n",
    "                           sampler=train_sampler,\n",
    "                           collate_fn=motif_collator)\n",
    "test_data_motif_dl = DataLoader(test_data,\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=10,\n",
    "                          sampler=test_sampler,\n",
    "                          collate_fn=motif_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "df5cf786-6595-48d9-860d-2edf0209d7f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/grover/grover/data/groverdataset.py\", line 158, in __getitem__\n    return self.data[dp_idx][real_idx]\n  File \"/tmp/ipykernel_200/3388227146.py\", line 49, in __getitem__\n    assert self.datapoints is not None\nAssertionError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_200/3091809959.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_motif_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"graph_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmoltree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"moltree\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/grover/grover/data/groverdataset.py\", line 158, in __getitem__\n    return self.data[dp_idx][real_idx]\n  File \"/tmp/ipykernel_200/3388227146.py\", line 49, in __getitem__\n    assert self.datapoints is not None\nAssertionError\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_data_motif_dl):\n",
    "    batch_graph = item[\"graph_input\"]\n",
    "    targets = item[\"targets\"]\n",
    "    moltree = item[\"moltree\"]\n",
    "    \n",
    "    if next(embed_model.parameters()).is_cuda:\n",
    "        targets[\"av_task\"] = targets[\"av_task\"].cuda()\n",
    "        targets[\"bv_task\"] = targets[\"bv_task\"].cuda()\n",
    "        targets[\"fg_task\"] = targets[\"fg_task\"].cuda()\n",
    "    #preds = embed_model(batch_graph)\n",
    "    #emb = preds['emb_vec']\n",
    "    #_, motif_loss, _ = motif_model(emb)\n",
    "    if i == 0 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d9095d13-4284-4658-aefd-e002a1dbd6bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCC'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moltree[0].nodes[0].smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "241c14c6-71d9-4316-b33d-44f4c5c3faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_data_dl):\n",
    "    batch_graph_test = item[\"graph_input\"]\n",
    "    targets_test = item[\"targets\"]\n",
    "    moltree_test = item[\"moltree\"]\n",
    "\n",
    "    if next(embed_model.parameters()).is_cuda:\n",
    "        targets_test[\"av_task\"] = targets_test[\"av_task\"].cuda()\n",
    "        targets_test[\"bv_task\"] = targets_test[\"bv_task\"].cuda()\n",
    "        targets_test[\"fg_task\"] = targets_test[\"fg_task\"].cuda()\n",
    "    preds_test = embed_model_test(batch_graph)\n",
    "    emb_test = preds_test['emb_vec']\n",
    "    #_, motif_loss, _ = motif_model(emb)\n",
    "    if i == 0 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d8919eca-a79a-4820-b0ac-680dee766370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom_from_atom': tensor([[ 1.1859,  0.1740, -0.9452,  ...,  1.4035,  0.3065,  1.4557],\n",
       "         [ 1.3639,  0.3965, -1.2151,  ...,  1.1539,  0.4090,  1.6906],\n",
       "         [ 1.1003,  1.0086, -1.0413,  ...,  0.5360,  0.8957,  1.4942],\n",
       "         ...,\n",
       "         [ 1.4797,  0.4890, -1.2851,  ...,  0.3559,  0.5014,  1.6542],\n",
       "         [ 1.4237,  0.5486, -1.3769,  ...,  0.3725,  0.4631,  1.7000],\n",
       "         [ 1.0127,  0.6267, -1.4408,  ...,  0.7031,  0.3877,  1.7992]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " 'bond_from_atom': tensor([[ 1.9872,  0.0787,  3.0122,  ..., -0.4602,  1.9317, -0.2936],\n",
       "         [ 1.7526, -0.0993,  2.8577,  ..., -0.3381,  1.6139, -0.0265],\n",
       "         [ 0.8892,  0.1347,  2.2546,  ...,  0.2686,  1.3640,  1.0550],\n",
       "         ...,\n",
       "         [ 1.0481, -0.2610,  2.6476,  ...,  0.0519,  1.5092,  0.8602],\n",
       "         [ 1.0814, -0.2726,  2.7533,  ...,  0.0844,  1.5442,  0.8107],\n",
       "         [ 1.3965, -0.5333,  2.4764,  ...,  0.0368,  1.5384,  0.8662]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " 'atom_from_bond': tensor([[ 0.2299,  0.7989,  0.3799,  ...,  0.2771,  0.2403,  0.2997],\n",
       "         [ 0.0176,  0.2336,  1.0223,  ...,  0.3107,  0.2858, -0.4031],\n",
       "         [-0.0868, -0.9352,  0.7137,  ..., -0.1492,  0.5033, -1.0589],\n",
       "         ...,\n",
       "         [-0.6195,  0.0789,  0.7504,  ...,  0.4629,  0.2154, -0.6188],\n",
       "         [-0.5954,  0.0320,  0.6553,  ...,  0.3703,  0.3197, -0.6114],\n",
       "         [-0.8172,  0.0421,  0.3967,  ...,  0.1537,  0.1022, -0.7344]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " 'bond_from_bond': tensor([[-7.9640e-01,  5.7121e-01,  1.0169e+00,  ..., -1.0146e-03,\n",
       "          -1.7365e-01, -5.4115e-01],\n",
       "         [-8.0784e-01,  6.2397e-01,  1.0356e+00,  ...,  1.4983e-02,\n",
       "          -1.8149e-01, -5.4238e-01],\n",
       "         [-3.2942e-01,  4.1108e-01,  1.1853e+00,  ...,  6.1108e-01,\n",
       "           3.5798e-01, -9.1229e-01],\n",
       "         ...,\n",
       "         [-2.9431e-01,  5.0177e-01,  1.1908e+00,  ...,  4.9046e-01,\n",
       "          -2.3342e-03, -9.9548e-01],\n",
       "         [-1.7648e-01,  4.9207e-01,  1.1199e+00,  ...,  4.4203e-01,\n",
       "          -4.9009e-02, -1.0362e+00],\n",
       "         [-2.6604e-01, -1.5016e-01,  1.0254e+00,  ...,  5.2851e-01,\n",
       "          -5.2222e-01, -3.0526e-01]], device='cuda:0',\n",
       "        grad_fn=<NativeLayerNormBackward0>)}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cfe21260-b788-465f-947e-7c2e0d516e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom_from_atom': tensor([[ 1.1859,  0.1740, -0.9452,  ...,  1.4035,  0.3065,  1.4557],\n",
       "         [ 1.2336,  0.4660, -1.1345,  ...,  1.1749,  0.5299,  1.5460],\n",
       "         [ 0.7674,  0.3911, -1.2583,  ...,  0.8974,  0.8591,  1.3976],\n",
       "         ...,\n",
       "         [ 0.3979,  0.6365, -2.1722,  ...,  0.5994,  0.5248,  1.4194],\n",
       "         [ 0.6119,  1.2031, -1.9464,  ...,  0.2268,  0.8142,  1.4497],\n",
       "         [ 0.3979,  0.6365, -2.1722,  ...,  0.5994,  0.5248,  1.4194]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " 'bond_from_atom': tensor([[ 1.9872,  0.0787,  3.0122,  ..., -0.4602,  1.9317, -0.2936],\n",
       "         [ 1.7658, -0.0789,  2.8364,  ..., -0.3204,  1.5859, -0.0390],\n",
       "         [ 1.0875,  0.2264,  2.3448,  ...,  0.2457,  1.4572,  0.8970],\n",
       "         ...,\n",
       "         [ 0.5334,  0.7396,  1.5271,  ...,  0.1219,  1.0316,  1.1022],\n",
       "         [ 0.5334,  0.7396,  1.5271,  ...,  0.1219,  1.0316,  1.1022],\n",
       "         [ 0.7041,  0.5886,  1.4270,  ...,  0.0203,  1.0522,  1.4338]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " 'atom_from_bond': tensor([[ 0.2299,  0.7989,  0.3799,  ...,  0.2771,  0.2403,  0.2997],\n",
       "         [ 0.1386,  0.2712,  0.8944,  ...,  0.4156,  0.4466, -0.5719],\n",
       "         [ 0.3402, -1.5440,  0.1417,  ..., -0.5024, -0.0780, -0.4001],\n",
       "         ...,\n",
       "         [-0.7372, -0.4672,  0.4571,  ...,  0.0114, -0.4689, -0.7872],\n",
       "         [-0.5476, -0.0720,  1.0307,  ...,  0.2603,  0.3158, -0.8319],\n",
       "         [-0.7372, -0.4672,  0.4571,  ...,  0.0114, -0.4689, -0.7872]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " 'bond_from_bond': tensor([[-7.9640e-01,  5.7121e-01,  1.0169e+00,  ..., -1.0146e-03,\n",
       "          -1.7365e-01, -5.4115e-01],\n",
       "         [-8.0784e-01,  6.2397e-01,  1.0356e+00,  ...,  1.4983e-02,\n",
       "          -1.8149e-01, -5.4238e-01],\n",
       "         [-8.4840e-01,  4.8810e-01,  1.2450e+00,  ...,  2.1720e-01,\n",
       "          -3.4826e-01, -5.8301e-01],\n",
       "         ...,\n",
       "         [-2.9804e-01,  3.9682e-01,  1.1630e+00,  ...,  2.9090e-01,\n",
       "           3.2020e-01, -1.0860e+00],\n",
       "         [-2.9804e-01,  3.9682e-01,  1.1630e+00,  ...,  2.9090e-01,\n",
       "           3.2020e-01, -1.0860e+00],\n",
       "         [-9.4956e-01,  4.6118e-01,  1.2720e+00,  ...,  1.7149e-01,\n",
       "          -3.4464e-01, -7.3250e-01]], device='cuda:0',\n",
       "        grad_fn=<NativeLayerNormBackward0>)}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a9f95791-c35c-4797-9bb1-082eb7d35075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[   0,    0,    0,    0],\n",
       "         [   2,    0,    0,    0],\n",
       "         [   1,    4,    0,    0],\n",
       "         ...,\n",
       "         [5241, 5244,    0,    0],\n",
       "         [5243, 5246,    0,    0],\n",
       "         [5237, 5245,    0,    0]]),\n",
       " tensor([   0,    1,    2,  ..., 2424, 2424, 2425]),\n",
       " tensor([   0,    2,    1,  ..., 5243, 5246, 5245]),\n",
       " tensor([[   1,   20],\n",
       "         [  21,   27],\n",
       "         [  48,   32],\n",
       "         [  80,   21],\n",
       "         [ 101,   21],\n",
       "         [ 122,   27],\n",
       "         [ 149,   16],\n",
       "         [ 165,   23],\n",
       "         [ 188,   27],\n",
       "         [ 215,   23],\n",
       "         [ 238,   20],\n",
       "         [ 258,   27],\n",
       "         [ 285,   21],\n",
       "         [ 306,   23],\n",
       "         [ 329,   25],\n",
       "         [ 354,   16],\n",
       "         [ 370,   20],\n",
       "         [ 390,   20],\n",
       "         [ 410,   30],\n",
       "         [ 440,   23],\n",
       "         [ 463,   23],\n",
       "         [ 486,   21],\n",
       "         [ 507,   26],\n",
       "         [ 533,   24],\n",
       "         [ 557,   22],\n",
       "         [ 579,   18],\n",
       "         [ 597,   12],\n",
       "         [ 609,   27],\n",
       "         [ 636,   32],\n",
       "         [ 668,   24],\n",
       "         [ 692,   30],\n",
       "         [ 722,   27],\n",
       "         [ 749,   29],\n",
       "         [ 778,   24],\n",
       "         [ 802,   34],\n",
       "         [ 836,   29],\n",
       "         [ 865,   27],\n",
       "         [ 892,   29],\n",
       "         [ 921,   21],\n",
       "         [ 942,   21],\n",
       "         [ 963,   25],\n",
       "         [ 988,   29],\n",
       "         [1017,   25],\n",
       "         [1042,   28],\n",
       "         [1070,   24],\n",
       "         [1094,   22],\n",
       "         [1116,   25],\n",
       "         [1141,   20],\n",
       "         [1161,   27],\n",
       "         [1188,   20],\n",
       "         [1208,   27],\n",
       "         [1235,   25],\n",
       "         [1260,   33],\n",
       "         [1293,   29],\n",
       "         [1322,   26],\n",
       "         [1348,   29],\n",
       "         [1377,   21],\n",
       "         [1398,   25],\n",
       "         [1423,   31],\n",
       "         [1454,   23],\n",
       "         [1477,   27],\n",
       "         [1504,   25],\n",
       "         [1529,   21],\n",
       "         [1550,   22],\n",
       "         [1572,   25],\n",
       "         [1597,   31],\n",
       "         [1628,   21],\n",
       "         [1649,   24],\n",
       "         [1673,   22],\n",
       "         [1695,   22],\n",
       "         [1717,   31],\n",
       "         [1748,   26],\n",
       "         [1774,   23],\n",
       "         [1797,   19],\n",
       "         [1816,   17],\n",
       "         [1833,   24],\n",
       "         [1857,   33],\n",
       "         [1890,   26],\n",
       "         [1916,   25],\n",
       "         [1941,   29],\n",
       "         [1970,   25],\n",
       "         [1995,   27],\n",
       "         [2022,   12],\n",
       "         [2034,   20],\n",
       "         [2054,   23],\n",
       "         [2077,   27],\n",
       "         [2104,   21],\n",
       "         [2125,   24],\n",
       "         [2149,   31],\n",
       "         [2180,   33],\n",
       "         [2213,   19],\n",
       "         [2232,   29],\n",
       "         [2261,   18],\n",
       "         [2279,   24],\n",
       "         [2303,   17],\n",
       "         [2320,   27],\n",
       "         [2347,   20],\n",
       "         [2367,   19],\n",
       "         [2386,   21],\n",
       "         [2407,   19]]),\n",
       " tensor([[   1,   42],\n",
       "         [  43,   60],\n",
       "         [ 103,   70],\n",
       "         [ 173,   44],\n",
       "         [ 217,   48],\n",
       "         [ 265,   58],\n",
       "         [ 323,   34],\n",
       "         [ 357,   48],\n",
       "         [ 405,   58],\n",
       "         [ 463,   50],\n",
       "         [ 513,   44],\n",
       "         [ 557,   56],\n",
       "         [ 613,   44],\n",
       "         [ 657,   50],\n",
       "         [ 707,   54],\n",
       "         [ 761,   34],\n",
       "         [ 795,   42],\n",
       "         [ 837,   42],\n",
       "         [ 879,   66],\n",
       "         [ 945,   48],\n",
       "         [ 993,   48],\n",
       "         [1041,   44],\n",
       "         [1085,   56],\n",
       "         [1141,   52],\n",
       "         [1193,   46],\n",
       "         [1239,   38],\n",
       "         [1277,   22],\n",
       "         [1299,   58],\n",
       "         [1357,   68],\n",
       "         [1425,   52],\n",
       "         [1477,   66],\n",
       "         [1543,   60],\n",
       "         [1603,   62],\n",
       "         [1665,   52],\n",
       "         [1717,   76],\n",
       "         [1793,   64],\n",
       "         [1857,   58],\n",
       "         [1915,   62],\n",
       "         [1977,   44],\n",
       "         [2021,   42],\n",
       "         [2063,   56],\n",
       "         [2119,   64],\n",
       "         [2183,   54],\n",
       "         [2237,   60],\n",
       "         [2297,   54],\n",
       "         [2351,   48],\n",
       "         [2399,   54],\n",
       "         [2453,   42],\n",
       "         [2495,   60],\n",
       "         [2555,   46],\n",
       "         [2601,   62],\n",
       "         [2663,   54],\n",
       "         [2717,   74],\n",
       "         [2791,   64],\n",
       "         [2855,   56],\n",
       "         [2911,   64],\n",
       "         [2975,   44],\n",
       "         [3019,   52],\n",
       "         [3071,   68],\n",
       "         [3139,   48],\n",
       "         [3187,   60],\n",
       "         [3247,   54],\n",
       "         [3301,   46],\n",
       "         [3347,   50],\n",
       "         [3397,   54],\n",
       "         [3451,   68],\n",
       "         [3519,   44],\n",
       "         [3563,   54],\n",
       "         [3617,   48],\n",
       "         [3665,   46],\n",
       "         [3711,   70],\n",
       "         [3781,   56],\n",
       "         [3837,   52],\n",
       "         [3889,   42],\n",
       "         [3931,   36],\n",
       "         [3967,   54],\n",
       "         [4021,   72],\n",
       "         [4093,   56],\n",
       "         [4149,   56],\n",
       "         [4205,   64],\n",
       "         [4269,   54],\n",
       "         [4323,   60],\n",
       "         [4383,   24],\n",
       "         [4407,   42],\n",
       "         [4449,   50],\n",
       "         [4499,   60],\n",
       "         [4559,   44],\n",
       "         [4603,   50],\n",
       "         [4653,   68],\n",
       "         [4721,   74],\n",
       "         [4795,   40],\n",
       "         [4835,   62],\n",
       "         [4897,   38],\n",
       "         [4935,   50],\n",
       "         [4985,   36],\n",
       "         [5021,   60],\n",
       "         [5081,   42],\n",
       "         [5123,   40],\n",
       "         [5163,   46],\n",
       "         [5209,   38]]),\n",
       " tensor([[   0,    0,    0,    0],\n",
       "         [   2,    0,    0,    0],\n",
       "         [   1,    3,    0,    0],\n",
       "         ...,\n",
       "         [2422, 2424,    0,    0],\n",
       "         [2423, 2425,    0,    0],\n",
       "         [2420, 2424,    0,    0]]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f3e791b-b405-4efb-9f3c-5faa402d5c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[   0,    0,    0,    0],\n",
       "         [   2,    0,    0,    0],\n",
       "         [   1,    4,    0,    0],\n",
       "         ...,\n",
       "         [5021, 5031, 5039,    0],\n",
       "         [5005, 5042,    0,    0],\n",
       "         [4997, 5041,    0,    0]]),\n",
       " tensor([   0,    1,    2,  ..., 2339, 2340, 2341]),\n",
       " tensor([   0,    2,    1,  ..., 5039, 5042, 5041]),\n",
       " tensor([[   1,   27],\n",
       "         [  28,   26],\n",
       "         [  54,   21],\n",
       "         [  75,   24],\n",
       "         [  99,   29],\n",
       "         [ 128,   18],\n",
       "         [ 146,   18],\n",
       "         [ 164,   20],\n",
       "         [ 184,   21],\n",
       "         [ 205,   21],\n",
       "         [ 226,   31],\n",
       "         [ 257,   23],\n",
       "         [ 280,   20],\n",
       "         [ 300,   18],\n",
       "         [ 318,   27],\n",
       "         [ 345,   27],\n",
       "         [ 372,   25],\n",
       "         [ 397,   23],\n",
       "         [ 420,   18],\n",
       "         [ 438,   29],\n",
       "         [ 467,   25],\n",
       "         [ 492,   25],\n",
       "         [ 517,   24],\n",
       "         [ 541,   24],\n",
       "         [ 565,   21],\n",
       "         [ 586,   18],\n",
       "         [ 604,   28],\n",
       "         [ 632,   29],\n",
       "         [ 661,   21],\n",
       "         [ 682,   18],\n",
       "         [ 700,   23],\n",
       "         [ 723,   20],\n",
       "         [ 743,   14],\n",
       "         [ 757,   27],\n",
       "         [ 784,   18],\n",
       "         [ 802,   23],\n",
       "         [ 825,   30],\n",
       "         [ 855,   29],\n",
       "         [ 884,   18],\n",
       "         [ 902,   18],\n",
       "         [ 920,   18],\n",
       "         [ 938,   31],\n",
       "         [ 969,   27],\n",
       "         [ 996,   15],\n",
       "         [1011,   26],\n",
       "         [1037,   32],\n",
       "         [1069,   27],\n",
       "         [1096,   24],\n",
       "         [1120,   27],\n",
       "         [1147,   26],\n",
       "         [1173,   30],\n",
       "         [1203,   27],\n",
       "         [1230,   26],\n",
       "         [1256,   22],\n",
       "         [1278,   20],\n",
       "         [1298,   15],\n",
       "         [1313,   27],\n",
       "         [1340,   31],\n",
       "         [1371,   33],\n",
       "         [1404,   24],\n",
       "         [1428,   23],\n",
       "         [1451,   22],\n",
       "         [1473,   17],\n",
       "         [1490,   19],\n",
       "         [1509,   24],\n",
       "         [1533,   29],\n",
       "         [1562,   18],\n",
       "         [1580,   15],\n",
       "         [1595,   22],\n",
       "         [1617,   23],\n",
       "         [1640,   28],\n",
       "         [1668,   25],\n",
       "         [1693,   17],\n",
       "         [1710,   21],\n",
       "         [1731,   27],\n",
       "         [1758,   20],\n",
       "         [1778,   22],\n",
       "         [1800,   18],\n",
       "         [1818,   27],\n",
       "         [1845,   29],\n",
       "         [1874,   21],\n",
       "         [1895,   27],\n",
       "         [1922,   25],\n",
       "         [1947,   21],\n",
       "         [1968,   17],\n",
       "         [1985,   27],\n",
       "         [2012,   21],\n",
       "         [2033,   24],\n",
       "         [2057,   20],\n",
       "         [2077,   24],\n",
       "         [2101,   34],\n",
       "         [2135,   17],\n",
       "         [2152,   19],\n",
       "         [2171,   27],\n",
       "         [2198,   27],\n",
       "         [2225,   20],\n",
       "         [2245,   22],\n",
       "         [2267,   29],\n",
       "         [2296,   22],\n",
       "         [2318,   24]]),\n",
       " tensor([[   1,   60],\n",
       "         [  61,   56],\n",
       "         [ 117,   42],\n",
       "         [ 159,   50],\n",
       "         [ 209,   64],\n",
       "         [ 273,   36],\n",
       "         [ 309,   38],\n",
       "         [ 347,   42],\n",
       "         [ 389,   44],\n",
       "         [ 433,   44],\n",
       "         [ 477,   70],\n",
       "         [ 547,   48],\n",
       "         [ 595,   44],\n",
       "         [ 639,   38],\n",
       "         [ 677,   60],\n",
       "         [ 737,   60],\n",
       "         [ 797,   54],\n",
       "         [ 851,   48],\n",
       "         [ 899,   38],\n",
       "         [ 937,   64],\n",
       "         [1001,   54],\n",
       "         [1055,   56],\n",
       "         [1111,   52],\n",
       "         [1163,   52],\n",
       "         [1215,   44],\n",
       "         [1259,   38],\n",
       "         [1297,   60],\n",
       "         [1357,   62],\n",
       "         [1419,   44],\n",
       "         [1463,   40],\n",
       "         [1503,   48],\n",
       "         [1551,   44],\n",
       "         [1595,   28],\n",
       "         [1623,   62],\n",
       "         [1685,   38],\n",
       "         [1723,   50],\n",
       "         [1773,   68],\n",
       "         [1841,   66],\n",
       "         [1907,   36],\n",
       "         [1943,   38],\n",
       "         [1981,   38],\n",
       "         [2019,   68],\n",
       "         [2087,   58],\n",
       "         [2145,   30],\n",
       "         [2175,   56],\n",
       "         [2231,   68],\n",
       "         [2299,   58],\n",
       "         [2357,   52],\n",
       "         [2409,   58],\n",
       "         [2467,   56],\n",
       "         [2523,   66],\n",
       "         [2589,   60],\n",
       "         [2649,   56],\n",
       "         [2705,   48],\n",
       "         [2753,   42],\n",
       "         [2795,   32],\n",
       "         [2827,   58],\n",
       "         [2885,   68],\n",
       "         [2953,   74],\n",
       "         [3027,   50],\n",
       "         [3077,   48],\n",
       "         [3125,   46],\n",
       "         [3171,   36],\n",
       "         [3207,   40],\n",
       "         [3247,   54],\n",
       "         [3301,   64],\n",
       "         [3365,   38],\n",
       "         [3403,   30],\n",
       "         [3433,   46],\n",
       "         [3479,   48],\n",
       "         [3527,   60],\n",
       "         [3587,   56],\n",
       "         [3643,   36],\n",
       "         [3679,   46],\n",
       "         [3725,   60],\n",
       "         [3785,   46],\n",
       "         [3831,   46],\n",
       "         [3877,   38],\n",
       "         [3915,   58],\n",
       "         [3973,   64],\n",
       "         [4037,   44],\n",
       "         [4081,   60],\n",
       "         [4141,   54],\n",
       "         [4195,   44],\n",
       "         [4239,   36],\n",
       "         [4275,   60],\n",
       "         [4335,   46],\n",
       "         [4381,   52],\n",
       "         [4433,   42],\n",
       "         [4475,   52],\n",
       "         [4527,   76],\n",
       "         [4603,   36],\n",
       "         [4639,   38],\n",
       "         [4677,   58],\n",
       "         [4735,   58],\n",
       "         [4793,   42],\n",
       "         [4835,   48],\n",
       "         [4883,   62],\n",
       "         [4945,   46],\n",
       "         [4991,   52]]),\n",
       " tensor([[   0,    0,    0,    0],\n",
       "         [   2,    0,    0,    0],\n",
       "         [   1,    3,    0,    0],\n",
       "         ...,\n",
       "         [2330, 2334, 2338,    0],\n",
       "         [2323, 2341,    0,    0],\n",
       "         [2320, 2340,    0,    0]]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "aa9bbbf4-9939-44c0-a84c-764759f197a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f5096622f30>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moltree[0].mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b9c50160-2ce4-4f8b-bcc6-3ff75b2c33c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f5095c904b0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moltree_test[0].mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "59ef633f-49f9-455b-bdc0-24813ff55d27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC(=O)Nc1nc2ccc(NC(=O)NCc3ccccc3)cc2s1\n",
      "C\n",
      "C1=CC=CC=C1\n",
      "C\n",
      "N\n",
      "C\n",
      "N\n",
      "N\n",
      "C\n",
      "C1=CC=C2SC=NC2=C1\n",
      "O\n",
      "O\n",
      "Cc1cccc(C)c1NC(=O)C[NH+]1CCC(OCc2ccc(F)cc2)CC1\n",
      "C\n",
      "C1CC[NH2+]CC1\n",
      "C1=CC=CC=C1\n",
      "C\n",
      "N\n",
      "O\n",
      "C\n",
      "C\n",
      "C1=CC=CC=C1\n",
      "C\n",
      "F\n",
      "O\n",
      "Cc1occc1C(=O)/C(C#N)=C\\c1cccc(C(F)(F)F)c1\n",
      "C\n",
      "C#N\n",
      "C1=CC=CC=C1\n",
      "C\n",
      "C\n",
      "C1=COC=C1\n",
      "C\n",
      "C\n",
      "O\n",
      "F\n",
      "F\n",
      "F\n",
      "Cc1cccn2c(=O)c(C(=O)NC[C@H]3CCO[C@@H]3C(C)C)cnc12\n",
      "C\n",
      "C1CCOC1\n",
      "C\n",
      "N\n",
      "C\n",
      "C\n",
      "C1=CC2=NC=CCN2C=C1\n",
      "O\n",
      "O\n",
      "C\n",
      "C\n",
      "COc1ccc(OC)c(/C=C2\\Oc3cc(OC(=O)c4ccncc4)cc(C)c3C2=O)c1\n",
      "CO\n",
      "C1=CC=CC=C1\n",
      "CO\n",
      "C1=CC=C2OCCC2=C1\n",
      "C1=CC=NC=C1\n",
      "C\n",
      "O\n",
      "C\n",
      "C\n",
      "O\n",
      "O\n",
      "Cc1ccc(-c2cc(NC(=O)C(C)C)c(=O)n(CC(=O)Nc3cccc(C)c3)n2)cc1\n",
      "C\n",
      "C1=CC=CC=C1\n",
      "C\n",
      "N\n",
      "C\n",
      "N\n",
      "C1=CC(C2=CC=CC=C2)=NNC1\n",
      "O\n",
      "C\n",
      "C\n",
      "O\n",
      "C\n",
      "C\n",
      "C\n",
      "O\n",
      "CC(=O)NCCC(=O)N1CCC[C@@H](C)C1\n",
      "C\n",
      "C1CCNCC1\n",
      "C\n",
      "N\n",
      "C\n",
      "C\n",
      "CC\n",
      "O\n",
      "O\n",
      "CCC[NH2+]C1CCC(O)(Cc2nc(C)cs2)CC1\n",
      "CCC\n",
      "C1CCCCC1\n",
      "CC1=NC=CS1\n",
      "[NH4+]\n",
      "O\n",
      "C\n",
      "CCn1cc(C(=O)N[C@H]2CC(=O)N(C)C2)c(C(C)C)n1\n",
      "CC\n",
      "C1=CNN=C1\n",
      "C1CCNC1\n",
      "C\n",
      "N\n",
      "O\n",
      "C\n",
      "C\n",
      "O\n",
      "C\n",
      "C\n",
      "Cc1c(F)cc(N)cc1S(=O)(=O)N[C@@H](C)C1CC1\n",
      "C\n",
      "C1CC1\n",
      "C\n",
      "N\n",
      "S\n",
      "C1=CC=CC=C1\n",
      "F\n",
      "N\n",
      "O\n",
      "O\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(moltree[i].smiles)\n",
    "    for j in range(len(moltree[i].nodes)):\n",
    "        print(moltree[i].nodes[j].smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9bd26180-7643-4276-aca7-4bbbb962c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_batch_nodeID(moltree, motif_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "02e26c4b-afd0-49b2-a867-9115aae39fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(moltree[1].mol.GetBonds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "298f528e-38a6-42e2-9fb9-2f8a02f60f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[5][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "2a6327b5-af83-4da7-b94f-d8839db754fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([233, 100])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['atom_from_atom'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "93d7d780-d094-40f5-b173-5fd3bef7ae74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([233, 100])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['atom_from_bond'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0fd176f7-db61-400a-b0e9-4c65c12edfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,  46],\n",
       "        [ 47,  68],\n",
       "        [115,  54],\n",
       "        [169,  44],\n",
       "        [213,  38],\n",
       "        [251,  76],\n",
       "        [327,  44],\n",
       "        [371,  38],\n",
       "        [409,  60],\n",
       "        [469,  32]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "60d2c100-c2e8-413d-a2d4-79f26aca06db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.7526, -0.0993,  2.8577,  ..., -0.3381,  1.6139, -0.0265],\n",
       "         [ 0.8892,  0.1347,  2.2546,  ...,  0.2686,  1.3640,  1.0550],\n",
       "         [ 0.9989,  0.0948,  2.0923,  ...,  0.0882,  1.2366,  0.7802],\n",
       "         ...,\n",
       "         [ 0.1021,  0.1924,  1.4996,  ...,  0.6037,  0.2489,  1.1135],\n",
       "         [ 0.7596,  0.0699,  1.5142,  ...,  0.7303,  0.1724,  1.0258],\n",
       "         [ 1.6360, -0.0166,  2.9026,  ..., -0.2519,  1.5676,  0.0445]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6737e+00, -2.5817e-03,  2.9433e+00,  ..., -2.7101e-01,\n",
       "           1.5551e+00,  8.2253e-02],\n",
       "         [ 6.8865e-01,  2.6351e-01,  1.5407e+00,  ...,  2.8127e-01,\n",
       "           7.3498e-01,  7.7785e-01],\n",
       "         [-1.1575e-01,  5.9401e-01,  1.4826e+00,  ...,  1.5004e-01,\n",
       "           8.0959e-01,  8.8538e-01],\n",
       "         ...,\n",
       "         [ 6.6971e-01,  4.9209e-01,  2.2563e+00,  ..., -4.9288e-01,\n",
       "           1.0688e+00,  2.5702e-01],\n",
       "         [ 3.4176e-01,  6.9647e-01,  1.9208e+00,  ...,  8.3852e-01,\n",
       "           2.4630e-01,  1.1235e+00],\n",
       "         [ 9.0764e-01,  5.1507e-01,  2.6571e+00,  ..., -9.4226e-01,\n",
       "           1.7855e+00,  1.2692e-01]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5618,  0.0197,  2.8233,  ..., -0.3092,  1.5546,  0.0092],\n",
       "         [ 0.4716,  0.7574,  2.2294,  ..., -0.6507,  1.2586,  0.4990],\n",
       "         [ 0.1336,  0.7363,  2.1116,  ..., -0.6494,  1.2172,  0.6843],\n",
       "         ...,\n",
       "         [ 0.1336,  0.7363,  2.1116,  ..., -0.6494,  1.2172,  0.6843],\n",
       "         [ 0.4574,  0.7311,  2.1708,  ..., -0.6534,  1.2173,  0.5006],\n",
       "         [ 1.5618,  0.0197,  2.8233,  ..., -0.3092,  1.5546,  0.0092]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7526, -0.0993,  2.8577,  ..., -0.3381,  1.6139, -0.0265],\n",
       "         [ 0.8118,  0.1227,  2.1385,  ...,  0.2262,  1.3604,  0.9814],\n",
       "         [ 1.0685,  0.1136,  2.1447,  ...,  0.0588,  1.2571,  0.8321],\n",
       "         ...,\n",
       "         [ 1.7185,  0.1068,  2.7049,  ..., -0.5731,  2.1825,  0.0679],\n",
       "         [ 1.3878, -0.5136,  2.5292,  ...,  0.0247,  1.5306,  0.6968],\n",
       "         [ 1.2889, -0.5866,  2.6461,  ..., -0.2964,  1.9218,  0.8645]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.3126,  0.5499,  2.5298,  ..., -1.0963,  2.3851, -0.5344],\n",
       "         [ 1.0281,  0.2989,  2.2750,  ..., -0.1686,  1.6129,  0.2447],\n",
       "         [ 0.3504,  0.6784,  1.7711,  ..., -1.4386,  2.2013, -0.3411],\n",
       "         ...,\n",
       "         [ 1.6849, -0.1975,  2.4657,  ...,  0.4297,  0.9744,  0.6830],\n",
       "         [ 1.9869, -0.2578,  2.4463,  ...,  0.1258,  0.9192,  0.9646],\n",
       "         [ 0.7842, -0.3514,  1.4647,  ...,  1.3712,  0.9298,  1.2854]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4617,  0.7960,  2.3061,  ..., -0.6474,  1.3033,  0.5667],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [ 1.6813,  0.0334,  2.8714,  ..., -0.2632,  1.5018,  0.0813],\n",
       "         [ 1.7901, -0.1598,  2.5121,  ...,  0.1322,  0.9640,  0.8867],\n",
       "         [ 1.6835, -0.0065,  2.4886,  ...,  0.0539,  0.9930,  0.7464]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8335, -0.1009,  2.9017,  ..., -0.3457,  1.5822,  0.0077],\n",
       "         [ 1.2384,  0.3016,  2.1183,  ...,  0.2557,  2.1905,  0.7496],\n",
       "         [ 0.9134,  0.1885,  2.0622,  ...,  0.1437,  1.1234,  0.8531],\n",
       "         ...,\n",
       "         [ 0.4036,  0.5034,  1.3266,  ...,  0.2239,  0.4795,  0.4475],\n",
       "         [ 0.6253,  0.4145,  1.3137,  ...,  0.7035,  0.0750,  0.7425],\n",
       "         [ 1.1301,  0.3856,  2.2622,  ..., -0.3125,  1.3115, -0.0135]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6940, -0.0107,  2.8052,  ..., -0.3307,  1.5912,  0.0250],\n",
       "         [-0.3489,  0.3839,  0.9793,  ..., -0.1816,  1.1672,  1.4834],\n",
       "         [-0.3489,  0.3839,  0.9793,  ..., -0.1816,  1.1672,  1.4834],\n",
       "         ...,\n",
       "         [ 1.8787, -0.2670,  2.4401,  ...,  0.0320,  0.8374,  0.8973],\n",
       "         [ 1.7262, -0.0716,  2.5035,  ...,  0.0299,  1.0003,  0.8050],\n",
       "         [ 1.8982,  0.0750,  2.4845,  ...,  0.0333,  0.7289,  0.8557]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4617,  0.7960,  2.3061,  ..., -0.6474,  1.3033,  0.5667],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [ 1.3914, -0.4932,  2.5103,  ..., -0.0127,  1.4398,  0.6104],\n",
       "         [ 1.7744, -0.1947,  2.4646,  ...,  0.0460,  1.0076,  0.8186],\n",
       "         [ 1.6836, -0.0065,  2.4887,  ...,  0.0539,  0.9930,  0.7464]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6360, -0.0166,  2.9026,  ..., -0.2519,  1.5676,  0.0445],\n",
       "         [ 0.8270,  0.1359,  1.4375,  ...,  0.7390,  0.1978,  1.1410],\n",
       "         [ 0.2615,  0.2869,  1.4471,  ...,  0.6347,  0.2898,  1.3414],\n",
       "         ...,\n",
       "         [ 1.3277,  0.1424,  2.9657,  ..., -0.3170,  2.2097,  0.2037],\n",
       "         [ 0.3777,  0.0334,  1.6567,  ...,  0.6374,  0.5263,  1.3670],\n",
       "         [ 0.5166,  0.2707,  1.6519,  ...,  1.3455, -0.0736,  1.1613]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6900,  0.0315,  2.9328,  ..., -0.2618,  1.5208,  0.0756],\n",
       "         [ 0.1210,  0.2469,  1.5302,  ...,  1.3329,  0.9927,  1.3002],\n",
       "         [-0.2958,  0.5028,  1.3904,  ...,  0.7249,  1.0067,  1.3666],\n",
       "         ...,\n",
       "         [ 1.0957, -0.1066,  2.2711,  ...,  0.9093,  1.5269,  1.0535],\n",
       "         [ 0.8943, -0.1435,  2.2330,  ...,  0.8574,  1.5763,  0.9599],\n",
       "         [ 0.9958, -0.1997,  2.4719,  ...,  0.9634,  1.2341,  0.8004]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5554,  0.0076,  2.8190,  ..., -0.3027,  1.5767,  0.0113],\n",
       "         [ 0.1943,  0.7510,  1.9554,  ..., -0.6131,  1.4357,  0.5264],\n",
       "         [ 0.3264,  0.6094,  2.0620,  ..., -0.5677,  1.3397,  0.7377],\n",
       "         ...,\n",
       "         [ 1.5729,  0.2733,  3.0636,  ..., -0.3351,  1.7743,  0.1203],\n",
       "         [-0.2447,  0.3394,  0.8179,  ...,  0.2989,  0.3148,  1.7042],\n",
       "         [ 1.5729,  0.2733,  3.0636,  ..., -0.3351,  1.7743,  0.1203]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.8831,  0.5270,  2.6141,  ..., -0.9740,  1.8759,  0.1144],\n",
       "         [ 1.0827,  0.3271,  1.9600,  ...,  0.7366,  0.7611,  1.1918],\n",
       "         [-0.2381,  1.1040,  1.2121,  ...,  0.0291,  0.3927,  1.1909],\n",
       "         ...,\n",
       "         [ 0.2060,  0.5162,  1.4667,  ...,  0.3408,  0.5614,  1.0001],\n",
       "         [ 0.9685,  0.2759,  1.6276,  ...,  0.7516,  0.2797,  1.2128],\n",
       "         [ 1.3926,  0.0281,  2.8022,  ..., -0.5945,  1.9394, -0.0854]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9118,  0.5078,  2.6384,  ..., -0.9614,  1.8446,  0.1425],\n",
       "         [ 0.7500,  0.2731,  1.7607,  ...,  0.9038,  0.8300,  0.5324],\n",
       "         [-0.6348,  1.0611,  1.0973,  ..., -0.0033,  0.3946,  0.7457],\n",
       "         ...,\n",
       "         [ 1.1900, -0.3491,  2.0697,  ...,  0.6153,  1.5115,  0.7827],\n",
       "         [ 1.0985, -0.3140,  2.1914,  ...,  0.8369,  1.6764,  0.7783],\n",
       "         [ 0.8556,  0.0310,  1.6295,  ...,  0.3337,  0.6641,  1.0281]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4390,  0.7761,  2.2429,  ..., -0.6517,  1.2624,  0.5860],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [ 1.3262,  0.1371,  2.9605,  ..., -0.3110,  2.2221,  0.2168],\n",
       "         [ 1.2889, -0.5866,  2.6461,  ..., -0.2964,  1.9218,  0.8645],\n",
       "         [ 1.3902, -0.5028,  2.4574,  ...,  0.0034,  1.5583,  0.7875]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7642, -0.0982,  2.8432,  ..., -0.3356,  1.6097, -0.0412],\n",
       "         [ 0.9293,  0.0917,  1.9097,  ..., -0.2506,  1.8672,  0.2854],\n",
       "         [ 0.8414,  0.1645,  2.0548,  ...,  0.1328,  1.2182,  0.7450],\n",
       "         ...,\n",
       "         [ 0.1449,  0.0726,  1.9443,  ...,  1.0037,  0.3093,  0.9347],\n",
       "         [ 0.5870, -0.0234,  1.7504,  ...,  1.0952,  0.0478,  1.2053],\n",
       "         [ 0.7757, -0.1009,  2.4579,  ...,  0.3770,  1.3405,  0.2072]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7762, -0.1048,  2.8551,  ..., -0.3446,  1.6340, -0.0448],\n",
       "         [ 1.1117, -0.0342,  1.9012,  ...,  0.1735,  1.9410,  0.4002],\n",
       "         [ 0.8605,  0.1810,  2.0925,  ...,  0.1276,  1.2076,  0.7424],\n",
       "         ...,\n",
       "         [ 0.9679,  0.3319,  2.1214,  ..., -0.2011,  2.0490,  0.7173],\n",
       "         [ 0.9174,  0.2345,  2.2626,  ...,  0.3905,  1.5207,  0.8805],\n",
       "         [ 1.8203,  0.1403,  2.7421,  ..., -0.5957,  2.1988,  0.0382]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5534,  0.0131,  2.8427,  ..., -0.3179,  1.5599, -0.0247],\n",
       "         [ 0.3837,  0.6003,  2.0493,  ..., -0.4699,  1.7444,  0.9602],\n",
       "         [ 0.5755,  0.6324,  2.0326,  ..., -0.5686,  1.4443,  0.7727],\n",
       "         ...,\n",
       "         [ 0.9440,  0.1203,  2.4003,  ...,  0.2233,  1.6787,  0.6065],\n",
       "         [ 1.8239,  0.0202,  2.4484,  ...,  0.1018,  0.8599,  0.9802],\n",
       "         [ 1.7824, -0.1127,  2.4988,  ...,  0.1063,  0.9748,  0.8871]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8389, -0.0833,  2.8819,  ..., -0.3653,  1.6418,  0.0051],\n",
       "         [ 0.2701,  0.6094,  2.0014,  ..., -0.4931,  1.5144,  0.7694],\n",
       "         [ 0.9082,  0.1907,  2.1231,  ...,  0.1166,  1.2729,  0.8662],\n",
       "         ...,\n",
       "         [ 0.7061,  0.2217,  2.4583,  ...,  0.3722,  1.3980,  0.7224],\n",
       "         [ 0.8886,  0.1267,  2.4414,  ...,  0.3590,  1.5711,  0.7156],\n",
       "         [ 0.7608,  0.4159,  2.5120,  ...,  0.4287,  1.4665,  0.8310]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6701, -0.0084,  2.8844,  ..., -0.2871,  1.5449,  0.0780],\n",
       "         [ 0.4417,  0.4036,  1.3609,  ...,  0.2132,  0.3649,  0.5583],\n",
       "         [ 0.2802,  0.2432,  1.3216,  ...,  0.6475,  0.1399,  1.2916],\n",
       "         ...,\n",
       "         [ 0.2287,  0.2100,  1.3479,  ...,  0.6470,  0.2360,  1.2692],\n",
       "         [ 0.6062,  0.3375,  1.4447,  ...,  0.8278,  0.2296,  0.8826],\n",
       "         [ 1.6340, -0.0161,  2.8446,  ..., -0.2601,  1.5557,  0.0371]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6444e+00, -6.0500e-03,  2.9355e+00,  ..., -2.5292e-01,\n",
       "           1.5778e+00,  6.4454e-02],\n",
       "         [ 8.8630e-01, -2.1855e-02,  1.3977e+00,  ...,  6.3143e-01,\n",
       "           2.9396e-01,  1.4058e+00],\n",
       "         [ 2.7146e-01,  1.1659e-01,  1.4233e+00,  ...,  5.5550e-01,\n",
       "           4.0835e-01,  1.4365e+00],\n",
       "         ...,\n",
       "         [ 1.6402e+00, -1.3851e-02,  2.7859e+00,  ..., -2.6513e-01,\n",
       "           1.5792e+00, -7.6628e-03],\n",
       "         [ 1.7080e-01,  1.5582e-03,  8.7898e-01,  ...,  1.0133e-01,\n",
       "           1.2533e+00,  1.0171e+00],\n",
       "         [ 1.6402e+00, -1.3851e-02,  2.7859e+00,  ..., -2.6513e-01,\n",
       "           1.5792e+00, -7.6628e-03]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6737, -0.0391,  2.8400,  ..., -0.3080,  1.4850, -0.0070],\n",
       "         [-0.6252,  0.6721,  1.1752,  ..., -0.1559,  1.0330,  0.9402],\n",
       "         [-0.1781,  0.3012,  1.2438,  ...,  0.4657,  0.8156,  1.2699],\n",
       "         ...,\n",
       "         [ 0.3922,  0.1094,  0.7938,  ...,  0.3306,  1.8335,  1.0166],\n",
       "         [ 0.4513, -0.1312,  1.0449,  ...,  0.8227,  1.4821,  1.0301],\n",
       "         [ 1.8230,  0.1522,  2.6606,  ..., -0.5130,  2.0931, -0.0085]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6444, -0.0061,  2.9355,  ..., -0.2529,  1.5778,  0.0645],\n",
       "         [ 0.9022, -0.0283,  1.3991,  ...,  0.6055,  0.2693,  1.3964],\n",
       "         [ 0.2861,  0.1113,  1.4310,  ...,  0.5227,  0.3819,  1.4253],\n",
       "         ...,\n",
       "         [ 1.7061, -0.1191,  2.3815,  ...,  0.1188,  0.9074,  0.8501],\n",
       "         [ 1.0542,  0.1968,  2.7786,  ..., -0.0373,  1.0477,  0.6835],\n",
       "         [ 1.0560,  0.1178,  2.4441,  ...,  0.0808,  1.0908,  0.8736]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6360, -0.0166,  2.9026,  ..., -0.2519,  1.5676,  0.0445],\n",
       "         [ 0.6459,  0.2777,  1.4661,  ...,  0.7071,  0.0988,  0.9009],\n",
       "         [-0.0055,  0.4273,  1.4418,  ...,  0.5736,  0.1465,  1.0682],\n",
       "         ...,\n",
       "         [ 1.0461,  0.4277,  2.2949,  ..., -0.3367,  1.2082, -0.1575],\n",
       "         [ 1.4945,  0.0323,  2.2168,  ..., -0.3433,  1.3419,  0.0952],\n",
       "         [ 0.7077,  0.6576,  1.3061,  ...,  0.4100,  0.2553,  0.7121]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7526e+00, -9.9303e-02,  2.8577e+00,  ..., -3.3814e-01,\n",
       "           1.6139e+00, -2.6480e-02],\n",
       "         [ 8.6249e-01,  6.7888e-02,  2.1889e+00,  ...,  2.6028e-01,\n",
       "           1.3691e+00,  8.5541e-01],\n",
       "         [ 9.6037e-01,  1.1056e-01,  2.1369e+00,  ...,  9.9380e-02,\n",
       "           1.2439e+00,  7.7811e-01],\n",
       "         ...,\n",
       "         [ 6.4482e-01, -1.6940e-03,  2.1636e+00,  ..., -5.6793e-01,\n",
       "           1.4858e+00,  6.5120e-01],\n",
       "         [ 8.6186e-01, -1.4600e-01,  2.5037e+00,  ..., -3.3848e-02,\n",
       "           1.3564e+00,  8.1208e-01],\n",
       "         [ 8.9630e-01, -1.1263e-02,  1.9932e+00,  ..., -7.0080e-01,\n",
       "           1.5494e+00,  4.2179e-01]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6444, -0.0061,  2.9355,  ..., -0.2529,  1.5778,  0.0645],\n",
       "         [ 0.8642, -0.1313,  1.3463,  ...,  0.6361,  0.2796,  1.3593],\n",
       "         [ 0.3898,  0.0686,  1.3862,  ...,  0.5516,  0.4082,  1.4973],\n",
       "         ...,\n",
       "         [ 0.9430, -0.0327,  2.0238,  ...,  0.3328,  1.7885,  1.0151],\n",
       "         [ 1.3261, -0.3847,  2.1021,  ...,  0.3708,  2.0960,  1.0314],\n",
       "         [ 1.3261, -0.3847,  2.1021,  ...,  0.3708,  2.0960,  1.0314]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7384, -0.0799,  2.8955,  ..., -0.3388,  1.6505, -0.0309],\n",
       "         [ 1.1112, -0.0125,  2.0279,  ...,  0.1787,  1.5884,  0.7239],\n",
       "         [ 0.8784,  0.2337,  2.0638,  ...,  0.1080,  1.2313,  0.7561],\n",
       "         ...,\n",
       "         [ 0.9789,  0.4606,  2.6041,  ..., -0.9915,  1.9713,  0.1206],\n",
       "         [-0.8696,  0.7989,  0.7676,  ...,  0.1713,  1.1054,  1.2216],\n",
       "         [ 1.2528,  0.6099,  2.3981,  ..., -1.0119,  2.0035,  0.3935]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5618e+00,  1.9737e-02,  2.8233e+00,  ..., -3.0917e-01,\n",
       "           1.5546e+00,  9.2241e-03],\n",
       "         [ 3.3948e-01,  7.3311e-01,  2.1360e+00,  ..., -6.2192e-01,\n",
       "           1.2026e+00,  4.9633e-01],\n",
       "         [ 1.3363e-01,  7.3630e-01,  2.1116e+00,  ..., -6.4943e-01,\n",
       "           1.2172e+00,  6.8429e-01],\n",
       "         ...,\n",
       "         [ 1.6640e+00, -1.6005e-02,  2.8088e+00,  ..., -3.0494e-01,\n",
       "           1.5299e+00,  7.1444e-04],\n",
       "         [ 1.3520e-01,  4.8995e-01,  1.1814e+00,  ...,  4.5321e-01,\n",
       "           1.1171e+00,  1.3322e+00],\n",
       "         [ 1.6640e+00, -1.6005e-02,  2.8088e+00,  ..., -3.0494e-01,\n",
       "           1.5299e+00,  7.1444e-04]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5065e+00,  6.8856e-02,  2.8389e+00,  ..., -3.2513e-01,\n",
       "           1.5437e+00,  5.0619e-02],\n",
       "         [-1.8176e-01,  1.0342e+00,  1.3313e+00,  ...,  2.1743e-01,\n",
       "           6.5638e-01,  9.2617e-01],\n",
       "         [-1.8176e-01,  1.0342e+00,  1.3313e+00,  ...,  2.1743e-01,\n",
       "           6.5638e-01,  9.2617e-01],\n",
       "         ...,\n",
       "         [ 1.0529e+00,  2.1923e-01,  2.1544e+00,  ..., -5.0418e-02,\n",
       "           1.4630e+00,  5.9633e-01],\n",
       "         [ 1.7756e+00, -1.6850e-01,  2.4430e+00,  ..., -1.5542e-03,\n",
       "           9.7754e-01,  8.0806e-01],\n",
       "         [ 1.6661e+00,  5.6827e-02,  2.4541e+00,  ...,  5.0625e-02,\n",
       "           9.3541e-01,  6.7842e-01]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4617,  0.7960,  2.3061,  ..., -0.6474,  1.3033,  0.5667],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [ 0.6140,  0.4173,  1.4860,  ...,  0.6943,  0.1293,  1.0247],\n",
       "         [ 1.7905, -0.2087,  2.4503,  ...,  0.0353,  1.0107,  0.8437],\n",
       "         [ 1.6836, -0.0065,  2.4887,  ...,  0.0539,  0.9930,  0.7464]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7658, -0.0789,  2.8364,  ..., -0.3204,  1.5859, -0.0390],\n",
       "         [ 1.0875,  0.2264,  2.3448,  ...,  0.2457,  1.4572,  0.8970],\n",
       "         [ 0.8590,  0.1577,  2.1300,  ...,  0.1674,  1.2836,  0.9108],\n",
       "         ...,\n",
       "         [ 1.8317,  0.0527,  2.5053,  ...,  0.0701,  0.9281,  0.9245],\n",
       "         [ 1.7592, -0.1597,  2.5045,  ...,  0.1032,  0.9615,  0.9098],\n",
       "         [ 1.7079, -0.1391,  2.4837,  ...,  0.1542,  1.0176,  0.8540]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5770e+00, -1.9306e-02,  2.8238e+00,  ..., -2.7675e-01,\n",
       "           1.5411e+00,  4.6303e-02],\n",
       "         [ 4.8222e-01, -2.9264e-01,  1.7873e+00,  ...,  4.2124e-01,\n",
       "           1.3693e+00,  1.5249e+00],\n",
       "         [ 2.3273e-01, -1.9188e-01,  1.5139e+00,  ...,  4.1197e-01,\n",
       "           1.0940e+00,  1.5056e+00],\n",
       "         ...,\n",
       "         [ 1.6750e+00,  8.6879e-04,  2.7767e+00,  ..., -2.3424e-01,\n",
       "           1.4328e+00, -2.4515e-02],\n",
       "         [ 1.3239e+00, -4.6668e-01,  2.5093e+00,  ..., -3.7985e-02,\n",
       "           1.5538e+00,  7.3953e-01],\n",
       "         [ 1.4229e+00, -4.1724e-01,  2.5014e+00,  ..., -1.1143e-01,\n",
       "           1.6953e+00,  6.9933e-01]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7658, -0.0788,  2.8363,  ..., -0.3204,  1.5859, -0.0390],\n",
       "         [ 0.9237,  0.1530,  2.2433,  ...,  0.2303,  1.2661,  0.6437],\n",
       "         [ 0.9274,  0.1589,  2.0833,  ...,  0.1561,  1.1711,  0.9094],\n",
       "         ...,\n",
       "         [ 1.5554,  0.0076,  2.8190,  ..., -0.3027,  1.5767,  0.0113],\n",
       "         [ 0.5301,  0.2933,  1.3640,  ...,  0.8016,  0.0525,  0.7468],\n",
       "         [ 1.6340, -0.0161,  2.8446,  ..., -0.2601,  1.5557,  0.0371]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4390,  0.7761,  2.2429,  ..., -0.6517,  1.2624,  0.5860],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [ 1.6589, -0.0668,  2.8189,  ..., -0.2893,  1.5028,  0.0558],\n",
       "         [-0.1919, -0.1482,  1.2530,  ...,  0.1204,  0.6532,  1.2064],\n",
       "         [ 0.5614,  0.4864,  2.0854,  ..., -0.4755,  0.9985,  0.2895]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7089, -0.0326,  2.9598,  ..., -0.2920,  1.5465,  0.1361],\n",
       "         [-0.1655,  0.7617,  0.8505,  ...,  0.3001,  1.0434,  0.9061],\n",
       "         [ 0.2461,  0.3561,  1.3323,  ...,  0.9972,  1.0007,  1.2877],\n",
       "         ...,\n",
       "         [ 1.2979, -0.4719,  2.3635,  ..., -0.0180,  1.4773,  0.7009],\n",
       "         [ 0.3103,  0.5012,  1.7606,  ...,  0.9635, -0.2876,  0.8475],\n",
       "         [ 0.9035,  0.5628,  2.5930,  ..., -0.9786,  1.8546,  0.0864]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.3126,  0.5499,  2.5298,  ..., -1.0963,  2.3851, -0.5344],\n",
       "         [ 1.0156,  0.3045,  2.3398,  ..., -0.2143,  1.6858,  0.2724],\n",
       "         [ 0.3528,  0.7035,  1.8237,  ..., -1.4373,  2.2437, -0.3231],\n",
       "         ...,\n",
       "         [ 1.7259,  0.0613,  2.4160,  ...,  0.0891,  0.9435,  0.8769],\n",
       "         [ 1.7259,  0.0613,  2.4160,  ...,  0.0891,  0.9435,  0.8769],\n",
       "         [ 1.8261,  0.0176,  2.5272,  ...,  0.1470,  1.0318,  0.8571]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5618,  0.0197,  2.8233,  ..., -0.3092,  1.5546,  0.0092],\n",
       "         [ 0.3964,  0.8102,  2.2558,  ..., -0.6686,  1.2700,  0.5371],\n",
       "         [ 0.1336,  0.7363,  2.1116,  ..., -0.6494,  1.2172,  0.6843],\n",
       "         ...,\n",
       "         [ 1.5618,  0.0197,  2.8233,  ..., -0.3092,  1.5546,  0.0092],\n",
       "         [ 1.0765,  0.4430,  2.2245,  ..., -0.3741,  1.3663, -0.1096],\n",
       "         [ 1.0073,  0.2758,  2.4041,  ..., -0.3804,  1.3542, -0.1253]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6061,  0.0226,  2.8079,  ..., -0.2763,  1.5740, -0.0276],\n",
       "         [ 0.0553,  0.2198,  1.0758,  ...,  0.5299,  1.1169,  1.3900],\n",
       "         [ 0.0553,  0.2198,  1.0758,  ...,  0.5299,  1.1169,  1.3900],\n",
       "         ...,\n",
       "         [ 0.4619,  0.4347,  1.1626,  ...,  0.2543,  0.3005,  0.3690],\n",
       "         [ 0.8337,  0.4962,  1.1057,  ...,  0.3279,  0.1788,  0.6297],\n",
       "         [ 1.1069,  0.2087,  2.2835,  ..., -0.2795,  1.2368, -0.1405]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7526, -0.0993,  2.8577,  ..., -0.3381,  1.6139, -0.0265],\n",
       "         [ 0.8819,  0.1856,  2.2086,  ...,  0.3296,  1.3183,  1.0162],\n",
       "         [ 1.0088,  0.1230,  2.0920,  ...,  0.0967,  1.2577,  0.7586],\n",
       "         ...,\n",
       "         [ 1.6680, -0.0773,  2.4434,  ...,  0.0859,  0.8812,  0.8358],\n",
       "         [ 1.6107, -0.2257,  2.4122,  ...,  0.5916,  1.4618,  0.4056],\n",
       "         [ 1.0866,  0.1499,  2.3346,  ...,  0.1319,  1.7906,  0.2974]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7933, -0.0931,  2.8512,  ..., -0.3449,  1.5741, -0.0120],\n",
       "         [ 0.7587,  0.5015,  2.3427,  ...,  0.1575,  1.2493,  0.8052],\n",
       "         [ 0.7829,  0.1661,  2.0685,  ...,  0.1779,  1.1063,  0.8881],\n",
       "         ...,\n",
       "         [ 0.2153,  0.1191,  1.7116,  ...,  0.4800,  0.2717,  0.8107],\n",
       "         [ 0.6175,  0.1548,  1.4253,  ...,  0.7064,  0.1560,  0.5528],\n",
       "         [ 1.5346,  0.2268,  3.0408,  ..., -0.3047,  1.8422,  0.1802]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9575,  0.5102,  2.6251,  ..., -0.9892,  1.8578,  0.1155],\n",
       "         [ 0.6856,  0.0226,  1.3869,  ...,  0.9241,  1.3652,  0.6559],\n",
       "         [-0.5599,  0.6436,  0.7314,  ..., -0.0761,  0.8973,  0.8360],\n",
       "         ...,\n",
       "         [-0.0403,  0.2603,  1.6862,  ...,  0.6781,  0.4797,  1.1814],\n",
       "         [ 0.8746, -0.0296,  1.7937,  ...,  1.0878,  0.1909,  1.2254],\n",
       "         [ 0.5883,  0.1650,  2.0181,  ..., -0.3058,  1.8355,  0.6496]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8061, -0.0628,  2.8809,  ..., -0.3427,  1.5608, -0.0225],\n",
       "         [ 0.8681,  0.4812,  2.4565,  ...,  0.3065,  1.5518,  0.6594],\n",
       "         [ 0.8707,  0.2336,  2.1757,  ...,  0.1454,  1.0491,  0.9260],\n",
       "         ...,\n",
       "         [ 0.9552, -0.1766,  2.2150,  ...,  0.0554,  1.4429,  0.4221],\n",
       "         [ 1.5163, -0.3548,  2.6027,  ...,  0.8210,  1.2344,  0.8527],\n",
       "         [ 0.9359,  0.5425,  2.4298,  ..., -0.1935,  1.9043,  0.1514]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7736e+00,  3.5393e-01,  3.2489e+00,  ..., -3.0568e-01,\n",
       "           2.0101e+00, -4.1972e-02],\n",
       "         [-6.6974e-01,  8.1737e-01,  9.7571e-01,  ...,  3.8134e-01,\n",
       "           7.9057e-01,  1.2555e+00],\n",
       "         [ 2.8804e-02,  4.2132e-01,  1.8399e+00,  ...,  1.2185e+00,\n",
       "           8.9989e-01,  1.2436e+00],\n",
       "         ...,\n",
       "         [ 5.1844e-01, -3.7834e-01,  1.6146e+00,  ...,  1.0018e+00,\n",
       "           8.0274e-01,  1.0016e+00],\n",
       "         [ 1.7543e+00, -3.1739e-03,  2.4940e+00,  ...,  1.4299e-01,\n",
       "           9.6426e-01,  9.1202e-01],\n",
       "         [ 1.6504e+00,  1.1461e-02,  2.4742e+00,  ...,  5.8852e-02,\n",
       "           9.4675e-01,  7.6227e-01]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6360, -0.0166,  2.9026,  ..., -0.2519,  1.5676,  0.0445],\n",
       "         [ 0.7402,  0.1315,  1.3948,  ...,  0.6467,  0.2329,  1.3127],\n",
       "         [ 0.1315,  0.2584,  1.3769,  ...,  0.5024,  0.2997,  1.3648],\n",
       "         ...,\n",
       "         [ 0.1315,  0.2584,  1.3769,  ...,  0.5024,  0.2997,  1.3648],\n",
       "         [ 0.6403,  0.0276,  1.4162,  ...,  0.6836,  0.2006,  1.1448],\n",
       "         [ 1.6360, -0.0166,  2.9026,  ..., -0.2519,  1.5676,  0.0445]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.8796,  0.5096,  2.6356,  ..., -0.9468,  1.8219,  0.1073],\n",
       "         [ 0.1449,  0.3699,  1.6653,  ...,  1.3854,  0.5808,  1.3024],\n",
       "         [-0.5899,  0.9443,  1.2336,  ...,  0.2372,  0.6041,  1.0107],\n",
       "         ...,\n",
       "         [ 1.3192,  0.1501,  3.0002,  ..., -0.3240,  2.2340,  0.2314],\n",
       "         [ 0.3067,  0.0654,  1.6535,  ...,  0.5444,  0.7235,  1.4492],\n",
       "         [ 1.6130, -0.0054,  2.3541,  ...,  0.1061,  0.7233,  0.6848]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6446e+00, -9.4222e-04,  2.8743e+00,  ..., -2.5173e-01,\n",
       "           1.4710e+00,  8.3876e-02],\n",
       "         [ 4.7420e-01,  3.8775e-01,  1.2545e+00,  ...,  1.1656e+00,\n",
       "           5.6008e-01,  8.9187e-01],\n",
       "         [-1.8964e-05,  5.3426e-01,  1.2267e+00,  ...,  1.0316e+00,\n",
       "           6.6904e-01,  1.0599e+00],\n",
       "         ...,\n",
       "         [-5.9771e-02,  5.2537e-01,  1.1979e+00,  ...,  9.8706e-01,\n",
       "           6.2186e-01,  9.9636e-01],\n",
       "         [ 2.1022e-01,  6.2865e-01,  1.1472e+00,  ...,  7.3599e-01,\n",
       "           1.0910e+00,  5.2221e-01],\n",
       "         [ 1.5835e+00,  6.2642e-02,  2.8788e+00,  ..., -3.0547e-01,\n",
       "           1.4709e+00,  8.5607e-02]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6966e+00,  1.4055e-02,  2.8972e+00,  ..., -2.7152e-01,\n",
       "           1.5553e+00,  9.7612e-02],\n",
       "         [ 2.3218e-01,  1.1400e-01,  1.5334e+00,  ...,  1.1587e+00,\n",
       "           6.5074e-01,  1.1995e+00],\n",
       "         [-2.4618e-01,  3.1829e-01,  1.2987e+00,  ...,  6.3969e-01,\n",
       "           7.8985e-01,  1.3612e+00],\n",
       "         ...,\n",
       "         [ 1.6446e+00, -9.4222e-04,  2.8743e+00,  ..., -2.5173e-01,\n",
       "           1.4710e+00,  8.3876e-02],\n",
       "         [ 9.7929e-01,  2.9296e-01,  2.2480e+00,  ..., -3.1646e-02,\n",
       "           1.7610e+00,  1.9890e-01],\n",
       "         [ 9.4119e-01, -1.2409e-01,  2.1869e+00,  ...,  1.3013e-01,\n",
       "           1.5515e+00,  6.1570e-01]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8061, -0.0628,  2.8809,  ..., -0.3427,  1.5608, -0.0225],\n",
       "         [ 0.8681,  0.4280,  2.4257,  ...,  0.2841,  1.5325,  0.7040],\n",
       "         [ 0.8707,  0.2336,  2.1757,  ...,  0.1454,  1.0491,  0.9260],\n",
       "         ...,\n",
       "         [ 1.6565,  0.0532,  2.7899,  ..., -0.2810,  1.5710, -0.0307],\n",
       "         [ 0.0717,  0.1470,  1.1279,  ...,  0.8550,  0.9913,  1.4410],\n",
       "         [ 1.6565,  0.0532,  2.7899,  ..., -0.2810,  1.5710, -0.0307]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6535,  0.0397,  2.8030,  ..., -0.2968,  1.5716, -0.0286],\n",
       "         [ 0.4466,  0.2911,  1.1453,  ...,  0.7120,  1.1220,  1.5636],\n",
       "         [ 0.4466,  0.2911,  1.1453,  ...,  0.7120,  1.1220,  1.5636],\n",
       "         ...,\n",
       "         [ 0.1246,  0.4755,  2.2571,  ..., -0.0617,  1.8338,  0.8891],\n",
       "         [ 0.1246,  0.4755,  2.2571,  ..., -0.0617,  1.8338,  0.8891],\n",
       "         [ 0.7779,  0.3452,  2.4191,  ..., -0.1942,  1.6005,  0.7912]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8746e+00, -2.4809e-01,  2.4232e+00,  ...,  2.5095e-02,\n",
       "           9.6925e-01,  1.0583e+00],\n",
       "         [ 1.9699e+00, -2.3944e-01,  2.4462e+00,  ..., -2.1522e-03,\n",
       "           9.4185e-01,  1.0636e+00],\n",
       "         [ 1.9630e+00, -1.6879e-01,  2.4449e+00,  ...,  1.7597e-02,\n",
       "           9.9279e-01,  1.1682e+00],\n",
       "         ...,\n",
       "         [ 1.1165e+00,  3.4005e-01,  2.3167e+00,  ..., -3.5050e-01,\n",
       "           1.2784e+00, -8.4663e-02],\n",
       "         [ 1.5747e+00,  1.1831e-01,  2.3328e+00,  ..., -3.4491e-01,\n",
       "           1.4315e+00,  2.6235e-01],\n",
       "         [ 3.1542e-01, -1.5219e-01,  1.4876e+00,  ...,  2.3138e-01,\n",
       "           4.4026e-01,  4.9566e-01]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6784, -0.0222,  2.8307,  ..., -0.2951,  1.4512, -0.0145],\n",
       "         [-0.2923,  0.4577,  1.2913,  ..., -0.3913,  0.8734,  1.0419],\n",
       "         [ 0.0529,  0.0666,  1.2574,  ...,  0.2331,  0.7259,  1.3292],\n",
       "         ...,\n",
       "         [ 0.6975,  0.3043,  1.4251,  ...,  0.7090,  0.1790,  1.0458],\n",
       "         [ 0.8444,  0.3737,  1.4597,  ...,  0.7129,  0.1530,  1.1617],\n",
       "         [ 1.7195, -0.2782,  2.4370,  ..., -0.0899,  0.9456,  0.7379]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.8132,  0.5104,  2.6950,  ..., -0.9625,  1.8977,  0.0496],\n",
       "         [-0.8871,  0.7069,  0.6532,  ...,  0.1369,  0.3101,  1.0915],\n",
       "         [-0.8871,  0.7069,  0.6532,  ...,  0.1369,  0.3101,  1.0915],\n",
       "         ...,\n",
       "         [ 1.3262,  0.1371,  2.9605,  ..., -0.3110,  2.2221,  0.2168],\n",
       "         [-0.0329,  0.2943,  1.5977,  ...,  0.5105,  0.5930,  1.0564],\n",
       "         [ 1.7195, -0.2388,  2.5732,  ..., -0.0234,  0.9206,  0.8244]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.8819,  0.5208,  2.6485,  ..., -1.0144,  1.9685,  0.0580],\n",
       "         [ 0.3050,  0.9929,  1.1497,  ...,  0.0350,  0.5626,  1.3512],\n",
       "         [-0.0560,  0.9710,  1.1424,  ..., -0.0042,  0.5863,  1.0171],\n",
       "         ...,\n",
       "         [ 1.5147, -0.0078,  2.3502,  ...,  0.0114,  0.7502,  0.6326],\n",
       "         [ 0.7643,  0.4351,  2.4271,  ..., -0.1836,  1.2817,  0.5452],\n",
       "         [ 0.9013,  0.3791,  2.3840,  ..., -0.1166,  1.2570,  0.6037]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6496,  0.0232,  2.8805,  ..., -0.2603,  1.4887,  0.0869],\n",
       "         [ 0.4435, -0.0231,  1.2313,  ...,  1.4504,  0.5600,  0.9132],\n",
       "         [ 0.1190,  0.1869,  1.2275,  ...,  1.2600,  0.3594,  1.3075],\n",
       "         ...,\n",
       "         [ 0.1074,  0.1800,  1.1994,  ...,  1.2927,  0.3684,  1.3033],\n",
       "         [ 0.4581,  0.1343,  1.3701,  ...,  1.4964,  0.1415,  1.2083],\n",
       "         [ 1.6273, -0.0106,  2.8388,  ..., -0.2330,  1.5120,  0.0617]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7750, -0.0591,  2.8481,  ..., -0.3361,  1.6207, -0.0507],\n",
       "         [ 0.8750,  0.0476,  2.2900,  ...,  0.6635,  1.4234,  0.9442],\n",
       "         [ 0.9757,  0.2139,  2.1451,  ...,  0.1677,  1.1315,  0.9099],\n",
       "         ...,\n",
       "         [ 0.0771,  0.2324,  1.4135,  ...,  1.3235,  0.6155,  1.3612],\n",
       "         [ 0.4318,  0.0831,  1.5068,  ...,  1.7562,  0.5026,  1.3295],\n",
       "         [ 1.6515,  0.0212,  2.8878,  ..., -0.2442,  1.5416,  0.0700]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6444, -0.0061,  2.9355,  ..., -0.2529,  1.5778,  0.0645],\n",
       "         [ 0.8741, -0.1000,  1.3780,  ...,  0.6886,  0.1179,  1.2764],\n",
       "         [ 0.3642,  0.0619,  1.4502,  ...,  0.5870,  0.2997,  1.4644],\n",
       "         ...,\n",
       "         [ 0.4720, -0.1032,  1.7359,  ...,  1.1999,  0.0536,  1.1407],\n",
       "         [ 0.0611,  0.0146,  1.8706,  ...,  1.0863,  0.2308,  0.8602],\n",
       "         [ 1.6791, -0.1426,  2.3215,  ...,  0.0413,  0.9494,  0.8760]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8389, -0.0833,  2.8819,  ..., -0.3653,  1.6418,  0.0051],\n",
       "         [ 0.1145,  0.6844,  2.0831,  ..., -0.5944,  1.5011,  0.7610],\n",
       "         [ 0.9136,  0.2009,  2.1194,  ...,  0.1124,  1.2381,  0.8777],\n",
       "         ...,\n",
       "         [ 1.1602,  0.6744,  2.5028,  ..., -1.0259,  1.9393,  0.2985],\n",
       "         [ 1.8128,  0.0509,  2.4223,  ...,  0.0634,  0.9353,  0.8552],\n",
       "         [ 1.6504,  0.0115,  2.4741,  ...,  0.0588,  0.9468,  0.7622]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6360, -0.0166,  2.9026,  ..., -0.2519,  1.5676,  0.0445],\n",
       "         [ 0.7645,  0.1701,  1.5402,  ...,  0.6259,  0.1681,  1.2028],\n",
       "         [ 0.1482,  0.2651,  1.4977,  ...,  0.5170,  0.2448,  1.3195],\n",
       "         ...,\n",
       "         [ 1.6065, -0.0324,  2.8198,  ..., -0.3312,  1.5778, -0.0309],\n",
       "         [-1.1969,  0.6644,  0.0683,  ...,  0.2242,  0.6375,  1.7944],\n",
       "         [ 1.6065, -0.0324,  2.8198,  ..., -0.3312,  1.5778, -0.0309]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5978, -0.0619,  2.8210,  ..., -0.2370,  1.5066,  0.0557],\n",
       "         [-0.0304, -0.1973,  1.5168,  ...,  0.2924,  0.9702,  1.2676],\n",
       "         [-0.2223, -0.1067,  1.3318,  ...,  0.2072,  0.7379,  1.2432],\n",
       "         ...,\n",
       "         [ 0.8819,  0.5208,  2.6485,  ..., -1.0144,  1.9685,  0.0580],\n",
       "         [-0.1174,  0.9924,  1.1304,  ..., -0.0359,  0.6127,  1.0443],\n",
       "         [ 1.1602,  0.6744,  2.5028,  ..., -1.0259,  1.9393,  0.2985]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4259,  0.7564,  2.1778,  ..., -0.6651,  1.2401,  0.6008],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [ 1.6304, -0.0912,  2.3888,  ...,  0.0437,  0.9914,  0.7155],\n",
       "         [ 0.4259,  0.7564,  2.1778,  ..., -0.6651,  1.2401,  0.6008],\n",
       "         [ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6360, -0.0166,  2.9026,  ..., -0.2519,  1.5676,  0.0445],\n",
       "         [ 0.6200,  0.0667,  1.4045,  ...,  0.7113,  0.1837,  1.0421],\n",
       "         [ 0.1326,  0.2548,  1.4092,  ...,  0.5990,  0.3178,  1.2096],\n",
       "         ...,\n",
       "         [ 0.2275,  0.2636,  1.5406,  ...,  0.3678,  0.5479,  0.8444],\n",
       "         [ 0.7100,  0.1055,  1.3534,  ...,  0.7171,  0.1456,  1.0986],\n",
       "         [ 1.6486,  0.2758,  2.9128,  ..., -0.3672,  2.0062,  0.0134]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6444, -0.0061,  2.9355,  ..., -0.2529,  1.5778,  0.0645],\n",
       "         [ 0.9289, -0.0889,  1.3786,  ...,  0.6637,  0.2167,  1.3768],\n",
       "         [ 0.3278,  0.0410,  1.4333,  ...,  0.5939,  0.3671,  1.3932],\n",
       "         ...,\n",
       "         [ 1.5025,  0.2455,  3.0880,  ..., -0.3235,  1.8760,  0.1905],\n",
       "         [ 0.2124,  0.3584,  1.8308,  ...,  0.5150,  0.2857,  0.9771],\n",
       "         [ 1.6015, -0.1829,  2.3391,  ...,  0.0585,  1.0076,  0.8154]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.8935,  0.5459,  2.6421,  ..., -0.9662,  1.8699,  0.0908],\n",
       "         [ 0.8279,  0.3835,  1.6491,  ...,  1.0220,  0.8384,  0.8245],\n",
       "         [-0.4759,  1.0248,  1.1248,  ...,  0.1942,  0.3644,  0.9767],\n",
       "         ...,\n",
       "         [-0.2305,  0.8221,  1.2020,  ..., -0.0648,  0.0372,  0.8781],\n",
       "         [ 1.0979,  0.2001,  1.7109,  ...,  0.5467,  0.2263,  0.7627],\n",
       "         [ 0.8859,  0.5357,  2.6167,  ..., -0.9787,  1.8546,  0.1421]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9432,  0.5394,  2.6602,  ..., -0.9472,  1.8432,  0.1019],\n",
       "         [ 0.4597,  0.8627,  1.8696,  ...,  1.0740,  0.5794,  1.0975],\n",
       "         [-0.2233,  1.0382,  1.3164,  ...,  0.6353,  0.4873,  1.0541],\n",
       "         ...,\n",
       "         [ 1.7413, -0.0867,  2.5045,  ...,  0.3576,  0.8972,  0.9669],\n",
       "         [ 1.9248, -0.2078,  2.4551,  ...,  0.0572,  0.9040,  0.9994],\n",
       "         [ 0.7486,  0.3363,  1.9174,  ...,  1.3976,  0.0906,  1.4591]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7750, -0.0591,  2.8481,  ..., -0.3361,  1.6207, -0.0507],\n",
       "         [ 0.9198, -0.0059,  2.1499,  ...,  0.6658,  1.2339,  1.0304],\n",
       "         [ 0.9310,  0.2158,  2.1119,  ...,  0.1874,  1.1536,  0.8739],\n",
       "         ...,\n",
       "         [ 1.7050, -0.0150,  2.9022,  ..., -0.2877,  1.5641,  0.1036],\n",
       "         [ 0.0492,  0.3920,  1.2847,  ...,  1.2164,  0.4070,  1.5047],\n",
       "         [ 0.9202,  0.5536,  2.6136,  ..., -1.0069,  1.8809,  0.0650]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5534,  0.0131,  2.8427,  ..., -0.3179,  1.5599, -0.0247],\n",
       "         [ 0.5231,  0.5041,  1.9067,  ..., -0.4816,  1.6581,  0.9173],\n",
       "         [ 0.5774,  0.5909,  2.0069,  ..., -0.5435,  1.4294,  0.7699],\n",
       "         ...,\n",
       "         [-0.6045,  1.1111,  1.3269,  ...,  0.3145,  0.2319,  0.9750],\n",
       "         [ 0.4895,  0.5184,  1.8242,  ...,  1.2980,  0.0789,  1.2413],\n",
       "         [ 0.8854,  0.5594,  2.6338,  ..., -0.9528,  1.8102,  0.0589]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8389, -0.0833,  2.8819,  ..., -0.3653,  1.6418,  0.0051],\n",
       "         [ 0.1145,  0.6844,  2.0831,  ..., -0.5944,  1.5011,  0.7610],\n",
       "         [ 0.9136,  0.2009,  2.1194,  ...,  0.1124,  1.2381,  0.8777],\n",
       "         ...,\n",
       "         [ 1.8272, -0.2725,  2.4148,  ...,  0.0688,  0.9499,  1.0098],\n",
       "         [ 1.8775, -0.0188,  2.4117,  ...,  0.0916,  0.8355,  0.8463],\n",
       "         [ 1.6964, -0.1465,  2.4844,  ...,  0.1827,  0.9763,  0.8682]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8053, -0.0958,  2.8869,  ..., -0.3675,  1.6081, -0.0063],\n",
       "         [ 0.6546,  0.5140,  1.9315,  ..., -0.5271,  1.6306,  0.8106],\n",
       "         [ 0.8653,  0.1996,  2.1255,  ...,  0.0755,  1.2185,  0.8091],\n",
       "         ...,\n",
       "         [ 0.1367,  0.1631,  1.9745,  ...,  1.0037,  0.3070,  0.9186],\n",
       "         [ 0.6254,  0.0694,  1.8082,  ...,  1.0546,  0.1143,  1.3067],\n",
       "         [ 0.8466, -0.0088,  2.5255,  ...,  0.4025,  1.3242,  0.2647]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9106,  0.5270,  2.6335,  ..., -0.9536,  1.8455,  0.0883],\n",
       "         [ 0.1193,  0.6465,  1.7316,  ...,  1.0099,  0.2499,  0.8067],\n",
       "         [-0.7639,  1.1683,  1.1949,  ...,  0.0391,  0.3746,  0.5658],\n",
       "         ...,\n",
       "         [ 1.5649,  0.2906,  3.0597,  ..., -0.3344,  1.7222,  0.1251],\n",
       "         [-0.6073,  0.3149,  0.7202,  ...,  0.4910,  0.0427,  1.5921],\n",
       "         [ 1.5649,  0.2906,  3.0597,  ..., -0.3344,  1.7222,  0.1251]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4617,  0.7960,  2.3061,  ..., -0.6474,  1.3033,  0.5667],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [ 1.5025,  0.2455,  3.0880,  ..., -0.3235,  1.8760,  0.1905],\n",
       "         [ 1.7885,  0.0706,  2.5005,  ...,  0.1126,  0.9634,  0.8744],\n",
       "         [ 1.6835, -0.0065,  2.4886,  ...,  0.0539,  0.9930,  0.7464]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6274, -0.0106,  2.8388,  ..., -0.2331,  1.5120,  0.0616],\n",
       "         [ 0.7202,  0.2174,  1.3990,  ...,  1.5072,  0.0210,  1.1339],\n",
       "         [ 0.4404,  0.0924,  1.3454,  ...,  1.2565,  0.1461,  1.4931],\n",
       "         ...,\n",
       "         [ 1.0762, -0.0483,  2.5528,  ...,  0.0907,  1.1410,  0.8433],\n",
       "         [ 1.4131, -0.1640,  2.5655,  ...,  0.4047,  1.0366,  0.8808],\n",
       "         [ 1.2199, -0.1734,  2.4777,  ...,  0.3597,  1.0716,  0.6842]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7658, -0.0789,  2.8364,  ..., -0.3204,  1.5859, -0.0390],\n",
       "         [ 1.0875,  0.2264,  2.3448,  ...,  0.2457,  1.4572,  0.8970],\n",
       "         [ 0.8590,  0.1577,  2.1300,  ...,  0.1674,  1.2836,  0.9108],\n",
       "         ...,\n",
       "         [ 1.1549,  0.3898,  2.3014,  ..., -0.2782,  1.1272,  0.0142],\n",
       "         [ 1.6038,  0.0795,  2.3659,  ...,  0.1015,  0.8081,  0.7716],\n",
       "         [ 1.7079, -0.1391,  2.4837,  ...,  0.1542,  1.0176,  0.8540]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.9857, -0.2112,  2.4283,  ...,  0.0092,  0.9303,  1.0603],\n",
       "         [ 1.9857, -0.2112,  2.4283,  ...,  0.0092,  0.9303,  1.0603],\n",
       "         [ 1.9630, -0.1688,  2.4449,  ...,  0.0176,  0.9928,  1.1682],\n",
       "         ...,\n",
       "         [ 0.8294,  0.3803,  1.4348,  ...,  0.7371,  0.0906,  0.9491],\n",
       "         [ 0.5081,  0.6943,  1.4550,  ...,  0.1762,  0.5292,  0.4080],\n",
       "         [ 1.9857, -0.2112,  2.4283,  ...,  0.0092,  0.9303,  1.0603]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7057, -0.0296,  2.8438,  ..., -0.2603,  1.3964, -0.0036],\n",
       "         [ 0.3025,  0.3379,  1.0171,  ...,  0.7099,  1.5573,  0.8415],\n",
       "         [-0.1150,  0.2375,  1.1214,  ...,  0.4932,  0.5884,  1.1434],\n",
       "         ...,\n",
       "         [ 0.9246, -0.2167,  2.2311,  ...,  0.8147,  1.5627,  0.9823],\n",
       "         [ 1.2686, -0.2442,  2.2294,  ...,  0.8352,  1.7355,  0.9330],\n",
       "         [ 1.1244,  0.1089,  2.4160,  ...,  0.6641,  1.4777,  0.9408]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6476, -0.0375,  2.8280,  ..., -0.2487,  1.4624, -0.0078],\n",
       "         [ 0.1483, -0.2233,  1.6630,  ...,  0.8136,  1.1332,  1.5607],\n",
       "         [-0.1917,  0.0638,  1.2916,  ...,  0.7198,  0.6723,  1.0635],\n",
       "         ...,\n",
       "         [-0.0436,  0.3691,  1.1875,  ...,  1.0266,  0.4619,  0.9011],\n",
       "         [ 0.3709,  0.2993,  1.3909,  ...,  1.2782,  0.1997,  0.7185],\n",
       "         [ 1.5384,  0.0422,  2.8242,  ..., -0.2873,  1.4675,  0.0556]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9235,  0.5154,  2.6469,  ..., -0.9679,  1.8320,  0.1114],\n",
       "         [ 0.3877,  0.3059,  1.4866,  ...,  0.6127,  0.5587,  1.1480],\n",
       "         [-0.6155,  0.6661,  0.6819,  ...,  0.0351,  0.8039,  1.2413],\n",
       "         ...,\n",
       "         [ 0.2731, -0.1887,  1.8399,  ...,  1.5000,  0.4758,  1.2536],\n",
       "         [ 0.8669, -0.2011,  1.8755,  ...,  1.2771,  0.2051,  1.2586],\n",
       "         [ 0.9781, -0.1966,  1.8847,  ...,  0.6463,  1.5051,  1.1840]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6444, -0.0061,  2.9355,  ..., -0.2529,  1.5778,  0.0645],\n",
       "         [ 1.0567,  0.0069,  1.4538,  ...,  0.6327,  0.3129,  1.5466],\n",
       "         [ 0.4713,  0.1663,  1.5232,  ...,  0.5718,  0.4508,  1.5479],\n",
       "         ...,\n",
       "         [ 1.6802,  0.0949,  2.4443,  ...,  0.0967,  0.7551,  0.7361],\n",
       "         [ 1.7592, -0.1597,  2.5045,  ...,  0.1032,  0.9615,  0.9098],\n",
       "         [ 1.7254, -0.0863,  2.3804,  ...,  0.0361,  1.0835,  0.9581]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6444, -0.0061,  2.9355,  ..., -0.2529,  1.5778,  0.0645],\n",
       "         [ 0.9708, -0.0580,  1.3449,  ...,  0.6092,  0.2015,  1.4106],\n",
       "         [ 0.3460,  0.0820,  1.3920,  ...,  0.5557,  0.3463,  1.4406],\n",
       "         ...,\n",
       "         [-0.4146,  1.0366,  0.8117,  ..., -0.3151,  0.2321,  0.5449],\n",
       "         [ 0.7203,  0.7155,  1.2059,  ...,  0.4813,  0.1286,  0.6485],\n",
       "         [ 0.8893,  0.5344,  2.6368,  ..., -0.9749,  1.8583,  0.1348]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7089, -0.0326,  2.9598,  ..., -0.2920,  1.5465,  0.1361],\n",
       "         [-0.0072,  0.6528,  0.8472,  ...,  0.2631,  1.0824,  0.7273],\n",
       "         [ 0.4493,  0.3176,  1.4083,  ...,  0.9725,  1.0739,  1.1204],\n",
       "         ...,\n",
       "         [-0.4144,  0.2990,  0.5348,  ...,  0.8058,  0.8762,  1.7298],\n",
       "         [-0.2478,  0.2744,  0.7147,  ...,  0.7826,  0.9584,  1.7193],\n",
       "         [ 1.5693, -0.0054,  2.7925,  ..., -0.1907,  1.5108, -0.0454]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9043,  0.5348,  2.6286,  ..., -0.9550,  1.8468,  0.0746],\n",
       "         [ 0.5242,  0.5359,  1.6955,  ...,  0.6628,  0.0471,  0.9676],\n",
       "         [-0.7845,  1.1385,  1.1305,  ..., -0.1384,  0.3606,  0.7159],\n",
       "         ...,\n",
       "         [ 1.7729, -0.1247,  2.4918,  ...,  0.1403,  0.9273,  0.8777],\n",
       "         [ 1.0355,  0.2694,  2.1442,  ..., -0.0377,  1.5479,  0.6245],\n",
       "         [ 1.1663,  0.1595,  2.6725,  ...,  0.1172,  0.9453,  0.7353]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5618,  0.0197,  2.8233,  ..., -0.3092,  1.5546,  0.0092],\n",
       "         [ 0.3964,  0.8102,  2.2558,  ..., -0.6686,  1.2700,  0.5371],\n",
       "         [ 0.1336,  0.7363,  2.1116,  ..., -0.6494,  1.2172,  0.6843],\n",
       "         ...,\n",
       "         [ 1.1924,  0.0737,  2.5115,  ...,  0.2939,  2.0031,  0.6656],\n",
       "         [ 1.6443, -0.1312,  2.3842,  ...,  0.8128,  1.5833,  1.1586],\n",
       "         [ 0.9694,  0.2577,  2.4432,  ...,  0.2190,  1.9112,  0.6087]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.3984,  0.0338,  2.8181,  ..., -0.5768,  1.9524, -0.0639],\n",
       "         [ 1.1269,  0.0356,  1.6186,  ...,  0.6057,  0.4227,  1.5005],\n",
       "         [ 0.4200,  0.2057,  1.3650,  ...,  0.2773,  0.6830,  1.3052],\n",
       "         ...,\n",
       "         [ 1.3926,  0.0281,  2.8022,  ..., -0.5945,  1.9394, -0.0854],\n",
       "         [ 0.2176,  0.3560,  1.4536,  ...,  0.3273,  0.5726,  1.1186],\n",
       "         [ 1.7506, -0.1337,  2.5122,  ...,  0.0498,  1.1355,  0.8650]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7642, -0.0982,  2.8432,  ..., -0.3356,  1.6097, -0.0412],\n",
       "         [ 0.8686,  0.1686,  1.8810,  ..., -0.1794,  1.7576,  0.4015],\n",
       "         [ 0.8810,  0.1834,  2.0567,  ...,  0.1393,  1.2166,  0.7195],\n",
       "         ...,\n",
       "         [ 1.3934,  0.1743,  3.0329,  ..., -0.2371,  2.1900,  0.2300],\n",
       "         [ 0.1538,  0.3314,  1.7618,  ...,  1.4880,  1.0663,  1.5716],\n",
       "         [ 1.0548,  0.0481,  2.1802,  ...,  0.2790,  1.5532,  0.4948]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6392, -0.0485,  2.8532,  ..., -0.2390,  1.4594, -0.0442],\n",
       "         [ 0.7241,  0.2564,  1.1702,  ...,  0.6145,  1.0936,  1.0645],\n",
       "         [ 0.2936,  0.3818,  1.2530,  ...,  0.6541,  0.8447,  1.0563],\n",
       "         ...,\n",
       "         [ 1.3262,  0.1371,  2.9605,  ..., -0.3110,  2.2221,  0.2168],\n",
       "         [-0.0329,  0.2943,  1.5977,  ...,  0.5105,  0.5930,  1.0564],\n",
       "         [ 1.7230, -0.2592,  2.5291,  ..., -0.0038,  0.8395,  0.8468]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.7658, -0.0789,  2.8364,  ..., -0.3204,  1.5859, -0.0390],\n",
       "         [ 1.0875,  0.2264,  2.3448,  ...,  0.2457,  1.4572,  0.8970],\n",
       "         [ 0.8590,  0.1577,  2.1300,  ...,  0.1674,  1.2836,  0.9108],\n",
       "         ...,\n",
       "         [ 0.8353,  0.0165,  2.5395,  ...,  0.4000,  1.4898,  0.2861],\n",
       "         [ 1.9121, -0.0036,  2.4556,  ...,  0.1140,  0.8392,  0.9771],\n",
       "         [ 1.7079, -0.1391,  2.4837,  ...,  0.1542,  1.0176,  0.8540]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6444, -0.0061,  2.9355,  ..., -0.2529,  1.5778,  0.0645],\n",
       "         [ 0.9404,  0.0074,  1.4365,  ...,  0.6717,  0.0690,  1.3257],\n",
       "         [ 0.3000,  0.1366,  1.4965,  ...,  0.5955,  0.2029,  1.3808],\n",
       "         ...,\n",
       "         [ 1.9035, -0.1458,  2.5425,  ...,  0.0850,  0.9873,  0.9037],\n",
       "         [ 0.5557,  0.5634,  1.7909,  ...,  1.0734,  0.1250,  1.0296],\n",
       "         [ 1.5644, -0.0864,  2.3601,  ...,  0.0224,  0.8196,  0.7602]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4259,  0.7564,  2.1778,  ..., -0.6651,  1.2401,  0.6008],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [ 1.5649,  0.2906,  3.0597,  ..., -0.3344,  1.7222,  0.1251],\n",
       "         [-0.5389,  0.2890,  0.7341,  ...,  0.3777,  0.1130,  1.4980],\n",
       "         [ 1.5649,  0.2906,  3.0597,  ..., -0.3344,  1.7222,  0.1251]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.9332,  0.0877,  2.7265,  ..., -0.6079,  2.3459, -0.5651],\n",
       "         [ 0.8111,  0.3001,  2.0892,  ..., -0.1035,  1.8694,  0.2940],\n",
       "         [ 1.4036,  0.2001,  2.0350,  ..., -0.7166,  2.1408, -0.4139],\n",
       "         ...,\n",
       "         [ 0.8819,  0.5208,  2.6485,  ..., -1.0144,  1.9685,  0.0580],\n",
       "         [-0.0686,  1.0072,  1.1603,  ..., -0.1250,  0.6913,  1.1159],\n",
       "         [ 1.1602,  0.6744,  2.5028,  ..., -1.0259,  1.9393,  0.2985]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5620,  0.0197,  2.8230,  ..., -0.3095,  1.5549,  0.0091],\n",
       "         [ 0.4390,  0.7761,  2.2429,  ..., -0.6517,  1.2624,  0.5860],\n",
       "         [ 0.1345,  0.7521,  2.1372,  ..., -0.6520,  1.2181,  0.7080],\n",
       "         ...,\n",
       "         [-0.4407,  1.0426,  1.1588,  ...,  0.6233,  0.4774,  1.0511],\n",
       "         [ 0.4896,  0.5393,  1.6478,  ...,  1.5105,  0.3071,  1.2661],\n",
       "         [ 0.8688,  0.5794,  2.6249,  ..., -0.9443,  1.8550,  0.0269]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9575,  0.5102,  2.6251,  ..., -0.9892,  1.8578,  0.1155],\n",
       "         [ 0.5288, -0.0512,  1.2838,  ...,  1.0120,  1.2553,  0.7985],\n",
       "         [-0.5471,  0.5112,  0.7411,  ...,  0.0825,  0.6796,  0.8420],\n",
       "         ...,\n",
       "         [ 1.8036, -0.3431,  2.4720,  ...,  0.0379,  0.8770,  0.8624],\n",
       "         [ 1.7262, -0.0716,  2.5035,  ...,  0.0299,  1.0003,  0.8050],\n",
       "         [ 1.7649, -0.1317,  2.6161,  ..., -0.0058,  0.8909,  0.9208]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8389, -0.0833,  2.8819,  ..., -0.3653,  1.6418,  0.0051],\n",
       "         [ 0.3023,  0.5951,  2.0459,  ..., -0.4874,  1.5141,  0.8075],\n",
       "         [ 0.9082,  0.1907,  2.1231,  ...,  0.1166,  1.2729,  0.8662],\n",
       "         ...,\n",
       "         [ 1.9321,  0.0189,  2.4903,  ...,  0.1232,  0.8592,  0.9850],\n",
       "         [ 0.8338,  0.2239,  2.2024,  ...,  0.0365,  1.7908,  0.1420],\n",
       "         [ 0.7005, -0.1983,  2.2640,  ...,  0.2501,  1.1121,  0.3263]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5618,  0.0197,  2.8233,  ..., -0.3092,  1.5546,  0.0092],\n",
       "         [ 0.4716,  0.7574,  2.2294,  ..., -0.6507,  1.2586,  0.4990],\n",
       "         [ 0.1336,  0.7363,  2.1116,  ..., -0.6494,  1.2172,  0.6843],\n",
       "         ...,\n",
       "         [ 0.1336,  0.7363,  2.1116,  ..., -0.6494,  1.2172,  0.6843],\n",
       "         [ 0.4574,  0.7311,  2.1708,  ..., -0.6534,  1.2173,  0.5006],\n",
       "         [ 1.5618,  0.0197,  2.8233,  ..., -0.3092,  1.5546,  0.0092]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.5618e+00,  1.9737e-02,  2.8233e+00,  ..., -3.0917e-01,\n",
       "           1.5546e+00,  9.2241e-03],\n",
       "         [ 3.0998e-01,  7.3528e-01,  2.2322e+00,  ..., -6.8365e-01,\n",
       "           1.2167e+00,  3.8500e-01],\n",
       "         [ 1.7126e-01,  7.2881e-01,  2.1559e+00,  ..., -6.0927e-01,\n",
       "           1.2115e+00,  7.1888e-01],\n",
       "         ...,\n",
       "         [ 1.6089e+00,  1.4615e-01,  2.3925e+00,  ..., -3.6712e-01,\n",
       "           1.5633e+00,  3.2904e-01],\n",
       "         [ 1.8269e+00, -2.5919e-01,  2.3721e+00,  ...,  7.2861e-02,\n",
       "           9.2382e-01,  8.2115e-01],\n",
       "         [ 1.1658e+00,  4.0038e-01,  2.3756e+00,  ..., -3.7769e-01,\n",
       "           1.4237e+00, -6.1124e-04]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.4997,  0.0153,  2.7661,  ..., -0.3418,  1.5768, -0.0302],\n",
       "         [ 0.1535,  0.1776,  1.3601,  ...,  0.4887,  1.0830,  1.4642],\n",
       "         [ 0.1535,  0.1776,  1.3601,  ...,  0.4887,  1.0830,  1.4642],\n",
       "         ...,\n",
       "         [ 0.6973,  0.4988,  2.4809,  ...,  0.1916,  1.3673,  0.6598],\n",
       "         [ 1.6267, -0.2882,  2.3279,  ...,  0.5567,  1.5870,  0.4608],\n",
       "         [ 1.1515,  0.1383,  2.3117,  ...,  0.0790,  1.7844,  0.3575]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6565,  0.0532,  2.7899,  ..., -0.2810,  1.5710, -0.0307],\n",
       "         [ 0.0946,  0.0324,  1.1654,  ...,  0.8326,  1.0507,  1.2831],\n",
       "         [ 0.0946,  0.0324,  1.1654,  ...,  0.8326,  1.0507,  1.2831],\n",
       "         ...,\n",
       "         [ 1.4984,  0.0151,  2.7506,  ..., -0.3176,  1.5722, -0.0498],\n",
       "         [ 0.2885,  0.2096,  1.1801,  ...,  0.2542,  1.4810,  0.9409],\n",
       "         [ 1.4984,  0.0151,  2.7506,  ..., -0.3176,  1.5722, -0.0498]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6588, -0.0349,  2.8040,  ..., -0.2643,  1.4387, -0.0344],\n",
       "         [ 0.2082,  0.3299,  1.5038,  ...,  0.0980,  0.2999,  0.8529],\n",
       "         [ 0.1480,  0.1865,  1.4167,  ...,  0.1868,  0.4095,  1.1553],\n",
       "         ...,\n",
       "         [ 1.3262,  0.1371,  2.9605,  ..., -0.3110,  2.2221,  0.2168],\n",
       "         [-0.0168,  0.3340,  1.5922,  ...,  0.6361,  0.6408,  1.0927],\n",
       "         [ 1.7745, -0.0989,  2.3464,  ...,  0.1030,  0.7696,  0.9067]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.8389, -0.0833,  2.8819,  ..., -0.3653,  1.6418,  0.0051],\n",
       "         [ 0.3110,  0.6093,  2.0155,  ..., -0.5109,  1.4877,  0.7587],\n",
       "         [ 0.9082,  0.1907,  2.1231,  ...,  0.1166,  1.2729,  0.8662],\n",
       "         ...,\n",
       "         [ 1.8519, -0.3313,  2.3590,  ...,  0.0117,  0.9911,  0.9043],\n",
       "         [ 1.7254, -0.0863,  2.3804,  ...,  0.0361,  1.0835,  0.9581],\n",
       "         [ 1.8764, -0.1411,  2.4367,  ..., -0.0229,  0.9655,  0.8701]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.3365,  0.5315,  2.5209,  ..., -1.0906,  2.4199, -0.4969],\n",
       "         [ 0.9591, -0.0641,  1.7740,  ..., -0.2905,  1.6158,  0.4406],\n",
       "         [ 0.4305,  0.6084,  1.6875,  ..., -1.4626,  2.0898, -0.3347],\n",
       "         ...,\n",
       "         [ 1.0179, -0.1546,  2.1600,  ...,  0.7352,  1.6894,  0.9289],\n",
       "         [ 1.2282, -0.2580,  2.2322,  ...,  0.7878,  1.8192,  0.9020],\n",
       "         [ 1.0873, -0.1973,  1.9204,  ...,  0.5861,  1.4998,  1.0603]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9481,  0.5146,  2.6599,  ..., -0.9588,  1.9266,  0.1821],\n",
       "         [-0.8710,  0.8676,  1.3189,  ...,  0.5088,  0.3754,  1.3677],\n",
       "         [-1.3057,  1.3222,  0.7037,  ..., -0.5968,  0.5515,  0.9764],\n",
       "         ...,\n",
       "         [ 1.0575, -0.2053,  2.6741,  ...,  0.0520,  1.5055,  0.8401],\n",
       "         [ 1.0814, -0.2726,  2.7533,  ...,  0.0844,  1.5442,  0.8107],\n",
       "         [ 1.3272, -0.4621,  2.4611,  ..., -0.0428,  1.5308,  0.7663]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 1.6402, -0.0139,  2.7859,  ..., -0.2651,  1.5792, -0.0077],\n",
       "         [ 0.1572,  0.0875,  0.8495,  ...,  0.0111,  1.3957,  0.8520],\n",
       "         [ 0.1572,  0.0875,  0.8495,  ...,  0.0111,  1.3957,  0.8520],\n",
       "         ...,\n",
       "         [ 1.0481, -0.2610,  2.6476,  ...,  0.0519,  1.5092,  0.8602],\n",
       "         [ 1.0814, -0.2726,  2.7533,  ...,  0.0844,  1.5442,  0.8107],\n",
       "         [ 1.3965, -0.5333,  2.4764,  ...,  0.0368,  1.5384,  0.8662]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_edge_grouped = group_edge_rep(moltree, emb['bond_from_atom'],batch_graph)\n",
    "emb_edge_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "9badaa43-c97d-4819-a7ee-fbc20620b1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([501, 100])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['bond_from_atom'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ea36c4a9-c861-40f3-8940-f664f1939845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 100])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_edgegrouped[6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "1f54f6ec-8277-4ab5-8faf-183fd727a76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,  46],\n",
       "        [ 47,  68],\n",
       "        [115,  54],\n",
       "        [169,  44],\n",
       "        [213,  38],\n",
       "        [251,  76],\n",
       "        [327,  44],\n",
       "        [371,  38],\n",
       "        [409,  60],\n",
       "        [469,  32]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "e15f971a-d5e3-4cfe-b5d2-3a0e141cffbd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-8.5820e-01,  1.6547e+00,  9.4280e-01,  8.4202e-01, -1.9089e-01,\n",
       "          -5.1609e-01,  8.8267e-02,  2.9910e-02,  2.4479e-01, -9.5501e-02,\n",
       "           1.0021e+00,  8.2473e-01, -1.1580e+00,  7.7930e-01, -9.4704e-01,\n",
       "           1.4190e+00,  9.8389e-01, -1.0539e+00,  2.8490e-01,  1.2800e+00,\n",
       "          -1.5545e+00,  7.8484e-01,  7.0774e-01,  1.9000e+00, -9.9584e-01,\n",
       "           1.5108e+00, -9.1161e-01,  1.5985e+00, -5.7780e-01,  1.0407e+00,\n",
       "          -5.9209e-01,  1.9629e+00, -3.4021e-01,  9.2519e-01, -3.4118e-01,\n",
       "          -2.5694e-01,  5.4684e-01, -1.2189e+00, -1.3635e+00,  3.2851e-02,\n",
       "          -7.1505e-01,  1.6838e-01,  2.8715e-02,  7.5729e-01, -1.1052e+00,\n",
       "          -6.0632e-01,  8.8837e-02,  6.8328e-01, -2.6477e+00, -5.6554e-03,\n",
       "           6.6163e-01, -9.7289e-01, -1.0368e+00,  8.4469e-01, -7.1414e-01,\n",
       "           5.7456e-01, -7.3513e-01, -1.2843e-01, -2.7095e-01,  4.1770e-01,\n",
       "          -9.9255e-01,  7.8305e-01, -2.0609e+00,  7.2598e-01,  4.0311e-01,\n",
       "          -2.7848e-01,  1.1140e+00, -1.9435e+00,  2.9807e-02,  8.3986e-01,\n",
       "          -3.6144e-01, -6.0413e-01, -1.8217e-01,  6.9692e-01, -6.2891e-01,\n",
       "           1.7527e+00, -2.1254e+00, -2.8392e-01, -4.0289e-01,  1.4487e+00,\n",
       "           1.4880e-01, -7.0063e-01,  1.3899e+00,  3.2674e-01, -2.6596e-01,\n",
       "          -2.3498e+00, -6.0381e-01, -2.6732e-01, -8.8608e-01,  8.4300e-01,\n",
       "          -7.7618e-01, -7.5188e-01,  1.4758e+00, -1.7856e+00,  9.6581e-01,\n",
       "          -6.6265e-01,  1.9928e-01,  2.8294e-01, -1.7343e-04,  1.7866e+00],\n",
       "         [ 3.8738e-01,  4.4443e-01,  5.1584e-01, -7.9318e-01, -9.9477e-02,\n",
       "           1.8988e+00,  7.1346e-02,  4.3122e-01,  1.1980e-01, -8.8377e-02,\n",
       "           1.1404e+00,  1.1213e-01, -1.4516e+00, -3.9193e-01, -1.7037e+00,\n",
       "           1.9872e+00,  3.1660e-01, -1.2831e+00,  3.7135e-01,  7.8207e-01,\n",
       "          -6.7328e-01,  4.0565e-01,  6.3893e-01, -3.2062e-01,  3.4728e-01,\n",
       "           9.9377e-01, -1.3887e+00,  2.6830e+00, -7.4201e-01, -4.7882e-01,\n",
       "           2.5153e-01,  7.7607e-01,  8.2906e-02,  8.7805e-01,  4.6088e-03,\n",
       "          -1.1470e+00, -2.0235e-01, -1.6441e+00, -3.3053e-01, -8.9367e-02,\n",
       "          -6.0920e-01,  1.2943e+00,  5.5048e-01,  3.0567e-01, -1.9843e-01,\n",
       "          -2.6809e-01, -2.9853e-01,  3.8231e-01, -1.8688e+00,  1.4532e+00,\n",
       "           1.1019e+00, -1.1937e+00, -1.1249e+00,  1.1245e+00,  2.5212e-01,\n",
       "           1.8517e+00, -7.9193e-01,  6.2734e-01, -1.2054e+00,  7.7575e-01,\n",
       "          -8.4056e-01,  3.4060e-01, -4.0139e-01,  5.0984e-01, -2.2883e-01,\n",
       "          -1.1122e+00,  2.6329e-01, -1.3707e+00, -6.7545e-01,  1.0172e+00,\n",
       "          -7.3995e-01, -8.7038e-02, -2.6219e-01, -2.4477e-01, -2.7202e-01,\n",
       "           5.4972e-01, -7.1447e-01, -3.7361e-02, -2.6222e-01,  2.4688e+00,\n",
       "           1.5361e+00,  5.1962e-02,  1.0850e+00,  3.9727e-01, -1.1397e+00,\n",
       "          -1.4944e+00, -1.5286e-01, -9.2288e-01,  1.6666e+00, -1.7906e+00,\n",
       "          -2.4672e+00, -1.1447e+00,  1.5466e+00, -1.7248e+00,  1.1831e+00,\n",
       "           3.5515e-01,  7.3421e-02, -1.1351e+00,  6.0031e-01,  6.0403e-01]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward>),\n",
       " tensor([[ 0.6599,  0.9159,  0.4179, -0.5326,  0.0775,  1.4185,  0.4286,  0.5351,\n",
       "           0.0290,  0.1077,  0.9937,  0.2461, -1.7849,  0.0914, -1.9364,  1.9959,\n",
       "           0.5665, -1.3083,  0.0750,  0.8515, -0.2101, -0.1155,  0.7573, -0.4392,\n",
       "           0.2370,  1.1982, -2.0473,  2.0568, -1.0875,  0.0600,  0.1889,  0.8403,\n",
       "           0.0925,  0.7463, -0.3985, -0.8505, -0.6498, -1.6066, -0.7291,  0.4007,\n",
       "          -0.2454,  1.0108,  0.7307,  0.2712, -0.3383,  0.0423, -0.0631,  0.6159,\n",
       "          -2.0499,  1.2861,  0.6133, -0.9226, -1.0172,  1.0567, -0.1113,  1.6893,\n",
       "          -0.9975,  0.6870, -1.2168,  1.1945, -0.6593,  0.2155, -0.7201,  0.5568,\n",
       "          -0.2059, -1.2387,  0.0835, -1.5493, -0.4600,  0.8626, -0.7488,  0.2698,\n",
       "          -0.3928, -0.2592,  0.0857,  0.4873, -1.0943,  0.4554, -0.4695,  2.2190,\n",
       "           1.0032,  0.0534,  0.9996,  0.3676, -1.5134, -1.8193, -0.3733, -0.9461,\n",
       "           1.8290, -1.0387, -2.1277, -1.3722,  1.8773, -1.2578,  1.8909,  0.5118,\n",
       "          -0.2507, -0.9942,  1.0018,  0.1935],\n",
       "         [-0.4223,  1.2414,  1.1390, -0.2158, -0.0688,  1.1340, -0.3790,  0.0048,\n",
       "          -0.3671, -0.0258,  1.2344,  0.4921, -1.3983, -0.0873, -1.3319,  1.6356,\n",
       "           0.4516, -1.5965,  0.4373,  1.4518, -1.3321,  0.5724,  0.3127,  0.8327,\n",
       "           0.3135,  1.3064, -0.7022,  2.6513, -0.1168,  0.3202, -0.0300,  1.7108,\n",
       "          -0.1336,  1.6041, -0.0181, -0.8434,  0.3138, -1.3881, -0.6085, -1.0297,\n",
       "          -1.0038,  0.3582, -0.1308,  0.5749, -0.8985, -0.5940, -0.3002,  0.1658,\n",
       "          -2.1109,  0.8338,  1.5142, -1.4202, -1.5729,  1.5994, -0.6406,  1.3370,\n",
       "          -0.3548,  0.4552, -0.3259,  0.5241, -0.5876,  0.8202, -1.5225,  0.5520,\n",
       "           0.3611, -1.0029,  0.8024, -1.6005, -0.1688,  0.6427, -0.7244, -0.8212,\n",
       "           0.5745, -0.0498, -0.3319,  1.6820, -1.1353, -0.3279, -0.2736,  1.6614,\n",
       "           1.2918, -0.2822,  0.7731,  0.5863, -0.8888, -1.6760, -0.0493, -0.4957,\n",
       "           0.2399, -0.8594, -1.7670, -0.3412,  1.6419, -2.3836,  0.8015, -0.8159,\n",
       "           0.1608, -0.8046, -0.1231,  1.3669]], device='cuda:0',\n",
       "        grad_fn=<SliceBackward>),\n",
       " tensor([[ 0.2704,  0.6303, -0.0518, -0.4976,  0.2454,  1.6955,  0.2292,  0.4622,\n",
       "          -0.1679, -0.1297,  1.0143,  0.0746, -1.1661, -0.2680, -1.6454,  1.7409,\n",
       "           0.3834, -1.4740,  0.6220,  0.9704, -0.7724,  0.2426,  0.5195,  0.1678,\n",
       "           0.2072,  1.0955, -1.2922,  2.4692, -0.7652, -0.2223,  0.5342,  1.2051,\n",
       "          -0.0486,  1.2813, -0.1865, -0.8717,  0.1455, -1.5859, -0.5502, -0.1984,\n",
       "          -0.3255,  1.0096,  0.4601,  0.0438, -0.0848, -0.2793, -0.2167,  0.6569,\n",
       "          -2.2039,  1.5634,  1.2388, -1.0107, -1.2866,  0.8378, -0.1775,  1.7056,\n",
       "          -0.5956,  0.5966, -1.0128,  0.8579, -0.5993,  0.3442, -0.7349,  0.2401,\n",
       "           0.0781, -1.0044,  0.2499, -1.5078, -0.8859,  0.7636, -0.9963, -0.1804,\n",
       "          -0.3232, -0.6040, -0.1135,  0.7260, -0.9568,  0.3756, -0.1963,  2.3539,\n",
       "           1.8444, -0.2173,  1.1548,  0.5975, -1.9735, -1.4872,  0.0632, -1.1540,\n",
       "           1.5248, -1.1278, -2.3131, -1.4278,  1.6674, -1.7922,  0.9819,  0.3121,\n",
       "          -0.2183, -0.9373,  0.7904,  0.5946],\n",
       "         [-0.3362,  0.8336,  1.9023, -0.8884, -0.7317,  0.9140, -0.3136,  0.8335,\n",
       "           0.0044,  0.3725,  1.1465,  1.0397, -1.7792, -0.0085, -1.1839,  1.6576,\n",
       "           1.0080, -0.5536,  0.0235,  0.8652, -0.9902,  0.8525,  1.1028,  0.2076,\n",
       "          -0.2642,  1.0142, -1.2512,  2.9066, -1.0212,  0.0880, -0.8263,  1.0862,\n",
       "           0.4284,  0.3569,  0.0641, -0.5894,  0.0200, -1.9469, -0.4728, -0.0472,\n",
       "          -1.5914,  0.4294,  0.1163,  0.8592, -0.7681, -0.4236, -0.0870,  0.3197,\n",
       "          -2.3226,  0.4064,  0.2401, -1.3679, -1.1166,  1.2570, -0.8492,  1.2830,\n",
       "           0.0725, -0.0178, -0.6734,  0.4511, -1.2723,  0.8437, -0.4593,  1.5929,\n",
       "          -0.8341, -0.9123,  1.3075, -1.6722, -0.1460,  0.6906, -0.1577, -0.2843,\n",
       "          -0.0890,  1.0723, -0.7980,  1.4359, -1.4271, -0.1250, -0.3581,  1.7574,\n",
       "          -0.2871,  0.0429,  1.5534,  0.4736,  0.2228, -1.6576, -0.8569,  0.1037,\n",
       "           0.3975, -0.4746, -1.4242, -1.3584,  1.5324, -2.0638,  0.4961,  0.2514,\n",
       "           0.2169, -0.8108,  0.2221,  1.5149]], device='cuda:0',\n",
       "        grad_fn=<SliceBackward>),\n",
       " tensor([[-4.6019e-01,  1.4429e+00,  1.1170e+00, -2.4974e-01, -1.6703e-01,\n",
       "           1.0457e+00, -1.8714e-01,  6.5309e-01, -1.0530e-01, -5.0114e-02,\n",
       "           1.3076e+00,  2.7833e-01, -1.1338e+00, -4.7216e-02, -1.4877e+00,\n",
       "           1.5326e+00,  1.0725e+00, -1.2915e+00,  3.3933e-01,  1.1403e+00,\n",
       "          -1.3146e+00,  1.0735e+00,  9.2569e-01,  8.8062e-01, -4.3286e-01,\n",
       "           1.2915e+00, -1.0312e+00,  2.4384e+00, -3.4508e-01,  2.1540e-01,\n",
       "          -2.7849e-01,  2.0805e+00,  3.7560e-02,  1.2535e+00, -3.3209e-01,\n",
       "          -5.3622e-01,  7.4635e-01, -1.3439e+00, -8.6428e-01, -3.6571e-01,\n",
       "          -9.9162e-01,  4.4643e-01, -1.1963e-01, -5.7403e-02, -8.4516e-01,\n",
       "          -6.4962e-01, -1.9710e-01,  4.5833e-01, -2.5146e+00,  9.4910e-01,\n",
       "           1.0569e+00, -1.2279e+00, -1.3644e+00,  1.3410e+00, -9.7688e-03,\n",
       "           1.2460e+00, -8.1920e-02,  1.7338e-01, -4.8556e-01,  3.0650e-01,\n",
       "          -7.4251e-01,  4.3097e-01, -1.2750e+00,  9.6627e-01,  9.0356e-02,\n",
       "          -1.2060e+00,  8.3985e-01, -1.9057e+00, -6.0660e-01,  4.9723e-01,\n",
       "          -7.3456e-01, -4.9562e-01, -1.2761e-01,  1.5122e-01, -7.0961e-01,\n",
       "           1.7818e+00, -1.0709e+00, -1.9550e-01, -4.4587e-01,  1.9137e+00,\n",
       "           1.2644e+00, -5.4238e-01,  1.4282e+00,  5.1102e-01, -1.1339e+00,\n",
       "          -1.6721e+00, -2.1113e-01, -7.9059e-01,  2.1487e-01, -4.1566e-01,\n",
       "          -1.6767e+00, -1.1792e+00,  1.1855e+00, -2.1739e+00,  7.6402e-01,\n",
       "          -1.2155e-01,  1.1268e-01, -6.9185e-01,  3.6125e-01,  1.3298e+00],\n",
       "         [ 1.4102e+00,  7.1734e-01,  8.1996e-02,  2.9231e-01, -4.9036e-02,\n",
       "           2.5405e+00,  1.7197e-01, -3.4630e-01, -3.8240e-01,  2.8961e-01,\n",
       "           1.7332e-01,  4.8793e-01, -1.0303e+00,  2.0027e-03, -1.4907e+00,\n",
       "           1.2632e+00, -3.9055e-01, -2.3007e+00,  2.9117e-01,  2.0988e+00,\n",
       "          -4.1606e-01, -5.4476e-01,  5.4096e-01, -1.8382e-01,  7.2108e-01,\n",
       "           3.9423e-01, -7.2875e-01,  2.7199e+00,  4.6915e-02, -8.6064e-01,\n",
       "           4.3384e-01,  1.5826e+00,  7.4978e-02,  9.2353e-01, -5.1749e-01,\n",
       "          -1.1797e+00, -5.4290e-01, -1.0046e+00, -2.6126e-01, -7.7785e-01,\n",
       "          -1.2486e-01,  5.9298e-01,  4.7024e-02,  1.0020e+00, -5.9251e-01,\n",
       "           9.9564e-02, -7.0972e-01,  2.0455e-01, -2.1682e+00,  1.1665e+00,\n",
       "           1.4601e+00, -6.0737e-01, -1.3329e+00,  6.3893e-01, -1.3315e+00,\n",
       "           1.4085e+00, -3.3917e-01,  6.2864e-01, -9.1388e-01,  1.3891e+00,\n",
       "           4.5509e-02,  4.9912e-01, -8.4372e-01,  4.1047e-01, -3.2198e-01,\n",
       "          -1.4838e+00, -1.7993e-01, -5.3024e-01, -7.6265e-01,  2.6483e-01,\n",
       "          -5.6407e-01, -3.0243e-01,  5.9169e-01, -1.4499e+00, -7.0992e-02,\n",
       "           2.8457e-01, -5.0560e-01, -6.2189e-01, -5.5619e-01,  1.4389e+00,\n",
       "           1.4699e+00, -1.0727e-01,  8.2756e-01,  1.2103e+00, -1.4317e+00,\n",
       "           3.6072e-01,  2.9572e-01, -5.8008e-01,  1.9784e+00, -1.7472e+00,\n",
       "          -2.0924e+00, -1.2620e-01,  1.2847e+00, -1.7375e+00,  3.1882e-01,\n",
       "          -1.1975e+00,  1.1523e+00, -1.1090e+00,  4.2185e-01,  6.9877e-01]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward>),\n",
       " tensor([[ 7.6498e-01,  2.7948e-01,  6.8428e-02, -9.2985e-02,  1.2601e-01,\n",
       "           2.5209e+00, -1.5417e-01, -3.0435e-01, -5.5438e-01, -4.8387e-02,\n",
       "           5.6613e-01,  1.5144e-01, -7.9453e-01, -4.5909e-01, -1.3620e+00,\n",
       "           1.4565e+00, -3.2990e-01, -1.9061e+00,  4.4651e-01,  1.5752e+00,\n",
       "          -7.7870e-01, -8.4992e-02,  4.4756e-01, -1.8867e-02,  9.0182e-01,\n",
       "           5.0279e-01, -5.9488e-01,  2.9264e+00, -3.7052e-01, -7.0881e-01,\n",
       "           6.3266e-01,  1.1433e+00,  8.0860e-02,  1.4851e+00, -1.9741e-01,\n",
       "          -1.3835e+00, -1.5783e-01, -1.2503e+00, -2.6679e-01, -9.8022e-01,\n",
       "          -2.5992e-01,  9.8478e-01,  3.0645e-01,  7.9413e-01, -2.4757e-01,\n",
       "          -7.7705e-02, -6.0203e-01,  2.9449e-01, -1.7752e+00,  1.4830e+00,\n",
       "           1.9473e+00, -9.1625e-01, -1.5063e+00,  7.3571e-01, -5.6404e-01,\n",
       "           1.6903e+00, -5.8863e-01,  6.2278e-01, -1.2540e+00,  9.4870e-01,\n",
       "          -4.3293e-01,  4.5693e-01, -6.6192e-01,  2.7329e-01, -1.0041e-01,\n",
       "          -1.0590e+00, -1.0412e-01, -8.6415e-01, -8.7360e-01,  6.2517e-01,\n",
       "          -7.1938e-01, -6.8049e-01,  1.1813e-01, -1.1022e+00,  3.7890e-02,\n",
       "           3.1147e-01, -5.0626e-01, -2.7805e-01, -2.7527e-01,  1.7064e+00,\n",
       "           2.1575e+00, -2.0054e-01,  7.9946e-01,  1.0333e+00, -1.3453e+00,\n",
       "          -5.1136e-01,  2.8317e-01, -7.1727e-01,  1.7497e+00, -1.8103e+00,\n",
       "          -2.3781e+00, -2.0893e-01,  1.4973e+00, -1.8481e+00,  4.3465e-01,\n",
       "          -7.3648e-01,  7.9092e-01, -1.1138e+00,  2.0546e-01,  7.5381e-01],\n",
       "         [ 4.6656e-01,  1.7735e+00,  9.3768e-01,  5.6262e-01, -3.5248e-01,\n",
       "           1.4599e+00, -9.6355e-02, -2.2272e-01, -2.5880e-01,  5.2849e-01,\n",
       "           3.3485e-01,  8.3656e-01, -1.4970e+00,  3.7745e-01, -1.4214e+00,\n",
       "           1.2107e+00,  2.7749e-02, -1.8069e+00,  3.0143e-01,  2.1250e+00,\n",
       "          -7.4088e-01, -1.5495e-03,  5.5832e-01,  4.7417e-01,  1.9749e-01,\n",
       "           1.0648e+00, -5.1262e-01,  2.9216e+00,  1.1829e-01,  9.4183e-02,\n",
       "          -4.7331e-02,  1.7981e+00,  2.9459e-02,  7.3850e-01, -1.5679e-01,\n",
       "          -6.5196e-01, -1.8441e-01, -5.5559e-01, -1.0617e+00, -5.1348e-01,\n",
       "          -6.5181e-01,  4.1912e-02, -1.2387e-01,  1.0503e+00, -1.3053e+00,\n",
       "          -6.9547e-01, -7.2514e-01,  4.4118e-01, -2.6618e+00,  5.5604e-01,\n",
       "           1.2702e+00, -8.5101e-01, -1.8069e+00,  1.0953e+00, -1.3884e+00,\n",
       "           1.0096e+00, -2.9195e-01,  1.6293e-01, -1.5808e-01,  1.3625e+00,\n",
       "          -1.3336e-01,  8.8722e-01, -1.4577e+00,  6.3705e-01,  1.3978e-01,\n",
       "          -1.2530e+00,  7.0193e-01, -1.0303e+00, -6.8123e-02,  2.2675e-01,\n",
       "          -3.7924e-01, -8.3725e-01,  8.2705e-01, -8.6183e-01, -7.1606e-01,\n",
       "           1.4123e+00, -1.2563e+00, -8.2211e-01, -6.4968e-01,  1.6926e+00,\n",
       "           5.1087e-01, -2.8889e-01,  1.2210e+00,  9.0284e-01, -9.4389e-01,\n",
       "          -4.9965e-01, -2.7906e-01, -4.4786e-01,  5.9846e-01, -9.1982e-01,\n",
       "          -1.4847e+00, -3.3871e-01,  1.4874e+00, -2.2159e+00,  5.8543e-01,\n",
       "          -1.5108e+00,  1.0148e+00, -8.3428e-01, -8.9537e-02,  1.2870e+00]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward>),\n",
       " tensor([[ 8.5455e-01,  3.7376e-01,  3.6412e-01, -1.0846e-01, -1.7872e-02,\n",
       "           2.4337e+00, -1.6960e-01, -2.4699e-01, -4.6257e-01,  1.1657e-01,\n",
       "           4.7331e-01,  3.7038e-01, -7.5842e-01, -5.5123e-01, -1.3040e+00,\n",
       "           1.6709e+00, -3.6563e-01, -2.0291e+00,  3.5443e-01,  1.7830e+00,\n",
       "          -7.9775e-01, -2.4218e-01,  1.3263e-01,  1.7152e-02,  1.0313e+00,\n",
       "           4.7482e-01, -5.7759e-01,  3.0224e+00, -8.8008e-02, -7.9506e-01,\n",
       "           8.0408e-01,  1.2096e+00,  1.1786e-01,  1.1738e+00, -2.3473e-03,\n",
       "          -1.2762e+00, -3.6690e-01, -1.2495e+00, -5.0909e-01, -1.0386e+00,\n",
       "          -2.1202e-01,  8.6727e-01,  3.7321e-01,  1.1707e+00, -2.7076e-01,\n",
       "          -2.9938e-01, -6.5000e-01,  2.5155e-01, -1.9206e+00,  1.3059e+00,\n",
       "           2.0623e+00, -8.7975e-01, -1.5105e+00,  8.1046e-01, -7.7781e-01,\n",
       "           1.5214e+00, -6.4081e-01,  5.9044e-01, -9.8474e-01,  1.0454e+00,\n",
       "          -3.1020e-01,  5.2408e-01, -5.2193e-01,  7.4488e-02, -9.7815e-02,\n",
       "          -1.1604e+00,  2.7767e-02, -6.3752e-01, -5.2348e-01,  6.5111e-01,\n",
       "          -7.4488e-01, -7.8681e-01,  4.3808e-01, -1.1681e+00, -2.4741e-02,\n",
       "           3.6665e-01, -5.7482e-01, -5.5922e-01, -3.7375e-01,  1.6655e+00,\n",
       "           1.7825e+00, -1.1339e-01,  6.4255e-01,  9.1588e-01, -1.1372e+00,\n",
       "          -5.1304e-01,  4.1151e-02, -5.8930e-01,  1.5663e+00, -1.8183e+00,\n",
       "          -2.2734e+00, -5.8299e-02,  1.5530e+00, -1.9880e+00,  4.8853e-01,\n",
       "          -1.0474e+00,  9.1772e-01, -1.0931e+00,  3.0442e-02,  7.5573e-01],\n",
       "         [ 9.3156e-01,  7.3918e-01, -4.3116e-01,  2.0358e-01,  9.3909e-02,\n",
       "           2.3892e+00, -2.1069e-01, -3.2819e-01, -6.8528e-01, -1.3066e-02,\n",
       "           5.9258e-01,  1.7923e-01, -1.0454e+00,  2.2905e-01, -1.4281e+00,\n",
       "           1.5267e+00, -4.4645e-01, -2.1275e+00,  3.7453e-01,  1.7136e+00,\n",
       "          -4.8082e-01, -2.6361e-01,  5.7458e-01,  3.0898e-01,  7.0799e-01,\n",
       "           7.7429e-01, -7.4864e-01,  2.6500e+00, -1.1757e-01, -2.6180e-01,\n",
       "           6.4276e-01,  1.2977e+00, -1.0172e-01,  1.6018e+00, -6.1982e-01,\n",
       "          -1.4346e+00, -6.6223e-01, -9.0216e-01, -2.8568e-01, -1.0065e+00,\n",
       "          -1.3558e-01,  6.1360e-01,  1.2695e-01,  7.3912e-01, -4.2650e-01,\n",
       "           2.9477e-01, -4.5271e-01,  4.8018e-01, -1.7461e+00,  1.3638e+00,\n",
       "           1.8219e+00, -5.8325e-01, -1.3270e+00,  6.4694e-01, -1.1743e+00,\n",
       "           1.4131e+00, -4.4085e-01,  6.7258e-01, -1.1733e+00,  9.8380e-01,\n",
       "          -1.5232e-01,  4.0260e-01, -1.3225e+00,  3.1594e-01,  9.7253e-02,\n",
       "          -7.0563e-01, -6.8246e-01, -9.1664e-01, -1.0446e+00,  3.6480e-01,\n",
       "          -7.4847e-01, -7.6769e-01,  2.4064e-01, -1.4212e+00,  3.0368e-01,\n",
       "           3.0624e-01, -7.3812e-01, -6.6416e-02, -2.0905e-01,  1.3794e+00,\n",
       "           2.1293e+00, -3.7075e-01,  7.1001e-01,  1.0730e+00, -1.9288e+00,\n",
       "          -1.8754e-01,  8.1869e-01, -8.0821e-01,  1.9159e+00, -1.2851e+00,\n",
       "          -2.0420e+00, -7.6751e-02,  1.6879e+00, -1.6030e+00,  2.9032e-01,\n",
       "          -1.1728e+00,  5.2860e-01, -9.7312e-01,  3.4568e-01,  6.8618e-01]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward>),\n",
       " tensor([[ 2.5122e-01,  1.7062e+00,  9.6235e-01,  2.8058e-01, -2.4293e-01,\n",
       "           1.4751e+00, -3.5257e-01, -1.4104e-01, -4.3734e-01,  3.8349e-01,\n",
       "           6.1807e-01,  7.1085e-01, -1.5735e+00,  3.1349e-01, -1.4591e+00,\n",
       "           1.2525e+00,  1.5620e-01, -1.6743e+00,  2.9087e-01,  1.8682e+00,\n",
       "          -8.9165e-01,  9.2807e-02,  5.1777e-01,  3.3060e-01,  4.3431e-01,\n",
       "           1.2769e+00, -6.8142e-01,  2.9712e+00, -3.7843e-02,  2.8880e-01,\n",
       "          -1.0532e-01,  1.6615e+00, -1.9678e-01,  1.2041e+00, -1.7541e-01,\n",
       "          -8.1132e-01, -1.1067e-01, -8.1646e-01, -9.0102e-01, -6.3858e-01,\n",
       "          -6.7343e-01,  1.4755e-01, -1.5249e-01,  1.0375e+00, -1.2421e+00,\n",
       "          -5.0683e-01, -6.9065e-01,  3.9795e-01, -2.5579e+00,  7.7357e-01,\n",
       "           1.3460e+00, -9.0941e-01, -1.7389e+00,  1.2661e+00, -1.3220e+00,\n",
       "           1.1259e+00, -2.9763e-01,  1.9719e-01, -3.0075e-01,  1.2356e+00,\n",
       "          -3.5452e-01,  8.3412e-01, -1.4444e+00,  7.2005e-01,  2.5325e-01,\n",
       "          -1.1579e+00,  6.7799e-01, -1.1710e+00, -1.1464e-01,  3.1299e-01,\n",
       "          -4.0797e-01, -7.4550e-01,  6.1791e-01, -7.0602e-01, -5.5508e-01,\n",
       "           1.3635e+00, -1.1923e+00, -5.6703e-01, -5.6383e-01,  1.6591e+00,\n",
       "           7.4162e-01, -2.8362e-01,  1.1347e+00,  9.2684e-01, -1.0525e+00,\n",
       "          -8.3757e-01, -9.9024e-02, -4.0159e-01,  5.9338e-01, -9.6060e-01,\n",
       "          -1.6171e+00, -3.1945e-01,  1.6765e+00, -2.2636e+00,  6.3213e-01,\n",
       "          -1.3315e+00,  8.0297e-01, -9.3257e-01, -6.7658e-02,  1.2649e+00],\n",
       "         [ 2.9577e-01,  1.6804e+00,  8.9375e-01,  4.1553e-01, -2.9220e-01,\n",
       "           1.5550e+00, -4.2946e-01, -7.9683e-02, -3.6488e-01,  3.5493e-01,\n",
       "           6.0479e-01,  6.5412e-01, -1.5453e+00,  3.3411e-01, -1.4463e+00,\n",
       "           1.2919e+00,  1.2169e-01, -1.6770e+00,  3.3358e-01,  1.9240e+00,\n",
       "          -8.5652e-01,  1.3653e-01,  5.2349e-01,  4.9098e-01,  3.7562e-01,\n",
       "           1.1651e+00, -5.2756e-01,  2.9173e+00,  5.3467e-04,  1.9908e-01,\n",
       "          -4.3048e-02,  1.7131e+00, -4.1565e-02,  1.2530e+00, -1.0992e-01,\n",
       "          -8.9966e-01, -1.7137e-01, -6.7166e-01, -9.2980e-01, -6.4229e-01,\n",
       "          -7.7818e-01,  1.5024e-01, -2.4025e-01,  1.0138e+00, -1.1937e+00,\n",
       "          -4.5331e-01, -6.4363e-01,  3.8044e-01, -2.4790e+00,  7.7370e-01,\n",
       "           1.5158e+00, -1.0609e+00, -1.7514e+00,  1.2359e+00, -1.3026e+00,\n",
       "           1.1270e+00, -2.1267e-01,  2.8103e-01, -3.4929e-01,  1.0862e+00,\n",
       "          -3.4530e-01,  8.6612e-01, -1.5970e+00,  7.0219e-01,  3.2938e-01,\n",
       "          -1.1071e+00,  5.4372e-01, -1.1793e+00, -2.5354e-01,  2.8300e-01,\n",
       "          -4.4127e-01, -8.5808e-01,  5.3863e-01, -7.9357e-01, -5.6415e-01,\n",
       "           1.3850e+00, -1.1895e+00, -6.1632e-01, -5.2659e-01,  1.6228e+00,\n",
       "           8.6986e-01, -3.9344e-01,  1.1177e+00,  9.1730e-01, -9.9388e-01,\n",
       "          -7.5555e-01, -1.4886e-02, -4.2829e-01,  5.2147e-01, -9.3190e-01,\n",
       "          -1.6075e+00, -2.8105e-01,  1.6737e+00, -2.2510e+00,  4.4956e-01,\n",
       "          -1.4217e+00,  7.5976e-01, -8.9154e-01, -9.6008e-02,  1.3528e+00]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward>),\n",
       " tensor([[ 4.6656e-01,  1.7735e+00,  9.3768e-01,  5.6262e-01, -3.5248e-01,\n",
       "           1.4599e+00, -9.6355e-02, -2.2272e-01, -2.5880e-01,  5.2849e-01,\n",
       "           3.3485e-01,  8.3656e-01, -1.4970e+00,  3.7745e-01, -1.4214e+00,\n",
       "           1.2107e+00,  2.7749e-02, -1.8069e+00,  3.0143e-01,  2.1250e+00,\n",
       "          -7.4088e-01, -1.5495e-03,  5.5832e-01,  4.7417e-01,  1.9749e-01,\n",
       "           1.0648e+00, -5.1262e-01,  2.9216e+00,  1.1829e-01,  9.4183e-02,\n",
       "          -4.7331e-02,  1.7981e+00,  2.9459e-02,  7.3850e-01, -1.5679e-01,\n",
       "          -6.5196e-01, -1.8441e-01, -5.5559e-01, -1.0617e+00, -5.1348e-01,\n",
       "          -6.5181e-01,  4.1912e-02, -1.2387e-01,  1.0503e+00, -1.3053e+00,\n",
       "          -6.9547e-01, -7.2514e-01,  4.4118e-01, -2.6618e+00,  5.5604e-01,\n",
       "           1.2702e+00, -8.5101e-01, -1.8069e+00,  1.0953e+00, -1.3884e+00,\n",
       "           1.0096e+00, -2.9195e-01,  1.6293e-01, -1.5808e-01,  1.3625e+00,\n",
       "          -1.3336e-01,  8.8722e-01, -1.4577e+00,  6.3705e-01,  1.3978e-01,\n",
       "          -1.2530e+00,  7.0193e-01, -1.0303e+00, -6.8123e-02,  2.2675e-01,\n",
       "          -3.7924e-01, -8.3725e-01,  8.2705e-01, -8.6183e-01, -7.1606e-01,\n",
       "           1.4123e+00, -1.2563e+00, -8.2211e-01, -6.4968e-01,  1.6926e+00,\n",
       "           5.1087e-01, -2.8889e-01,  1.2210e+00,  9.0284e-01, -9.4389e-01,\n",
       "          -4.9965e-01, -2.7906e-01, -4.4786e-01,  5.9846e-01, -9.1982e-01,\n",
       "          -1.4847e+00, -3.3871e-01,  1.4874e+00, -2.2159e+00,  5.8543e-01,\n",
       "          -1.5108e+00,  1.0148e+00, -8.3428e-01, -8.9537e-02,  1.2870e+00],\n",
       "         [ 1.2946e+00,  6.4231e-01, -2.8085e-01,  2.5481e-01, -9.4376e-02,\n",
       "           2.7422e+00, -1.3765e-01, -3.8629e-01, -6.0483e-01,  1.2504e-01,\n",
       "           3.1566e-01,  8.6873e-02, -9.5144e-01, -1.0965e-01, -1.5329e+00,\n",
       "           1.1808e+00, -3.3200e-01, -1.9455e+00,  3.4621e-01,  1.8223e+00,\n",
       "          -3.3564e-01, -2.7165e-01,  8.4836e-01, -1.6629e-02,  7.0912e-01,\n",
       "           3.7808e-01, -5.5207e-01,  2.6979e+00, -1.9517e-01, -8.0992e-01,\n",
       "           4.8343e-01,  1.3506e+00,  9.6821e-02,  1.3294e+00, -4.7170e-01,\n",
       "          -1.6143e+00, -4.4576e-01, -8.5425e-01, -2.0646e-01, -7.1420e-01,\n",
       "          -1.9429e-01,  8.5371e-01, -8.6567e-02,  6.1714e-01, -4.7548e-01,\n",
       "           4.8163e-01, -5.3043e-01,  2.7747e-01, -1.7531e+00,  1.3767e+00,\n",
       "           1.8380e+00, -7.8784e-01, -1.4310e+00,  5.8932e-01, -8.9136e-01,\n",
       "           1.5236e+00, -3.4800e-01,  7.4051e-01, -1.3184e+00,  1.0300e+00,\n",
       "          -3.2251e-01,  4.6497e-01, -1.1762e+00,  5.4819e-01, -1.7800e-01,\n",
       "          -1.0627e+00, -4.8026e-01, -7.0799e-01, -1.1500e+00,  4.2026e-01,\n",
       "          -5.8027e-01, -5.4210e-01,  8.0156e-02, -1.3822e+00,  1.4074e-01,\n",
       "           1.9426e-01, -3.5876e-01, -3.9393e-01, -2.4994e-01,  1.5131e+00,\n",
       "           2.1344e+00, -2.9719e-01,  9.1453e-01,  1.1712e+00, -1.4184e+00,\n",
       "           2.2044e-01,  4.4562e-01, -6.7283e-01,  1.9472e+00, -1.7808e+00,\n",
       "          -2.1798e+00, -2.0852e-01,  1.3239e+00, -1.5683e+00,  1.0389e-01,\n",
       "          -1.0161e+00,  9.0877e-01, -1.0852e+00,  3.2682e-01,  6.0071e-01]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward>),\n",
       " tensor([[ 1.4300,  0.7130, -0.0625,  0.3993, -0.1202,  2.6043,  0.0982, -0.3168,\n",
       "          -0.3636,  0.2759,  0.1887,  0.4560, -0.9317, -0.0557, -1.4410,  1.1902,\n",
       "          -0.3887, -2.1754,  0.3692,  2.1967, -0.2712, -0.5040,  0.5386,  0.0207,\n",
       "           0.6093,  0.2285, -0.5903,  2.7182,  0.0777, -0.9334,  0.4292,  1.5800,\n",
       "           0.2438,  0.9706, -0.4847, -1.3164, -0.5045, -0.8368, -0.3280, -0.7929,\n",
       "          -0.1806,  0.6492, -0.1055,  0.9693, -0.5738,  0.1681, -0.6951,  0.1363,\n",
       "          -2.0766,  1.1902,  1.6373, -0.7114, -1.3547,  0.5121, -1.3124,  1.3664,\n",
       "          -0.2916,  0.7033, -1.0193,  1.2825,  0.0072,  0.5643, -1.0640,  0.4321,\n",
       "          -0.2559, -1.3946, -0.3194, -0.5330, -0.9142,  0.3155, -0.6036, -0.4376,\n",
       "           0.5260, -1.4988, -0.0909,  0.3155, -0.4501, -0.6684, -0.4850,  1.4352,\n",
       "           1.5825, -0.1896,  0.8670,  1.1808, -1.4322,  0.4628,  0.3916, -0.5918,\n",
       "           1.9216, -1.7341, -2.0019, -0.0983,  1.2393, -1.6647,  0.0452, -1.3253,\n",
       "           1.1642, -1.0686,  0.3826,  0.7750],\n",
       "         [ 1.1869,  0.6075, -0.2502, -0.2147, -0.2123,  2.7106, -0.2279, -0.3823,\n",
       "          -0.3307,  0.0152,  0.5003,  0.0064, -1.1092, -0.2194, -1.5898,  1.6322,\n",
       "           0.0328, -1.7004,  0.2938,  1.2563, -0.3719,  0.1150,  0.6323,  0.1340,\n",
       "           0.5642,  0.4746, -0.3942,  2.6763, -0.2377, -0.7780,  0.3415,  1.1375,\n",
       "           0.2309,  1.4437, -0.2521, -1.6898, -0.7321, -0.8193, -0.1495, -0.3931,\n",
       "          -0.4264,  1.0025, -0.0962,  0.3190, -0.6361,  0.4426, -0.3873,  0.0213,\n",
       "          -1.7217,  1.3049,  2.0796, -1.4259, -1.2734,  0.9261, -0.8007,  1.8994,\n",
       "          -0.3562,  0.7391, -1.1172,  0.7660, -0.5806,  0.4802, -1.4681,  0.4350,\n",
       "           0.2012, -0.9843, -0.1367, -0.6802, -1.2038,  0.4972, -0.9051, -0.1415,\n",
       "           0.1840, -1.0558,  0.2636,  0.4124, -0.1917, -0.5720,  0.0093,  1.4382,\n",
       "           2.1070, -0.4154,  0.6926,  0.9755, -1.2709, -0.2354,  0.6104, -0.6903,\n",
       "           1.9152, -1.7205, -2.1219, -0.2523,  1.5753, -1.6002,  0.0666, -0.9792,\n",
       "           0.8161, -1.4990,  0.1926,  0.6355]], device='cuda:0',\n",
       "        grad_fn=<SliceBackward>),\n",
       " tensor([[ 1.4583,  0.8146,  0.0119,  0.3040, -0.2127,  2.6938, -0.1555, -0.3206,\n",
       "          -0.5598,  0.2029,  0.2418,  0.2443, -0.9146, -0.1703, -1.4739,  1.3967,\n",
       "          -0.3275, -1.9508,  0.2872,  2.0220, -0.2740, -0.4176,  0.5939, -0.0203,\n",
       "           0.7554,  0.2931, -0.4742,  2.7608,  0.0547, -0.8837,  0.5498,  1.3980,\n",
       "           0.1593,  1.0352, -0.3695, -1.5641, -0.7003, -0.7997, -0.4503, -0.7651,\n",
       "          -0.2579,  0.6935, -0.0462,  0.9708, -0.4567,  0.2251, -0.6530,  0.2902,\n",
       "          -1.9731,  1.2637,  1.9079, -0.7183, -1.4190,  0.6010, -1.1043,  1.3350,\n",
       "          -0.3518,  0.6314, -1.0931,  1.0831, -0.1653,  0.4591, -0.9595,  0.4904,\n",
       "          -0.1136, -1.2609, -0.3441, -0.5382, -0.8952,  0.4008, -0.6201, -0.5431,\n",
       "           0.3763, -1.4560,  0.0525,  0.3045, -0.4541, -0.6301, -0.3890,  1.4271,\n",
       "           1.7366, -0.1874,  0.8853,  1.1430, -1.3066,  0.3671,  0.3415, -0.6668,\n",
       "           1.8884, -1.8177, -2.0503, -0.1881,  1.3914, -1.7389,  0.0512, -1.2903,\n",
       "           1.0477, -1.0702,  0.2742,  0.6468],\n",
       "         [ 0.9416,  0.8003, -0.3674,  0.1381,  0.1245,  2.3738, -0.1119, -0.3160,\n",
       "          -0.7964, -0.0705,  0.6095,  0.1591, -1.0714,  0.2640, -1.4628,  1.4539,\n",
       "          -0.3914, -2.0455,  0.3299,  1.6871, -0.4887, -0.2827,  0.6849,  0.0956,\n",
       "           0.7117,  0.8848, -0.8134,  2.6479, -0.1780, -0.1857,  0.5347,  1.2582,\n",
       "          -0.2553,  1.5812, -0.7121, -1.3767, -0.6866, -0.9797, -0.3153, -0.9583,\n",
       "          -0.1404,  0.5105,  0.2324,  0.7512, -0.4442,  0.2573, -0.5896,  0.5143,\n",
       "          -1.8647,  1.4491,  1.6380, -0.4047, -1.2794,  0.6427, -1.2224,  1.3668,\n",
       "          -0.4864,  0.5149, -1.1966,  1.0269, -0.1492,  0.3346, -1.1133,  0.4765,\n",
       "           0.0694, -0.8394, -0.5469, -0.9406, -1.0509,  0.2837, -0.7032, -0.6118,\n",
       "           0.3000, -1.3738,  0.3246,  0.3219, -0.7373,  0.0081, -0.2933,  1.4351,\n",
       "           1.9696, -0.2480,  0.8430,  1.1482, -2.0891, -0.0610,  0.8407, -0.8692,\n",
       "           2.0393, -1.3232, -1.9944, -0.2499,  1.7796, -1.6505,  0.3472, -0.9908,\n",
       "           0.5300, -0.9929,  0.4541,  0.6327]], device='cuda:0',\n",
       "        grad_fn=<SliceBackward>)]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_edge_grouped = group_edge_rep(moltree, emb['bond_from_atom'],batch_graph)\n",
    "emb_edge_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "02d17d65-d125-4476-9000-4f63efda2182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([501, 100])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['bond_from_atom'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e34c9a0f-d21d-46c5-8b0c-dc4854fac2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([231, 100])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['atom_from_atom'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "c68d35b8-b8cf-4332-8ec7-f6a1bdbec6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([491, 100])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['bond_from_atom'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "11e35869-ac88-4141-9b97-d31ba0adc1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "count_test=0\n",
    "for i in range(10):\n",
    "    count_test+=len(emb_edge_grouped[i])\n",
    "    \n",
    "print(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "99947d10-cf14-430c-92ad-3712d8be1d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(53.9152, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(15.8666, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 0.010101010091602802,\n",
       " 0.44711539149284363)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_model(moltree, emb_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b8ec8e29-1590-43d4-ac95-e8da60df4b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb['atom_from_bond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "029f7cca-f55e-4fdf-909a-e518fc7b5e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd5b5a90>\n",
      "1\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd598990>\n",
      "2\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd5cb890>\n",
      "3\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd5fb750>\n",
      "4\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd607090>\n",
      "5\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd4b2150>\n",
      "6\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd4b2b90>\n",
      "7\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd4b5b50>\n",
      "8\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd498350>\n",
      "9\n",
      "<grover.pretrain_motif.mol_tree.MolTree object at 0x7f0cbd49d210>\n"
     ]
    }
   ],
   "source": [
    "for mol_index, mol_tree in enumerate(mol_batch):\n",
    "    print(mol_index)\n",
    "    print(mol_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b7664e58-f8b7-475c-8ff7-5f1645ae932d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd5b5a90>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd598990>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd5cb890>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd5fb750>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd607090>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd4b2150>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd4b2b90>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd4b5b50>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd498350>,\n",
       " <grover.pretrain_motif.mol_tree.MolTree at 0x7f0cbd49d210>]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "513aef43-a2c1-4556-9b4a-b937b3daff91",
   "metadata": {},
   "source": [
    "##### 데이터 구조 및 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "61bde3f9-dab2-46c7-aa61-56b5bd401aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([496, 100])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['atom_from_atom'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "311af8c1-c809-4fde-b900-78b491e036e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([496, 324])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[\"av_task\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2a190b1-2fde-4608-b7da-4087b5823ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([531, 353])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[\"bv_task\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d137e2c0-9245-4ec4-8692-52f834780217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_graph[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d7c96f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a18d45fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_from_epoch is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.6085e-03, -3.0400e-03,  0.0000e+00,  ..., -5.4301e-03,\n",
      "          2.4351e-03,  3.8916e-02],\n",
      "        [ 7.1513e-01, -5.5678e-01,  2.3509e+00,  ..., -7.1710e-01,\n",
      "         -4.2909e-01,  4.4382e+00],\n",
      "        [ 8.0203e-01, -1.1722e+00, -2.2792e-01,  ..., -1.0905e+00,\n",
      "         -1.1718e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 9.9632e-02,  1.5563e+00,  8.1791e-02,  ..., -2.7661e-01,\n",
      "         -9.3305e-01, -5.8124e-01],\n",
      "        [-5.3853e-01,  6.1812e-01, -2.7733e-01,  ..., -9.3885e-01,\n",
      "         -9.6680e-01, -7.6313e-02],\n",
      "        [ 3.4529e-01,  2.7850e+00, -0.0000e+00,  ..., -2.7454e-01,\n",
      "         -4.0088e-01,  3.0151e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.1022, -0.0084,  0.0221,  ...,  0.0205, -0.0240, -0.0036],\n",
      "        [ 1.0024, -0.1991,  1.9776,  ..., -0.0144, -0.5643,  2.4008],\n",
      "        [ 1.5665,  0.5782,  0.2745,  ..., -0.8584, -1.9646, -0.4699],\n",
      "        ...,\n",
      "        [-0.0081, -0.3479, -0.0000,  ..., -0.4042, -0.9947,  0.4765],\n",
      "        [ 1.6408, -0.1646, -1.5888,  ..., -0.5236, -1.1869,  2.0677],\n",
      "        [ 3.5471,  1.9654, -0.0000,  ...,  0.5842, -0.1466,  3.4527]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-4.4031e-03,  6.4113e-04, -0.0000e+00,  ..., -1.5790e-03,\n",
      "         -2.7476e-04,  0.0000e+00],\n",
      "        [-4.2126e-01, -4.3792e-01,  2.2212e+00,  ..., -1.9410e-01,\n",
      "         -2.7101e-01,  0.0000e+00],\n",
      "        [-6.0819e-01, -3.1730e-01, -1.3173e-01,  ...,  2.6165e+00,\n",
      "         -2.6642e-02, -1.9738e-01],\n",
      "        ...,\n",
      "        [-0.0000e+00,  7.8174e-01,  2.5464e-01,  ..., -9.9424e-02,\n",
      "         -2.4622e-01, -3.7739e-01],\n",
      "        [-5.0065e-01,  9.8739e-01, -9.7391e-02,  ...,  6.0092e-01,\n",
      "          2.3603e-01, -7.7568e-02],\n",
      "        [-5.6471e-01,  2.1074e+00, -2.4351e-01,  ...,  8.6045e-01,\n",
      "          4.0221e-01,  2.2342e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0038, -0.0032,  0.0023,  ...,  0.0049,  0.0098, -0.0025],\n",
      "        [-0.0990, -0.0793, -0.1415,  ..., -0.2202, -0.2001, -0.1534],\n",
      "        [-0.5200,  0.7294,  1.4529,  ...,  0.3508, -0.3089,  1.3468],\n",
      "        ...,\n",
      "        [-0.0000,  0.3532, -0.0107,  ..., -0.2599,  0.0199, -0.3569],\n",
      "        [-0.2494,  0.4687, -0.0982,  ..., -0.3321,  0.5332, -0.2959],\n",
      "        [-0.1806,  0.3789, -0.3023,  ..., -0.1941,  0.0000, -0.1784]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 6.7876e-03, -3.1903e-03, -1.1118e-03,  ...,  4.1439e-02,\n",
      "          2.2910e-02,  1.9011e-03],\n",
      "        [-9.3464e-02, -7.9298e-02, -1.4318e-01,  ..., -2.1106e-01,\n",
      "         -1.9684e-01, -1.5044e-01],\n",
      "        [ 5.0004e-02,  0.0000e+00,  8.6089e-01,  ..., -4.5805e-01,\n",
      "         -0.0000e+00,  4.0490e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00,  1.9477e+00, -2.1061e-01,  ..., -4.7111e-01,\n",
      "          1.9495e+00,  3.0323e-01],\n",
      "        [-3.5719e-01,  1.8796e+00, -3.4982e-01,  ..., -2.0310e-01,\n",
      "          1.8730e+00,  1.8388e-01],\n",
      "        [-3.5740e-01,  1.0286e+00, -4.5449e-01,  ..., -1.6196e-01,\n",
      "          0.0000e+00, -9.4777e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.9658e-03, -1.2651e-05, -2.7200e-03,  ..., -2.7307e-03,\n",
      "          3.1365e-03, -1.1650e-03],\n",
      "        [-9.1399e-02, -7.3491e-02, -1.3988e-01,  ..., -2.1650e-01,\n",
      "         -1.9481e-01, -1.4687e-01],\n",
      "        [-9.6608e-02,  1.7386e+00,  9.4350e-01,  ..., -1.9356e-01,\n",
      "         -6.1867e-02,  1.5554e+00],\n",
      "        ...,\n",
      "        [-3.1750e-01,  8.2406e-01, -3.1362e-01,  ..., -5.3405e-02,\n",
      "          1.3861e+00, -1.7240e-01],\n",
      "        [-3.4643e-01,  0.0000e+00, -0.0000e+00,  ..., -2.5953e-02,\n",
      "          1.4637e+00, -1.6114e-01],\n",
      "        [-0.0000e+00,  8.6520e-01, -4.0621e-01,  ..., -6.0570e-02,\n",
      "          6.7842e-01, -3.2382e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 8.3255e-04,  2.0451e-02,  0.0000e+00,  ...,  9.8886e-03,\n",
      "          1.0885e-03, -2.3305e-03],\n",
      "        [ 1.1047e+00, -2.2252e-01,  2.3450e+00,  ..., -5.3288e-02,\n",
      "         -3.5528e-01,  4.1239e+00],\n",
      "        [ 0.0000e+00,  3.0026e+00, -4.2892e-01,  ...,  1.9716e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  3.1266e-03, -2.2618e-01,  ...,  3.3030e-01,\n",
      "         -3.7828e-01, -3.3865e-01],\n",
      "        [-2.7652e-01, -8.3380e-02,  2.9997e-01,  ..., -0.0000e+00,\n",
      "         -4.9345e-01, -2.2392e-01],\n",
      "        [-1.3117e-01, -1.8026e-01, -3.7552e-01,  ...,  6.7045e-01,\n",
      "         -3.6545e-01, -7.7064e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0128,  0.0466,  0.0203,  ...,  0.0269, -0.0104,  0.0096],\n",
      "        [-0.1447, -0.2752,  1.6996,  ..., -0.1559, -0.2528,  2.5280],\n",
      "        [-0.3315,  1.7283, -0.1638,  ...,  2.9053, -0.6408,  1.3242],\n",
      "        ...,\n",
      "        [-0.5940, -0.2227,  1.0849,  ..., -0.2287, -0.1754, -0.0964],\n",
      "        [-0.4158,  2.1306, -0.0593,  ..., -0.3797, -0.5349,  0.3224],\n",
      "        [-0.5330, -0.2483, -0.1342,  ..., -0.0414, -0.1637,  0.0999]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.6270e-03,  1.2879e-02, -0.0000e+00,  ..., -2.0352e-03,\n",
      "          0.0000e+00, -2.0281e-03],\n",
      "        [-1.3136e-01, -2.6031e-01,  0.0000e+00,  ..., -2.2950e-01,\n",
      "         -1.3860e-01,  2.8103e+00],\n",
      "        [-5.0705e-01,  2.4288e+00, -1.7285e-01,  ...,  0.0000e+00,\n",
      "         -1.6002e-01, -2.3755e-01],\n",
      "        ...,\n",
      "        [-2.7582e-01, -0.0000e+00,  4.2374e-01,  ...,  0.0000e+00,\n",
      "         -1.6576e-01, -4.3326e-01],\n",
      "        [-2.1163e-01,  3.9481e-01,  3.2578e-01,  ...,  1.6605e+00,\n",
      "         -2.4545e-01, -6.0229e-03],\n",
      "        [-3.3529e-01, -1.8404e-01, -2.2897e-01,  ...,  7.0052e-01,\n",
      "         -2.3993e-01,  1.9848e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.2404e-02,  1.8170e-02, -3.3524e-03,  ...,  7.7771e-03,\n",
      "          0.0000e+00,  1.3374e-02],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -2.1954e-01,\n",
      "         -2.0128e-01, -1.4741e-01],\n",
      "        [ 2.4180e+00,  9.9916e-01,  5.6964e-01,  ..., -6.9096e-02,\n",
      "         -2.7922e-01, -7.3281e-02],\n",
      "        ...,\n",
      "        [-5.6528e-02,  0.0000e+00,  2.8011e-01,  ..., -4.6248e-01,\n",
      "         -8.2422e-02, -2.8142e-01],\n",
      "        [-2.3302e-01,  1.3112e+00, -1.5073e-01,  ...,  2.7882e-01,\n",
      "         -0.0000e+00, -1.2303e-01],\n",
      "        [ 3.6979e+00, -0.0000e+00,  3.1607e+00,  ..., -2.6610e-01,\n",
      "         -2.8878e-01,  1.8334e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.6984e-03,  5.7668e-03,  8.7087e-03,  ...,  3.6874e-02,\n",
      "          1.9436e-02,  1.2471e-02],\n",
      "        [-0.0000e+00, -7.3739e-02, -1.4000e-01,  ..., -2.1227e-01,\n",
      "         -1.9772e-01, -1.4764e-01],\n",
      "        [ 2.1776e+00,  9.1083e-01,  4.2601e-02,  ..., -1.8271e-01,\n",
      "         -2.2579e-01,  7.2689e-01],\n",
      "        ...,\n",
      "        [-1.6546e-01,  0.0000e+00, -0.0000e+00,  ...,  4.5217e-01,\n",
      "         -3.9483e-01,  4.8365e-01],\n",
      "        [-2.8319e-01,  1.2727e+00, -4.7100e-02,  ..., -0.0000e+00,\n",
      "         -3.3278e-01,  6.6485e-01],\n",
      "        [ 3.6782e+00, -1.7768e-01,  3.1828e+00,  ..., -2.5883e-01,\n",
      "         -2.8522e-01,  0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0000e+00, -2.1309e-03, -2.1192e-04,  ..., -2.3437e-03,\n",
      "          0.0000e+00, -1.4639e-03],\n",
      "        [ 1.6254e-03, -7.4711e-02, -1.3747e-01,  ..., -2.1617e-01,\n",
      "         -0.0000e+00, -1.4700e-01],\n",
      "        [ 2.0579e+00,  1.1817e+00, -1.6319e-01,  ..., -2.2297e-01,\n",
      "         -2.9113e-01, -4.5395e-02],\n",
      "        ...,\n",
      "        [-1.6038e-01,  1.4337e+00, -4.4768e-02,  ..., -4.0794e-01,\n",
      "         -3.3466e-01, -1.2959e-01],\n",
      "        [-3.0540e-01,  1.9670e+00, -2.3376e-01,  ..., -7.0862e-02,\n",
      "         -1.9120e-01,  1.7443e-01],\n",
      "        [ 3.6803e+00, -1.7506e-01,  3.1732e+00,  ..., -2.6112e-01,\n",
      "         -2.7930e-01, -2.6683e-04]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.3897e-02,  1.2905e-02,  1.8163e-02,  ...,  3.7774e-02,\n",
      "         -1.0160e-02, -3.2619e-04],\n",
      "        [ 0.0000e+00, -1.7933e-01,  2.1662e+00,  ...,  3.9760e-01,\n",
      "         -3.7157e-01,  3.8122e+00],\n",
      "        [ 7.8073e-01,  1.0381e+00, -2.2784e-01,  ...,  0.0000e+00,\n",
      "         -1.9143e-01,  1.8152e+00],\n",
      "        ...,\n",
      "        [-8.3552e-01, -1.6119e-01, -2.3485e-01,  ..., -2.7803e-01,\n",
      "         -7.1135e-01, -4.7530e-01],\n",
      "        [-2.8905e-01, -9.4806e-02, -3.5516e-02,  ..., -0.0000e+00,\n",
      "         -5.8142e-01, -4.1373e-01],\n",
      "        [-2.2728e-01,  0.0000e+00, -2.8927e-01,  ..., -1.5280e-01,\n",
      "         -9.1269e-01, -6.2621e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0115,  0.0601,  0.0593,  ...,  0.0000, -0.0107,  0.0370],\n",
      "        [-0.1972, -0.2262,  1.2690,  ..., -0.0556, -0.2966,  2.6014],\n",
      "        [-0.2525,  0.0448,  1.2447,  ...,  0.0419, -0.3340, -0.1076],\n",
      "        ...,\n",
      "        [-0.3676, -0.2484, -0.5064,  ...,  0.6233,  0.2579,  0.0000],\n",
      "        [-0.0463, -0.5005,  0.1008,  ...,  0.5467, -0.4025, -0.2211],\n",
      "        [ 0.1994,  0.1029, -0.4410,  ...,  0.0000, -0.6978, -0.0217]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.0010e-03,  1.0082e-02, -2.5040e-03,  ..., -2.9341e-03,\n",
      "          3.0052e-03, -2.1109e-03],\n",
      "        [-2.9260e-01, -2.4867e-01,  8.6909e-01,  ..., -0.0000e+00,\n",
      "         -2.1332e-01,  3.1964e+00],\n",
      "        [-7.7621e-02,  3.5191e-02,  7.5543e-01,  ...,  3.5235e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [-4.1187e-01, -1.5025e-01,  0.0000e+00,  ...,  7.6365e-01,\n",
      "          3.9293e-01, -3.8508e-01],\n",
      "        [-4.4637e-01, -3.4244e-01,  4.9561e-01,  ..., -3.4462e-02,\n",
      "         -1.5664e-01, -0.0000e+00],\n",
      "        [-6.0784e-01,  5.7794e-01,  2.2814e-01,  ...,  2.5969e-01,\n",
      "         -1.4763e-01, -0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.4282e-02,  8.4838e-03, -3.6296e-03,  ...,  1.1287e-02,\n",
      "         -1.2382e-03, -6.6810e-04],\n",
      "        [-8.8047e-02, -7.1496e-02, -0.0000e+00,  ..., -2.1878e-01,\n",
      "         -1.2382e-03, -1.5119e-01],\n",
      "        [ 1.2709e+00,  0.0000e+00,  2.4133e+00,  ..., -1.6348e-01,\n",
      "         -4.9430e-01,  1.1163e+00],\n",
      "        ...,\n",
      "        [ 2.5388e+00, -2.2870e-01,  2.8928e+00,  ..., -3.0580e-01,\n",
      "         -0.0000e+00,  7.4050e-01],\n",
      "        [-2.6385e-01,  2.0117e+00,  7.7387e-02,  ..., -4.9800e-01,\n",
      "          3.1338e+00, -2.1595e-01],\n",
      "        [-3.1011e-01,  1.2157e+00, -2.0851e-01,  ..., -1.8682e-01,\n",
      "          8.5995e-01, -6.9566e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 3.2023e-03,  0.0000e+00,  2.4328e-02,  ...,  2.4960e-02,\n",
      "          2.9289e-03,  7.9944e-03],\n",
      "        [-9.3317e-02, -6.8655e-02, -1.3520e-01,  ..., -2.1536e-01,\n",
      "          2.9291e-03, -1.4852e-01],\n",
      "        [ 0.0000e+00,  5.5648e-01,  2.7731e+00,  ..., -3.9218e-01,\n",
      "         -3.8996e-01,  1.9563e+00],\n",
      "        ...,\n",
      "        [ 2.5178e+00, -2.2585e-01,  2.9316e+00,  ..., -3.0239e-01,\n",
      "         -3.1541e-01,  0.0000e+00],\n",
      "        [-9.0189e-02,  2.1560e+00, -2.9801e-01,  ..., -2.1516e-01,\n",
      "          1.8500e+00,  8.7409e-01],\n",
      "        [-1.7721e-01,  5.7091e-01, -5.9738e-02,  ..., -1.4211e-01,\n",
      "          1.3933e+00,  2.4573e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.5278e-03, -4.3214e-03, -1.1605e-03,  ..., -9.2832e-04,\n",
      "         -5.6792e-04, -3.5532e-03],\n",
      "        [-9.3390e-02, -7.5392e-02, -1.3755e-01,  ..., -2.1486e-01,\n",
      "         -5.6792e-04, -1.4887e-01],\n",
      "        [ 9.6226e-01,  3.6445e-01,  0.0000e+00,  ..., -2.0475e-01,\n",
      "         -3.6242e-01,  1.6881e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -2.2715e-01,  2.9025e+00,  ..., -2.9888e-01,\n",
      "         -3.0578e-01,  0.0000e+00],\n",
      "        [-1.5239e-02,  8.7150e-01, -2.5280e-01,  ..., -2.9078e-01,\n",
      "          2.7610e+00, -2.0219e-01],\n",
      "        [-2.6396e-01,  9.4648e-01, -3.2972e-01,  ..., -8.0535e-02,\n",
      "          1.5354e+00, -2.5777e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 8.8030e-02, -4.6441e-03,  2.0864e-03,  ...,  6.5448e-02,\n",
      "         -3.5231e-03,  7.7107e-02],\n",
      "        [ 4.1598e-01, -2.1157e-01,  2.5299e+00,  ...,  2.2884e-01,\n",
      "         -2.6601e-01,  0.0000e+00],\n",
      "        [-2.3142e-01,  2.5074e+00, -6.9474e-01,  ...,  1.8758e+00,\n",
      "         -6.6133e-01,  2.5973e-01],\n",
      "        ...,\n",
      "        [-1.6936e-01,  5.3105e-01, -4.6520e-01,  ..., -3.9386e-01,\n",
      "         -5.6386e-01,  0.0000e+00],\n",
      "        [-5.5737e-01,  2.6139e+00, -3.5383e-01,  ...,  8.8039e-01,\n",
      "         -6.7394e-01, -1.8479e-01],\n",
      "        [-7.9810e-01,  1.2969e+00, -7.0457e-01,  ..., -7.5642e-01,\n",
      "         -1.2749e+00, -4.1701e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.1997e-03, -6.9971e-04,  7.6242e-03,  ...,  9.2423e-03,\n",
      "          1.6256e-02,  1.5059e-02],\n",
      "        [-1.9843e-01, -2.4266e-01,  1.5108e+00,  ...,  3.8155e-01,\n",
      "         -2.3304e-01, -1.7305e-01],\n",
      "        [-5.7186e-02,  0.0000e+00, -1.5722e-01,  ...,  1.9978e+00,\n",
      "         -1.0235e-01,  1.3037e+00],\n",
      "        ...,\n",
      "        [-5.0020e-01, -3.0446e-01, -5.1185e-01,  ...,  1.6260e-01,\n",
      "         -6.5790e-02,  1.3120e+00],\n",
      "        [-2.0023e-01,  2.1393e+00, -3.6429e-01,  ...,  1.9909e+00,\n",
      "         -3.0094e-01,  3.5580e-01],\n",
      "        [-1.0206e-01,  7.6775e-01, -4.0719e-01,  ..., -3.4865e-01,\n",
      "         -1.9491e-01,  0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000e+00,  0.0000e+00, -5.5876e-03,  ..., -2.6018e-03,\n",
      "         -0.0000e+00, -4.8435e-03],\n",
      "        [-4.2045e-01, -0.0000e+00,  4.1394e-01,  ..., -1.4061e-01,\n",
      "         -3.0832e-01, -8.4143e-03],\n",
      "        [-5.5048e-01,  2.6793e+00, -2.5471e-02,  ...,  4.9973e+00,\n",
      "         -2.2375e-01,  1.9840e-01],\n",
      "        ...,\n",
      "        [ 1.5537e-01, -1.0704e-01,  1.0734e+00,  ..., -0.0000e+00,\n",
      "          3.8153e-01, -7.2108e-02],\n",
      "        [-3.3552e-01,  2.2619e+00, -1.9313e-01,  ...,  2.4179e+00,\n",
      "         -2.3202e-01, -1.1547e-02],\n",
      "        [-5.0088e-01,  6.9098e-01, -1.5596e-01,  ...,  4.7820e-01,\n",
      "         -0.0000e+00, -2.1320e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 8.1775e-03,  1.2290e-03, -1.4764e-03,  ..., -5.6760e-03,\n",
      "         -3.3756e-03, -3.3201e-03],\n",
      "        [-9.2317e-02, -7.2569e-02, -1.4365e-01,  ..., -0.0000e+00,\n",
      "         -2.0449e-01, -1.5356e-01],\n",
      "        [ 2.8116e+00,  2.0997e+00,  2.6883e-01,  ..., -1.7550e-02,\n",
      "         -3.5728e-01, -2.9450e-01],\n",
      "        ...,\n",
      "        [ 4.0241e-01,  2.1858e+00, -6.8280e-02,  ..., -1.8026e-01,\n",
      "         -2.4734e-01,  1.5047e-01],\n",
      "        [-8.4403e-02,  5.9410e-01, -5.2965e-02,  ..., -3.9451e-01,\n",
      "          0.0000e+00, -4.9530e-02],\n",
      "        [-2.8801e-01,  1.7861e+00, -1.1480e-01,  ..., -2.4181e-01,\n",
      "          1.2628e+00, -1.1013e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 7.5176e-03,  6.3176e-04, -1.4520e-03,  ...,  2.8684e-02,\n",
      "          7.8535e-03,  0.0000e+00],\n",
      "        [-9.2482e-02, -7.2718e-02, -1.4363e-01,  ..., -2.1454e-01,\n",
      "         -1.9915e-01, -1.4735e-01],\n",
      "        [ 2.4282e+00,  1.1377e+00,  3.2927e-01,  ..., -2.3769e-01,\n",
      "         -2.6871e-01,  1.0288e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  2.1852e+00, -6.8256e-02,  ..., -0.0000e+00,\n",
      "         -2.4200e-01,  0.0000e+00],\n",
      "        [-2.4420e-01,  1.7429e+00, -1.3272e-01,  ...,  1.1887e-01,\n",
      "          3.5577e+00, -3.4308e-01],\n",
      "        [-9.0904e-02,  1.3650e+00, -2.2126e-01,  ..., -0.0000e+00,\n",
      "          1.5534e+00,  6.6278e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000e+00,  1.6338e-03, -1.6018e-03,  ..., -0.0000e+00,\n",
      "         -1.2999e-03, -5.2513e-04],\n",
      "        [-9.1523e-02, -6.9956e-02, -1.3885e-01,  ..., -2.1596e-01,\n",
      "         -1.9545e-01, -1.4556e-01],\n",
      "        [ 1.8539e+00,  1.2638e+00, -2.1275e-01,  ..., -2.8993e-01,\n",
      "         -2.4971e-01, -4.6483e-03],\n",
      "        ...,\n",
      "        [ 3.9244e-01,  2.1862e+00, -6.6090e-02,  ..., -1.7047e-01,\n",
      "         -2.3681e-01,  1.6158e-01],\n",
      "        [-5.4145e-01,  9.1648e-01, -3.9148e-01,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -1.3893e-01],\n",
      "        [-3.0641e-01,  9.6389e-01, -5.0569e-01,  ..., -8.9552e-02,\n",
      "          1.7978e+00, -3.0503e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.1171e-02,  4.1334e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -5.2370e-03, -2.9865e-03],\n",
      "        [-2.0319e-01,  1.2337e+00,  2.0238e+00,  ..., -2.3808e-01,\n",
      "         -7.3420e-01,  2.7751e+00],\n",
      "        [ 1.5655e+00,  3.0603e+00,  9.1625e-02,  ...,  4.0763e+00,\n",
      "         -1.5022e-01, -1.4832e-01],\n",
      "        ...,\n",
      "        [-1.5425e-01,  1.2348e+00, -2.4751e-01,  ..., -3.2321e-01,\n",
      "         -7.0369e-01, -4.7611e-01],\n",
      "        [-1.0648e-01,  0.0000e+00,  3.0207e-01,  ..., -2.3562e-01,\n",
      "         -6.5860e-01, -1.9017e-01],\n",
      "        [-1.6103e-01,  0.0000e+00, -6.0477e-01,  ..., -3.4388e-01,\n",
      "         -0.0000e+00, -5.5945e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.5351e-02,  6.4274e-02,  2.8030e-02,  ..., -2.7514e-04,\n",
      "         -1.1985e-02,  0.0000e+00],\n",
      "        [-2.6765e-02,  7.1706e-01,  1.5419e+00,  ..., -1.1872e-01,\n",
      "         -0.0000e+00,  2.5728e+00],\n",
      "        [-2.7605e-02, -1.3320e-01, -2.4900e-01,  ...,  3.0108e+00,\n",
      "         -4.1446e-01, -3.9178e-01],\n",
      "        ...,\n",
      "        [-9.3136e-02, -5.9472e-02, -1.1632e+00,  ..., -5.1973e-02,\n",
      "         -6.0218e-01,  4.0396e-01],\n",
      "        [-2.1276e-01,  9.9522e-01, -7.6800e-01,  ..., -2.9175e-01,\n",
      "         -7.5413e-01, -1.9406e-01],\n",
      "        [ 6.2795e-01, -2.0422e-01, -7.2270e-01,  ...,  2.0983e-01,\n",
      "         -8.6546e-01, -0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-4.9828e-03,  0.0000e+00, -3.1625e-03,  ..., -2.4341e-03,\n",
      "         -0.0000e+00, -1.4166e-04],\n",
      "        [-3.5766e-01, -0.0000e+00,  1.3859e+00,  ..., -8.1265e-02,\n",
      "         -2.0803e-01,  1.4467e+00],\n",
      "        [-3.8098e-01, -2.6818e-01,  0.0000e+00,  ...,  4.0206e+00,\n",
      "         -1.2716e-01, -7.5021e-01],\n",
      "        ...,\n",
      "        [ 8.2776e-02,  8.7895e-01,  6.3527e-01,  ...,  7.5405e-01,\n",
      "         -2.0443e-01, -2.6871e-01],\n",
      "        [-2.4625e-01,  8.8304e-01, -1.7165e-01,  ..., -6.5215e-02,\n",
      "         -2.0720e-01, -4.6977e-01],\n",
      "        [-0.0000e+00,  4.3807e-01,  4.1740e-01,  ...,  9.5654e-01,\n",
      "         -2.2919e-01, -0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 5.2948e-03,  4.6554e-03, -2.7386e-03,  ...,  6.6816e-03,\n",
      "          1.6686e-02, -2.4584e-03],\n",
      "        [-9.2157e-02, -6.9691e-02, -3.6577e-03,  ...,  9.8926e-03,\n",
      "         -1.9829e-01, -1.2069e-03],\n",
      "        [ 3.2720e+00,  1.6904e+00, -6.2261e-02,  ...,  8.2408e-01,\n",
      "         -3.8353e-01, -6.1397e-01],\n",
      "        ...,\n",
      "        [-3.5607e-01,  0.0000e+00,  1.3356e+00,  ..., -0.0000e+00,\n",
      "          2.4462e-01, -2.0390e-01],\n",
      "        [-2.8684e-01,  1.9411e+00, -1.0710e-01,  ..., -3.8733e-01,\n",
      "          4.2547e-01, -1.6831e-01],\n",
      "        [-2.4123e-01,  2.2562e+00, -9.7319e-02,  ..., -2.4392e-01,\n",
      "         -0.0000e+00,  2.7455e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.4080e-04,  7.4118e-03,  6.6989e-03,  ...,  0.0000e+00,\n",
      "          2.0613e-02,  5.9482e-04],\n",
      "        [-9.3721e-02, -6.9002e-02,  0.0000e+00,  ...,  3.4406e-02,\n",
      "         -1.9731e-01,  5.6007e-03],\n",
      "        [ 2.1332e-01,  0.0000e+00, -8.4793e-02,  ...,  4.9309e-01,\n",
      "          1.4294e+00, -1.0346e-01],\n",
      "        ...,\n",
      "        [-2.6119e-01,  1.6634e+00, -8.8781e-02,  ..., -2.1484e-01,\n",
      "          2.1154e+00,  6.0557e-02],\n",
      "        [-3.2348e-01,  0.0000e+00, -2.1095e-01,  ..., -3.4874e-01,\n",
      "          2.0060e+00, -0.0000e+00],\n",
      "        [-1.1943e-01,  0.0000e+00, -2.5716e-01,  ..., -1.2157e-01,\n",
      "          7.2546e-01,  2.9947e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-9.9953e-04, -1.5251e-03, -4.5899e-05,  ..., -1.2638e-04,\n",
      "          2.9695e-03, -0.0000e+00],\n",
      "        [-9.1232e-02, -6.9918e-02, -0.0000e+00,  ...,  2.6873e-03,\n",
      "         -1.9471e-01, -4.2292e-04],\n",
      "        [ 6.8333e-01,  1.3560e+00, -2.8329e-01,  ..., -1.7260e-01,\n",
      "         -1.7651e-01, -1.1642e-01],\n",
      "        ...,\n",
      "        [-4.1371e-01,  1.2877e+00,  2.6040e-01,  ...,  1.1909e-01,\n",
      "          1.4738e+00, -1.1761e-01],\n",
      "        [-4.1894e-01,  0.0000e+00, -2.9332e-01,  ..., -2.1717e-02,\n",
      "          0.0000e+00, -1.3119e-01],\n",
      "        [-3.0020e-01,  1.0381e+00, -4.1805e-01,  ..., -8.8554e-02,\n",
      "          1.0470e+00, -2.9992e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0603, -0.0050, -0.0037,  ...,  0.0545,  0.0437,  0.0206],\n",
      "        [-0.2689, -0.2359,  0.0000,  ..., -0.0000, -0.3690,  2.0972],\n",
      "        [-0.3324,  2.4216, -0.1918,  ...,  1.0225,  0.0867, -0.2601],\n",
      "        ...,\n",
      "        [ 0.4312, -0.4043, -0.4579,  ..., -0.6330, -1.0307, -0.3422],\n",
      "        [-0.4175,  0.2724, -0.0952,  ..., -0.5787, -1.3580, -0.6507],\n",
      "        [-0.4221,  1.4071, -0.0000,  ..., -0.6452, -0.9474, -0.4088]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0189,  0.0675,  0.0532,  ...,  0.0070, -0.0108,  0.0332],\n",
      "        [-0.2794,  1.1008,  1.8568,  ..., -0.0000, -0.3501,  3.2789],\n",
      "        [-1.0621,  0.9717,  1.6616,  ...,  0.0661, -0.0489, -0.2883],\n",
      "        ...,\n",
      "        [-0.0506,  2.6838,  1.2856,  ..., -0.1931, -0.3332,  0.4092],\n",
      "        [-0.1346,  0.2797, -0.4056,  ..., -0.2512, -0.0000,  0.7864],\n",
      "        [ 0.6186,  0.0000, -0.5525,  ...,  0.0280, -0.0000,  0.2024]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.9841e-03,  0.0000e+00, -4.5398e-03,  ..., -1.4621e-03,\n",
      "         -1.7401e-03,  0.0000e+00],\n",
      "        [-2.0240e-01, -0.0000e+00,  1.0405e+00,  ..., -2.6436e-01,\n",
      "         -4.6655e-01,  2.7587e+00],\n",
      "        [-8.4837e-01,  9.5123e-02, -1.6772e-01,  ...,  1.3205e+00,\n",
      "          0.0000e+00, -3.8255e-01],\n",
      "        ...,\n",
      "        [-4.0847e-01, -3.3623e-01, -4.4602e-03,  ..., -0.0000e+00,\n",
      "         -2.1100e-01, -2.3584e-01],\n",
      "        [-4.5598e-01,  5.2732e-01,  1.4675e+00,  ...,  2.6082e-01,\n",
      "         -1.2845e-01, -2.8268e-01],\n",
      "        [-2.1688e-01,  5.8103e-01, -3.2353e-02,  ...,  1.5333e-01,\n",
      "         -3.0715e-01, -3.2098e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0145, -0.0062,  0.0046,  ..., -0.0062,  0.0170,  0.0167],\n",
      "        [-0.0883, -0.0760, -0.1410,  ..., -0.2290, -0.1982, -0.1455],\n",
      "        [-0.0000,  1.6701,  1.2379,  ..., -0.2458, -0.7748,  0.0968],\n",
      "        ...,\n",
      "        [-0.2307,  0.7209, -0.0637,  ..., -0.4041, -0.0317, -0.1782],\n",
      "        [-0.2231,  0.5305, -0.0440,  ..., -0.3704,  0.5183,  0.2629],\n",
      "        [-0.0000,  1.2987, -0.1077,  ..., -0.2449,  1.3988, -0.1366]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0038,  0.0179, -0.0019,  ...,  0.0250,  0.0391,  0.0115],\n",
      "        [-0.0000, -0.0654, -0.1440,  ..., -0.2165, -0.1927, -0.1468],\n",
      "        [ 0.6232,  0.1400,  1.3667,  ..., -0.6296, -0.2805,  1.2011],\n",
      "        ...,\n",
      "        [-0.2366, -0.3179, -0.3474,  ..., -0.2120,  0.1986, -0.0608],\n",
      "        [-0.2779,  0.0000, -0.0000,  ..., -0.1727,  0.3125,  0.1496],\n",
      "        [-0.0000,  1.4775, -0.3110,  ..., -0.1587,  1.5784, -0.1531]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.4291e-03, -0.0000e+00, -1.2452e-03,  ...,  1.3898e-03,\n",
      "          2.4255e-03, -2.1234e-03],\n",
      "        [-8.8414e-02, -0.0000e+00, -1.3845e-01,  ..., -2.1464e-01,\n",
      "         -1.9479e-01, -1.4662e-01],\n",
      "        [ 3.2938e-01, -1.5649e-01,  1.4573e+00,  ..., -5.4148e-01,\n",
      "         -3.2371e-01,  8.1688e-01],\n",
      "        ...,\n",
      "        [-3.5157e-01, -4.8831e-02, -4.6693e-01,  ..., -0.0000e+00,\n",
      "          2.3983e-01, -3.7639e-01],\n",
      "        [-2.8254e-01,  1.1702e+00, -4.7265e-01,  ..., -5.7390e-03,\n",
      "          1.0303e+00, -1.2885e-01],\n",
      "        [-2.7300e-01,  0.0000e+00, -4.2494e-01,  ..., -3.7240e-02,\n",
      "          1.9345e+00, -2.2346e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.3720e-01, -2.1931e-03,  1.3102e-01,  ...,  4.2583e-02,\n",
      "          4.0949e-02,  1.1715e-01],\n",
      "        [-5.0738e-01, -5.1430e-01,  0.0000e+00,  ..., -2.6483e-01,\n",
      "         -6.3112e-01,  3.7311e+00],\n",
      "        [ 2.5359e+00,  3.6462e-01, -1.3644e-01,  ..., -9.9568e-01,\n",
      "         -1.3223e+00, -1.9023e-01],\n",
      "        ...,\n",
      "        [-1.2560e-01, -0.0000e+00, -3.6641e-01,  ..., -8.1921e-01,\n",
      "         -1.4959e+00, -8.6701e-01],\n",
      "        [ 1.4214e+00,  1.4868e+00, -7.1419e-01,  ..., -7.0320e-01,\n",
      "         -1.0759e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -3.0902e-01,  6.5903e-01,  ..., -5.4506e-01,\n",
      "         -9.6141e-01,  0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0585,  0.1051,  0.0617,  ...,  0.0036, -0.0052,  0.0649],\n",
      "        [ 0.7625, -0.2489, -0.0541,  ..., -0.4108, -0.3526,  3.2021],\n",
      "        [-0.4670,  0.0880, -0.5973,  ..., -0.4182, -0.2321, -0.0681],\n",
      "        ...,\n",
      "        [ 1.8135, -0.4171, -0.5198,  ...,  0.1764, -0.6231,  0.6018],\n",
      "        [ 3.0696, -0.5689, -0.3020,  ..., -0.0987, -0.6216,  0.4741],\n",
      "        [ 1.1625, -0.3691,  1.3717,  ..., -0.0355, -0.3961,  3.2834]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000,  0.0000, -0.0066,  ..., -0.0061, -0.0000, -0.0052],\n",
      "        [-0.0625, -0.6847,  2.2121,  ...,  0.6221, -0.3883,  1.7840],\n",
      "        [-1.1952, -0.2824, -0.0000,  ...,  0.0000,  0.6573, -0.8416],\n",
      "        ...,\n",
      "        [-0.4596, -0.2675,  0.0000,  ..., -0.1380, -0.2326, -0.2700],\n",
      "        [-0.2168, -0.2480,  1.0688,  ...,  0.0000,  0.0000, -0.4805],\n",
      "        [-0.4073, -0.5322,  1.6493,  ..., -0.2450, -0.2061,  2.6956]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000e+00,  1.2534e-02,  8.9304e-03,  ..., -2.0297e-03,\n",
      "          6.1105e-03, -3.8031e-03],\n",
      "        [-9.5988e-02, -6.6078e-02,  5.2232e-03,  ..., -1.2305e-03,\n",
      "         -2.0090e-01, -0.0000e+00],\n",
      "        [ 4.6021e-01, -8.5756e-02, -2.2325e-02,  ..., -8.3324e-01,\n",
      "         -2.5719e-01,  6.8428e-01],\n",
      "        ...,\n",
      "        [-2.6165e-01,  2.3215e+00, -1.5338e-01,  ..., -4.8308e-01,\n",
      "          2.3962e+00, -3.8177e-01],\n",
      "        [-1.2179e-02,  2.8878e+00,  5.7893e-01,  ..., -6.6998e-01,\n",
      "          1.4884e-01, -4.2539e-01],\n",
      "        [-9.5989e-02, -6.6078e-02, -1.3997e-01,  ..., -2.2403e-01,\n",
      "         -2.0090e-01, -2.5497e-03]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0033,  0.0056, -0.0026,  ...,  0.0000,  0.0212, -0.0040],\n",
      "        [-0.0952, -0.0678, -0.0036,  ...,  0.0307, -0.1971, -0.1536],\n",
      "        [ 0.5731, -0.4521, -0.3687,  ...,  0.7863, -0.4565,  0.0000],\n",
      "        ...,\n",
      "        [-0.3434,  0.7592, -0.4178,  ..., -0.1889,  2.1454, -0.1671],\n",
      "        [ 0.0298,  1.2025, -0.1982,  ..., -0.3466,  0.6120, -0.1003],\n",
      "        [-0.0952, -0.0678, -0.1448,  ..., -0.2151, -0.1971, -0.0028]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-8.9416e-04, -1.0282e-03, -1.5691e-03,  ..., -1.1646e-03,\n",
      "          4.2744e-03, -2.3292e-03],\n",
      "        [-8.9549e-02, -6.7815e-02, -0.0000e+00,  ..., -3.9358e-04,\n",
      "         -1.9430e-01, -1.4664e-01],\n",
      "        [ 1.9525e+00,  6.7370e-02, -4.1846e-01,  ..., -5.0292e-01,\n",
      "         -1.5521e-01,  1.4667e+00],\n",
      "        ...,\n",
      "        [-5.1173e-01,  1.2804e+00, -4.4149e-01,  ..., -7.1583e-02,\n",
      "          3.2899e+00, -3.6084e-01],\n",
      "        [-2.6524e-01,  1.4524e+00, -2.5287e-01,  ...,  0.0000e+00,\n",
      "          1.5345e+00, -6.1799e-01],\n",
      "        [-8.9549e-02, -6.7815e-02, -1.3879e-01,  ..., -2.1539e-01,\n",
      "         -1.9430e-01, -1.1202e-03]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0267,  0.0152,  0.0000,  ...,  0.0388, -0.0000, -0.0028],\n",
      "        [ 0.0000,  1.0580,  0.1050,  ..., -0.0808, -0.2444, -0.0734],\n",
      "        [-0.2875,  0.7406, -0.1124,  ...,  2.4676, -0.3085,  2.6910],\n",
      "        ...,\n",
      "        [-0.2942,  1.8009, -0.1329,  ..., -0.0725, -0.8411, -0.0000],\n",
      "        [ 0.0000,  1.6879,  0.4521,  ...,  0.0461, -0.6460, -0.5258],\n",
      "        [-0.2614,  2.4703, -0.2262,  ..., -0.0149,  0.7394, -0.4396]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0081, -0.0000,  0.0073,  ...,  0.0000,  0.0170,  0.0299],\n",
      "        [-0.1202, -0.0754, -0.1168,  ..., -0.0000,  0.3271, -0.3947],\n",
      "        [ 0.3749, -0.5575, -0.0000,  ..., -0.1183, -0.0000,  0.5385],\n",
      "        ...,\n",
      "        [-0.0679,  0.0769, -0.0000,  ..., -0.3449, -0.0000,  0.4259],\n",
      "        [ 0.0923, -0.1804, -0.2280,  ...,  0.3148,  0.1454,  1.1482],\n",
      "        [-0.1634,  2.3660, -0.2197,  ..., -0.2361,  2.9148, -0.0612]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.0335e-03,  8.2742e-03, -6.0422e-03,  ...,  6.6790e-03,\n",
      "          4.9043e-04,  5.2773e-03],\n",
      "        [-3.4981e-01, -1.0906e-01, -4.5043e-02,  ..., -0.0000e+00,\n",
      "          1.1387e+00, -3.0074e-01],\n",
      "        [-4.5288e-01, -4.7553e-01,  4.1782e-02,  ...,  3.3884e+00,\n",
      "         -1.6635e-01, -1.3987e-01],\n",
      "        ...,\n",
      "        [-2.6112e-01, -8.1863e-04,  6.8315e-01,  ..., -2.2858e-01,\n",
      "         -5.8830e-02, -2.4606e-01],\n",
      "        [-9.3668e-02, -2.2419e-01,  2.2398e-01,  ...,  5.0608e-01,\n",
      "          9.3804e-01, -1.8530e-01],\n",
      "        [-2.7441e-01,  0.0000e+00,  1.7466e-01,  ..., -1.8894e-01,\n",
      "          3.1562e+00, -1.8091e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000e+00, -2.3654e-04, -1.7555e-03,  ...,  1.4593e-02,\n",
      "         -2.7002e-03, -3.3826e-03],\n",
      "        [ 4.8801e-01, -5.3300e-02, -2.1725e-01,  ..., -2.0384e-01,\n",
      "         -3.0212e-01, -0.0000e+00],\n",
      "        [ 3.9172e-01,  6.3074e-01, -1.9099e-01,  ..., -3.8963e-01,\n",
      "         -3.6783e-01,  1.6599e+00],\n",
      "        ...,\n",
      "        [-6.1373e-02,  1.3703e+00,  1.5067e-01,  ..., -0.0000e+00,\n",
      "          1.7956e+00, -2.0904e-01],\n",
      "        [ 2.7431e-01,  3.1762e+00,  1.0194e+00,  ..., -7.0115e-01,\n",
      "         -4.1538e-01, -2.3906e-01],\n",
      "        [ 0.0000e+00,  1.7160e-03, -2.2802e-01,  ..., -2.1810e-01,\n",
      "         -0.0000e+00, -2.6579e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.8394e-02,  0.0000e+00,  2.2285e-02,  ...,  3.1331e-02,\n",
      "          2.2666e-02, -2.2756e-03],\n",
      "        [ 5.2929e-01, -5.0698e-02, -2.0993e-01,  ..., -1.9965e-01,\n",
      "         -2.9375e-01, -1.8843e-01],\n",
      "        [-0.0000e+00,  1.4928e-01, -2.7843e-02,  ..., -6.1729e-01,\n",
      "         -5.4662e-01,  2.9423e+00],\n",
      "        ...,\n",
      "        [-3.7961e-01, -1.7424e-01, -3.2109e-01,  ..., -4.9008e-02,\n",
      "          2.7811e+00,  2.7581e-02],\n",
      "        [-1.0805e-02,  2.4608e+00, -0.0000e+00,  ..., -3.1424e-01,\n",
      "          0.0000e+00, -2.3623e-02],\n",
      "        [ 1.1257e-01,  1.2126e-02, -2.2069e-01,  ..., -2.1391e-01,\n",
      "         -2.4349e-01, -0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0000e+00, -2.3495e-03,  7.3818e-04,  ..., -0.0000e+00,\n",
      "          7.2754e-03, -1.3724e-03],\n",
      "        [ 5.0356e-01, -5.3543e-02, -2.0772e-01,  ..., -2.0117e-01,\n",
      "         -2.8711e-01, -1.8096e-01],\n",
      "        [-0.0000e+00,  1.7538e+00, -3.1906e-01,  ..., -4.8074e-01,\n",
      "         -1.3717e-01,  1.2265e+00],\n",
      "        ...,\n",
      "        [-1.7469e-01, -6.5995e-02, -5.9541e-01,  ..., -1.0007e-02,\n",
      "          3.1889e+00, -2.2687e-01],\n",
      "        [-1.2918e-01,  1.6145e+00, -2.1128e-01,  ...,  2.0570e-01,\n",
      "          6.3777e-01, -3.6780e-01],\n",
      "        [ 8.6838e-02, -1.7073e-03, -2.1811e-01,  ..., -2.1493e-01,\n",
      "         -0.0000e+00, -2.5453e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 6.4634e-02,  5.7211e-03, -2.5901e-04,  ...,  1.2221e-02,\n",
      "          2.2814e-02, -4.2132e-03],\n",
      "        [-2.3602e-01, -1.3852e-01,  1.7692e+00,  ..., -3.3746e-01,\n",
      "          2.4317e-01,  2.7173e+00],\n",
      "        [-9.4544e-02, -4.3623e-01, -2.4139e-01,  ..., -9.2481e-01,\n",
      "         -1.1491e+00,  9.0035e-01],\n",
      "        ...,\n",
      "        [ 4.6274e-01,  1.6802e+00, -7.4624e-02,  ..., -1.0109e+00,\n",
      "         -9.9495e-01, -2.9359e-01],\n",
      "        [ 1.3854e-01,  1.3875e+00, -6.5574e-01,  ..., -8.0589e-01,\n",
      "         -1.6658e+00,  8.3028e-01],\n",
      "        [-2.4121e-01,  0.0000e+00, -6.2269e-02,  ..., -0.0000e+00,\n",
      "         -7.1952e-01, -1.0684e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 5.4291e-02,  4.3485e-03,  2.9012e-02,  ..., -3.0615e-04,\n",
      "         -0.0000e+00,  5.3807e-02],\n",
      "        [-4.1754e-02, -3.1819e-01,  1.9675e+00,  ..., -6.2188e-02,\n",
      "          6.1744e-02,  2.7433e+00],\n",
      "        [ 0.0000e+00, -4.0092e-01, -0.0000e+00,  ...,  8.3997e-01,\n",
      "         -1.1046e+00,  1.4282e+00],\n",
      "        ...,\n",
      "        [ 6.8147e+00, -1.2895e+00, -4.5274e-01,  ..., -1.3434e-01,\n",
      "         -1.1308e+00, -3.3004e-01],\n",
      "        [ 2.6077e+00, -4.5989e-01, -1.2151e-01,  ...,  1.5108e+00,\n",
      "         -4.7765e-01,  0.0000e+00],\n",
      "        [ 1.8505e+00, -4.3425e-01,  3.1972e-01,  ..., -7.7773e-02,\n",
      "         -0.0000e+00, -2.6870e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 8.5741e-03,  0.0000e+00, -2.6354e-03,  ..., -1.7256e-04,\n",
      "          2.2602e-05,  3.8967e-03],\n",
      "        [-1.9245e-01, -6.3815e-01,  2.3511e+00,  ..., -1.5046e-01,\n",
      "          6.0569e-01,  1.8740e+00],\n",
      "        [-2.0006e-01, -5.4898e-01,  2.1661e+00,  ...,  1.6899e+00,\n",
      "          1.9098e-01, -5.3856e-01],\n",
      "        ...,\n",
      "        [-2.9015e-01, -2.4185e-01, -9.5056e-02,  ...,  6.0551e-01,\n",
      "          1.2067e+00, -1.7749e-01],\n",
      "        [-0.0000e+00, -8.3430e-02, -2.2885e-02,  ...,  1.0743e+00,\n",
      "         -1.2993e-01, -2.3614e-01],\n",
      "        [-4.6642e-01, -8.8590e-02, -4.7111e-02,  ...,  5.5907e-01,\n",
      "         -0.0000e+00, -3.5782e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 7.8246e-03, -1.8000e-03, -2.7396e-03,  ...,  1.7431e-02,\n",
      "         -5.6294e-03, -3.4965e-03],\n",
      "        [ 4.7026e-03, -6.9713e-02, -1.4499e-01,  ..., -2.1782e-01,\n",
      "         -6.9820e-03, -1.5272e-01],\n",
      "        [ 0.0000e+00,  3.4918e+00,  1.0108e+00,  ..., -1.1289e-01,\n",
      "         -1.5396e-01, -1.2305e-01],\n",
      "        ...,\n",
      "        [-2.3245e-01,  6.8408e-01, -4.7484e-02,  ..., -3.0155e-01,\n",
      "          2.5316e+00,  4.3287e-01],\n",
      "        [-1.6077e-01,  2.2338e+00,  1.6772e+00,  ..., -3.4887e-01,\n",
      "         -1.5691e-01,  0.0000e+00],\n",
      "        [ 4.2026e-01,  2.1870e+00, -7.0916e-02,  ..., -0.0000e+00,\n",
      "         -2.5161e-01,  1.7013e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.6251e-03,  2.5396e-02,  1.7174e-03,  ...,  3.3915e-02,\n",
      "          2.2560e-02, -2.9002e-03],\n",
      "        [-3.4056e-03, -6.1564e-02, -1.4182e-01,  ..., -2.1370e-01,\n",
      "          1.7149e-02, -1.5212e-01],\n",
      "        [ 3.3150e-01,  5.4473e-01,  1.0943e-01,  ..., -1.3760e-01,\n",
      "         -3.2054e-02,  0.0000e+00],\n",
      "        ...,\n",
      "        [-2.8118e-01,  1.6239e+00, -2.2416e-01,  ...,  1.0424e+00,\n",
      "          3.2725e+00, -1.3823e-01],\n",
      "        [-2.8836e-01,  1.7170e+00, -4.1912e-01,  ...,  1.5427e+00,\n",
      "         -4.6739e-02, -0.0000e+00],\n",
      "        [ 4.0194e-01,  2.2196e+00, -6.7747e-02,  ..., -1.6790e-01,\n",
      "         -2.4034e-01,  1.7251e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 6.9144e-03, -1.9362e-03, -2.3502e-03,  ..., -1.0802e-03,\n",
      "          2.2065e-03, -2.2285e-03],\n",
      "        [ 3.7924e-03, -6.7445e-02, -1.3957e-01,  ..., -0.0000e+00,\n",
      "         -7.7264e-04, -1.4617e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -3.6267e-01,  ..., -3.1328e-01,\n",
      "         -5.1816e-02, -0.0000e+00],\n",
      "        ...,\n",
      "        [-3.5549e-01,  8.8535e-01, -0.0000e+00,  ..., -1.8521e-02,\n",
      "          3.3676e+00, -1.7020e-01],\n",
      "        [-8.7108e-02,  1.4731e+00, -4.6960e-01,  ...,  1.0032e+00,\n",
      "          9.1499e-01, -0.0000e+00],\n",
      "        [ 4.1935e-01,  0.0000e+00, -6.8113e-02,  ..., -1.7121e-01,\n",
      "         -2.3674e-01,  1.7487e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0140, -0.0143,  0.0566,  ..., -0.0075, -0.0113, -0.0090],\n",
      "        [-0.2876, -0.4130,  0.0000,  ..., -0.2857, -0.5297,  0.0000],\n",
      "        [-0.1213, -0.0686, -0.0000,  ..., -0.5610, -0.5235, -0.2304],\n",
      "        ...,\n",
      "        [-0.1079, -0.3334, -0.0000,  ..., -0.2568, -0.8787,  0.4897],\n",
      "        [ 1.3229, -0.3805, -0.0251,  ..., -0.2656, -1.1656,  1.2124],\n",
      "        [ 0.0000, -0.1920, -0.4237,  ...,  2.1224, -0.4775,  3.4666]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-7.7357e-03,  0.0000e+00,  5.4108e-02,  ..., -2.5106e-03,\n",
      "         -7.1797e-03,  0.0000e+00],\n",
      "        [-1.2143e-01, -0.0000e+00,  1.1602e+00,  ...,  2.1925e-02,\n",
      "         -3.6236e-01,  3.3477e+00],\n",
      "        [-1.1096e-01, -2.4581e-01,  4.0839e-02,  ..., -1.4760e-01,\n",
      "         -1.5523e-01, -3.5854e-01],\n",
      "        ...,\n",
      "        [-7.4281e-01,  1.2233e+00,  1.2566e+00,  ..., -6.1040e-01,\n",
      "         -4.7764e-01, -2.0027e-01],\n",
      "        [-5.0699e-01,  6.1535e-01,  8.4731e-01,  ..., -3.0592e-01,\n",
      "         -3.3245e-01, -4.6273e-02],\n",
      "        [-8.1176e-01,  4.4369e-01, -1.4336e-02,  ..., -1.5539e-01,\n",
      "         -4.6026e-01, -4.2517e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.6674e-03,  0.0000e+00, -1.0818e-03,  ...,  8.3247e-04,\n",
      "         -1.2424e-03, -1.9327e-03],\n",
      "        [-3.7897e-01, -2.1089e-01,  1.6127e+00,  ..., -4.9672e-02,\n",
      "         -2.2640e-01,  2.7843e+00],\n",
      "        [-5.0967e-01, -2.6714e-01, -2.3472e-01,  ..., -1.4534e-01,\n",
      "          7.0191e-01, -6.2721e-01],\n",
      "        ...,\n",
      "        [-6.2958e-01, -2.7262e-01, -3.4340e-01,  ...,  2.5405e-02,\n",
      "         -1.9141e-01,  1.4241e-01],\n",
      "        [-6.4520e-01, -0.0000e+00, -2.5282e-01,  ..., -2.1187e-01,\n",
      "         -1.3265e-01,  7.5840e-02],\n",
      "        [-7.4070e-01, -2.3358e-01, -1.9109e-01,  ...,  3.0002e+00,\n",
      "         -8.4951e-02,  0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0237,  0.0147,  0.0200,  ..., -0.0039, -0.0034,  0.0183],\n",
      "        [-0.0000, -0.0644, -0.1373,  ..., -0.2262, -0.2057, -0.1445],\n",
      "        [ 0.3850,  1.6260,  0.2488,  ..., -0.8652, -0.4206, -0.5151],\n",
      "        ...,\n",
      "        [ 0.6089, -0.2763, -0.1695,  ..., -0.0704,  0.1558,  0.5497],\n",
      "        [-0.1399,  0.0000, -0.0197,  ..., -0.0000, -0.0000, -0.0971],\n",
      "        [ 1.1703, -0.0190,  0.4639,  ...,  0.0000, -0.4223,  1.3611]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0129,  0.0075,  0.0374,  ...,  0.0256,  0.0000,  0.0170],\n",
      "        [-0.0853, -0.0662, -0.1329,  ..., -0.2158, -0.1973, -0.1448],\n",
      "        [-0.1401,  1.7746, -0.2795,  ..., -0.5084, -0.2361, -0.1852],\n",
      "        ...,\n",
      "        [-0.2952,  0.4233, -0.1385,  ...,  0.2333,  0.4810, -0.0000],\n",
      "        [-0.1001,  0.3125, -0.2292,  ..., -0.0000, -0.0881, -0.0038],\n",
      "        [ 0.9545, -0.2629, -0.1849,  ..., -0.4567,  0.2080,  2.1829]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 7.5475e-03, -2.1980e-03, -0.0000e+00,  ..., -3.1977e-03,\n",
      "          2.8450e-03, -1.1097e-03],\n",
      "        [-8.3534e-02, -6.7844e-02, -1.3869e-01,  ..., -2.1756e-01,\n",
      "         -1.9439e-01, -1.4486e-01],\n",
      "        [ 9.5404e-01,  1.7894e+00, -6.6717e-01,  ..., -9.6292e-02,\n",
      "         -7.4251e-02, -4.5132e-01],\n",
      "        ...,\n",
      "        [-2.4692e-01, -8.7262e-02, -2.6696e-01,  ..., -1.3327e-01,\n",
      "         -1.3965e-01,  1.8257e-01],\n",
      "        [-8.8736e-02, -3.2666e-02, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -2.1225e-01,  2.6124e-01],\n",
      "        [-5.8173e-02, -3.3678e-01, -9.6905e-02,  ..., -1.1269e-01,\n",
      "         -4.1085e-01,  1.7880e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.2024e-03,  1.4822e-02,  1.1627e-02,  ...,  2.1786e-02,\n",
      "          7.5529e-03, -3.0193e-04],\n",
      "        [ 3.9354e-01,  9.5282e-01,  1.0931e+00,  ...,  3.1069e-01,\n",
      "         -1.8352e-01,  1.4177e+00],\n",
      "        [ 6.1928e-01,  3.1771e+00, -4.0635e-01,  ...,  2.6696e+00,\n",
      "         -4.9709e-01,  2.2014e+00],\n",
      "        ...,\n",
      "        [ 1.4881e+00,  8.4815e-01, -7.2566e-02,  ..., -0.0000e+00,\n",
      "         -8.0850e-01, -2.1613e-01],\n",
      "        [-1.4844e-01, -1.1700e-01, -0.0000e+00,  ...,  7.2261e-01,\n",
      "         -3.1695e-01,  5.2525e-01],\n",
      "        [-6.3606e-02,  5.7060e-01, -6.1920e-02,  ..., -2.5542e-01,\n",
      "         -0.0000e+00, -4.1883e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.3621e-03,  7.3832e-04, -5.6851e-04,  ...,  4.2886e-05,\n",
      "          4.7649e-04, -1.2722e-03],\n",
      "        [-2.6178e-01, -1.6761e-01, -1.1024e-01,  ..., -1.4526e-02,\n",
      "         -1.9718e-01, -1.5824e-01],\n",
      "        [-1.5366e-01,  1.3754e+00, -2.8200e-02,  ...,  2.7287e+00,\n",
      "         -3.2821e-01,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.0716e-01,  1.5592e+00, -1.1849e-01,  ...,  6.5808e-01,\n",
      "         -3.1311e-01, -1.1361e-01],\n",
      "        [-0.0000e+00, -2.9302e-02, -9.0721e-02,  ..., -1.1757e-01,\n",
      "         -2.2536e-01,  2.7368e-01],\n",
      "        [-5.1231e-02, -2.9102e-02, -3.9996e-02,  ..., -2.5642e-01,\n",
      "         -6.8276e-02, -3.5582e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 5.8772e-03,  2.0789e-02, -4.0311e-03,  ..., -8.5350e-04,\n",
      "         -7.6706e-04,  1.2213e-02],\n",
      "        [-4.2312e-01, -2.0126e-01, -1.7089e-01,  ..., -7.5151e-02,\n",
      "         -4.0399e-01,  1.8641e-01],\n",
      "        [-0.0000e+00,  3.0476e+00, -3.7491e-01,  ...,  4.1463e+00,\n",
      "         -4.0842e-01, -1.4929e-01],\n",
      "        ...,\n",
      "        [-5.1136e-01, -2.4562e-01, -4.1810e-01,  ..., -7.0641e-02,\n",
      "         -0.0000e+00, -2.2270e-01],\n",
      "        [ 1.7428e-01, -4.4050e-01,  6.3187e-01,  ...,  9.9841e-01,\n",
      "          0.0000e+00, -1.2717e-01],\n",
      "        [-5.7819e-01, -1.5810e-01, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          7.1654e-01, -3.1023e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.0928e-02,  0.0000e+00, -3.9827e-03,  ...,  1.3478e-02,\n",
      "         -2.9156e-03, -1.5442e-03],\n",
      "        [-8.2140e-02, -6.1351e-02, -1.4628e-01,  ...,  1.6681e-02,\n",
      "         -2.0508e-01, -1.5040e-01],\n",
      "        [ 2.2914e+00,  2.2940e+00,  0.0000e+00,  ...,  3.8811e-01,\n",
      "         -2.7198e-01, -1.9377e-01],\n",
      "        ...,\n",
      "        [-2.5119e-01,  1.8550e+00, -5.2424e-02,  ..., -7.2054e-01,\n",
      "          2.7982e+00, -1.3125e-01],\n",
      "        [ 9.8651e-01,  0.0000e+00,  1.6077e+00,  ..., -8.2643e-01,\n",
      "         -0.0000e+00, -2.2824e-01],\n",
      "        [ 4.0609e+00, -2.1745e-01,  0.0000e+00,  ..., -3.1950e-01,\n",
      "         -2.8705e-01, -1.1890e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.3529e-02, -3.7301e-04,  0.0000e+00,  ...,  6.6254e-03,\n",
      "          2.3436e-02, -3.0028e-03],\n",
      "        [-8.1490e-02, -6.7229e-02, -1.3949e-01,  ...,  0.0000e+00,\n",
      "         -1.9631e-01, -1.5186e-01],\n",
      "        [ 2.1543e+00,  5.4352e-01,  2.8775e+00,  ...,  3.7260e-01,\n",
      "         -2.2962e-01, -6.9799e-02],\n",
      "        ...,\n",
      "        [-3.0322e-01,  2.2557e+00, -2.1115e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  1.3422e+00],\n",
      "        [ 2.6478e-01,  2.1455e-01, -2.2214e-01,  ..., -3.0464e-01,\n",
      "         -1.7028e-01, -2.5979e-01],\n",
      "        [ 4.0635e+00, -0.0000e+00,  1.7357e+00,  ..., -3.2122e-01,\n",
      "         -0.0000e+00, -1.2036e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 3.4255e-04, -0.0000e+00,  5.6520e-03,  ..., -7.7871e-04,\n",
      "          7.9377e-03, -5.1423e-03],\n",
      "        [-8.4179e-02, -6.7012e-02, -1.3587e-01,  ..., -6.4963e-06,\n",
      "         -0.0000e+00, -1.4870e-01],\n",
      "        [ 2.1271e+00,  1.6248e+00,  2.1734e+00,  ..., -2.0567e-01,\n",
      "         -2.8330e-01,  3.5145e-02],\n",
      "        ...,\n",
      "        [-0.0000e+00,  5.7950e-01, -4.5194e-01,  ..., -3.9764e-01,\n",
      "          3.3770e+00, -7.1710e-02],\n",
      "        [-8.9260e-02, -2.3990e-01, -3.8469e-01,  ..., -1.5116e-01,\n",
      "         -4.9855e-02, -3.0807e-01],\n",
      "        [ 4.0403e+00, -0.0000e+00,  1.7301e+00,  ..., -3.1216e-01,\n",
      "         -2.7210e-01, -1.1832e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 8.5621e-02,  1.7464e-03, -3.5565e-03,  ...,  4.0192e-02,\n",
      "          0.0000e+00,  5.6832e-02],\n",
      "        [ 3.2259e-01,  1.7143e+00, -6.0474e-02,  ..., -1.6636e-01,\n",
      "          2.0426e+00,  2.4185e-01],\n",
      "        [ 2.2895e+00,  0.0000e+00, -3.3519e-01,  ...,  1.1849e+00,\n",
      "         -6.5090e-01,  1.6300e+00],\n",
      "        ...,\n",
      "        [-9.3444e-02, -2.1576e-01, -5.8935e-01,  ...,  1.9467e+00,\n",
      "         -8.3400e-01,  2.2794e+00],\n",
      "        [-2.4372e-01, -1.8074e-01,  1.0693e+00,  ..., -7.9380e-02,\n",
      "         -9.9101e-01,  3.4974e-01],\n",
      "        [-0.0000e+00, -6.0562e-01, -5.5052e-01,  ..., -3.7253e-01,\n",
      "         -6.9580e-01,  0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0070,  0.0520,  0.0000,  ..., -0.0000, -0.0081,  0.0402],\n",
      "        [ 0.7453,  0.2941, -0.2291,  ..., -0.0871,  2.3411, -0.2159],\n",
      "        [ 0.6692,  0.8994, -0.4758,  ...,  4.0302, -0.6047,  0.0000],\n",
      "        ...,\n",
      "        [-0.2839,  0.0000,  0.8351,  ...,  1.5711, -0.0000,  1.5326],\n",
      "        [-0.0000,  0.7263,  1.4880,  ..., -0.0000, -0.2787,  0.2760],\n",
      "        [-0.2939,  1.6633,  0.7871,  ..., -0.0280, -0.3041,  1.3241]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.0699e-03,  2.7455e-02, -6.9603e-03,  ..., -3.4036e-04,\n",
      "         -2.4506e-03, -8.6086e-04],\n",
      "        [-4.1880e-01, -9.7054e-02, -1.9333e-01,  ..., -3.0273e-01,\n",
      "          3.3060e+00, -3.1510e-01],\n",
      "        [-2.9559e-01, -4.2083e-01,  2.4070e+00,  ...,  3.3314e+00,\n",
      "         -9.5462e-02, -9.0568e-01],\n",
      "        ...,\n",
      "        [-3.5466e-01,  1.4943e+00, -3.7998e-01,  ...,  2.4920e+00,\n",
      "         -2.8593e-01,  1.0511e+00],\n",
      "        [-8.1565e-01, -4.4128e-01, -2.7979e-01,  ..., -8.8195e-02,\n",
      "         -2.5194e-01, -4.9384e-02],\n",
      "        [-3.3151e-01, -0.0000e+00, -1.0106e-01,  ..., -1.7997e-02,\n",
      "         -1.0540e-01, -1.4346e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-3.5227e-03, -4.1456e-03, -1.8621e-03,  ..., -0.0000e+00,\n",
      "         -6.5881e-04, -3.5848e-03],\n",
      "        [-2.7334e-02, -5.8433e-02,  1.1974e+00,  ..., -2.6417e-01,\n",
      "         -2.3550e-01, -2.3261e-03],\n",
      "        [-1.8140e-01,  1.8809e+00,  3.7358e-01,  ...,  1.9675e-01,\n",
      "         -2.6399e-01, -4.1319e-01],\n",
      "        ...,\n",
      "        [-8.1150e-02,  0.0000e+00, -6.7286e-02,  ..., -2.3600e-01,\n",
      "         -2.7421e-01, -1.4418e-01],\n",
      "        [ 0.0000e+00, -3.3370e-01, -2.7488e-01,  ..., -2.8380e-02,\n",
      "         -2.0306e-01,  2.0518e-01],\n",
      "        [-6.7614e-02,  0.0000e+00, -1.7822e-01,  ..., -3.2232e-01,\n",
      "         -2.6903e-01, -6.6657e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0204,  0.0000,  0.0061,  ...,  0.0000,  0.0153, -0.0042],\n",
      "        [-0.0187, -0.0000,  0.0000,  ..., -0.2613, -0.2310, -0.0029],\n",
      "        [-0.0080,  0.2471, -0.1249,  ..., -0.0457,  2.0464,  2.1581],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0631, -0.0000,  ..., -0.3103,  0.4585, -0.0212],\n",
      "        [-0.3240,  0.2928, -0.1960,  ...,  0.4963, -0.0798, -0.2445],\n",
      "        [-0.3619, -0.1858, -0.0000,  ..., -0.2583,  0.0700,  0.5293]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0000e+00, -2.8091e-03, -3.3409e-03,  ..., -9.8651e-04,\n",
      "          0.0000e+00, -3.8591e-04],\n",
      "        [-2.1916e-02, -5.5160e-02,  1.1910e+00,  ..., -2.5337e-01,\n",
      "         -2.2632e-01,  3.4345e-03],\n",
      "        [-0.0000e+00,  2.7084e-02, -6.9044e-01,  ..., -5.6409e-01,\n",
      "          1.0947e-03, -2.5042e-01],\n",
      "        ...,\n",
      "        [-7.2961e-02, -2.8518e-03, -1.9527e-01,  ..., -5.3870e-01,\n",
      "         -7.0760e-02,  1.4517e-01],\n",
      "        [-2.6319e-01, -2.1337e-01, -3.0265e-01,  ..., -1.4078e-01,\n",
      "         -1.7079e-01,  5.2041e-01],\n",
      "        [-2.2799e-01, -0.0000e+00, -3.2792e-01,  ..., -3.2849e-01,\n",
      "         -1.0932e-01, -1.9680e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 6.5487e-02,  9.3921e-04, -2.1424e-03,  ...,  5.6638e-02,\n",
      "          3.7202e-02,  1.4599e-02],\n",
      "        [-1.9197e-01, -3.3295e-02,  1.2332e+00,  ..., -2.3660e-01,\n",
      "         -3.1440e-01,  2.3473e-01],\n",
      "        [-7.2105e-02, -1.7331e-01, -1.1003e+00,  ..., -0.0000e+00,\n",
      "         -5.8027e-01, -2.1332e-01],\n",
      "        ...,\n",
      "        [ 2.1037e+00,  2.7456e+00, -2.9468e-01,  ...,  2.8896e+00,\n",
      "         -4.9540e-01,  0.0000e+00],\n",
      "        [-5.6928e-02,  2.0235e-01, -6.5260e-01,  ..., -3.2182e-01,\n",
      "          0.0000e+00,  3.0445e-01],\n",
      "        [ 5.5481e-01,  3.8345e-01, -6.3282e-01,  ..., -4.3835e-03,\n",
      "         -1.2898e-01,  1.0521e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.0392e-02,  2.5205e-02,  6.7761e-03,  ..., -2.0823e-03,\n",
      "         -7.5131e-03,  5.8921e-03],\n",
      "        [-0.0000e+00, -3.5346e-01,  1.4276e+00,  ..., -3.6985e-01,\n",
      "         -3.8177e-01,  1.6441e-01],\n",
      "        [-2.3736e-01, -3.7095e-01, -0.0000e+00,  ..., -2.7152e-01,\n",
      "         -8.4750e-02,  2.0857e-01],\n",
      "        ...,\n",
      "        [-5.4007e-01,  2.1149e+00, -1.2302e-01,  ...,  1.4039e+00,\n",
      "         -5.1420e-01, -4.7052e-02],\n",
      "        [-9.1461e-03,  1.4640e+00, -1.3970e-01,  ...,  7.8828e-01,\n",
      "          7.8668e-01, -2.4390e-01],\n",
      "        [ 7.4594e-01,  1.6451e+00, -1.1991e-01,  ...,  2.0580e+00,\n",
      "         -4.2665e-01, -5.6992e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0113,  0.0342, -0.0056,  ..., -0.0000, -0.0028,  0.0127],\n",
      "        [-0.4135, -0.0000,  1.6821,  ..., -0.2103,  0.2005,  0.1453],\n",
      "        [ 0.2868, -0.0000, -0.1663,  ..., -0.0837,  0.3687, -0.2491],\n",
      "        ...,\n",
      "        [-1.6937,  1.1819, -0.4710,  ..., -0.1228, -0.3481, -0.4540],\n",
      "        [-0.5465, -0.1718, -0.1757,  ...,  0.6771,  1.3757, -0.4218],\n",
      "        [-0.3609, -0.1289, -0.1569,  ...,  1.9469, -0.2651, -0.2444]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0296,  0.0000, -0.0036,  ...,  0.0177,  0.0107,  0.0078],\n",
      "        [-0.0784, -0.0645, -0.1461,  ..., -0.2187, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.6290, -0.4702, -0.0744],\n",
      "        ...,\n",
      "        [-0.0157, -0.0515,  1.1887,  ..., -0.2581, -0.2336, -0.1896],\n",
      "        [ 3.6128, -0.3226, -0.1660,  ..., -0.1641, -0.5657, -0.1639],\n",
      "        [ 0.0000,  0.3431,  1.3318,  ..., -0.3133, -0.2763, -0.0251]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0197,  0.0061,  0.0070,  ...,  0.0062,  0.0252, -0.0020],\n",
      "        [-0.0809, -0.0655, -0.1407,  ..., -0.2215, -0.1959, -0.1518],\n",
      "        [ 0.2722,  1.4380,  0.2480,  ...,  0.0946,  0.1942, -0.0000],\n",
      "        ...,\n",
      "        [-0.0182, -0.0525,  1.2103,  ..., -0.2610, -0.2300, -0.1935],\n",
      "        [ 0.6969,  1.1207,  1.3266,  ...,  0.7108,  1.1376, -0.4544],\n",
      "        [ 1.5357,  0.3391,  1.3534,  ..., -0.3162, -0.0000, -0.0290]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-5.6554e-04, -6.4421e-04, -6.3741e-04,  ..., -1.4570e-03,\n",
      "          5.6162e-03, -2.6084e-03],\n",
      "        [-8.3276e-02, -6.5315e-02, -0.0000e+00,  ..., -2.1657e-01,\n",
      "         -1.9364e-01, -1.4711e-01],\n",
      "        [-8.1097e-02,  1.6670e+00, -0.0000e+00,  ..., -3.5592e-01,\n",
      "          6.1256e-01, -9.0801e-02],\n",
      "        ...,\n",
      "        [-2.2839e-02, -5.2767e-02,  1.2006e+00,  ..., -2.5464e-01,\n",
      "         -2.2648e-01, -1.8729e-01],\n",
      "        [ 6.9460e-01,  1.6455e-01, -3.3435e-01,  ..., -3.9646e-01,\n",
      "         -1.6076e-01, -0.0000e+00],\n",
      "        [ 1.5136e+00,  3.3037e-01,  1.3437e+00,  ..., -3.0782e-01,\n",
      "         -0.0000e+00, -2.8648e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0348,  0.0433,  0.0185,  ...,  0.0251, -0.0102, -0.0038],\n",
      "        [ 1.3007, -0.0000,  2.1282,  ..., -0.0432, -0.5553,  3.6527],\n",
      "        [-0.0000,  0.6372, -0.4897,  ...,  3.5511, -0.3157,  1.5234],\n",
      "        ...,\n",
      "        [ 0.3518,  0.5325, -0.2337,  ..., -0.5193, -0.2778, -0.0516],\n",
      "        [ 0.3518,  0.5325, -0.0000,  ..., -0.5193, -0.2778, -0.0516],\n",
      "        [ 0.3518,  0.5325, -0.2337,  ..., -0.3515, -0.2778, -0.0516]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.3548e-02,  4.3500e-02,  3.7538e-02,  ..., -2.2747e-03,\n",
      "         -1.0336e-02,  5.4390e-03],\n",
      "        [-1.8396e-01, -9.6213e-03,  1.5054e+00,  ...,  1.0449e-01,\n",
      "         -2.9495e-01,  2.6908e+00],\n",
      "        [-6.4771e-01,  7.4234e-01,  0.0000e+00,  ...,  1.2931e+00,\n",
      "         -3.7959e-01,  1.3976e+00],\n",
      "        ...,\n",
      "        [-5.4269e-01, -1.8441e-01, -0.0000e+00,  ..., -4.6172e-01,\n",
      "         -1.9317e-01, -5.8754e-01],\n",
      "        [-0.0000e+00, -1.8441e-01, -2.4435e-02,  ..., -4.6172e-01,\n",
      "         -1.9317e-01, -5.8754e-01],\n",
      "        [-5.4269e-01, -0.0000e+00, -2.4435e-02,  ..., -0.0000e+00,\n",
      "         -1.9317e-01, -5.8754e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.0806e-03,  3.6462e-03, -3.6758e-04,  ...,  9.2982e-04,\n",
      "         -0.0000e+00, -1.2526e-04],\n",
      "        [-0.0000e+00, -4.7285e-01,  7.9190e-01,  ..., -2.1396e-01,\n",
      "         -4.3947e-01,  2.4452e+00],\n",
      "        [-7.2124e-01,  0.0000e+00, -2.5238e-01,  ...,  3.6548e+00,\n",
      "         -0.0000e+00, -6.2027e-02],\n",
      "        ...,\n",
      "        [-9.1084e-01, -8.6977e-01, -4.7540e-01,  ..., -1.4175e-01,\n",
      "          1.3614e-01, -7.3386e-01],\n",
      "        [-0.0000e+00, -8.6977e-01, -4.7540e-01,  ..., -1.4175e-01,\n",
      "          1.3614e-01, -7.3386e-01],\n",
      "        [-0.0000e+00, -8.6977e-01, -4.7540e-01,  ...,  7.3628e-02,\n",
      "          1.3614e-01, -7.3386e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.3294e-03,  2.8828e-02,  4.4319e-03,  ..., -0.0000e+00,\n",
      "          1.0178e-03,  6.1731e-04],\n",
      "        [-8.7708e-02, -6.0351e-02, -1.4133e-01,  ..., -2.2634e-01,\n",
      "         -2.0192e-01,  5.6805e-03],\n",
      "        [-2.2645e-01,  6.5195e-01,  2.5032e+00,  ..., -1.9910e-01,\n",
      "         -5.9773e-01,  2.2275e+00],\n",
      "        ...,\n",
      "        [ 4.0410e+00, -0.0000e+00,  1.7268e+00,  ..., -3.2715e-01,\n",
      "         -2.8402e-01, -1.1123e-01],\n",
      "        [-8.6352e-02,  3.2329e+00, -2.1912e-01,  ..., -3.4809e-01,\n",
      "          2.6706e-01, -0.0000e+00],\n",
      "        [ 4.0410e+00, -2.1571e-01,  1.7268e+00,  ..., -3.2715e-01,\n",
      "         -2.8402e-01, -1.1123e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0124,  0.0053, -0.0087,  ..., -0.0134,  0.0441,  0.0215],\n",
      "        [-0.0823, -0.0662, -0.0000,  ..., -0.2359, -0.1911,  0.0265],\n",
      "        [-0.1347,  0.4894,  2.4800,  ..., -0.3708, -0.4116,  2.7248],\n",
      "        ...,\n",
      "        [ 4.0627, -0.2216,  1.6876,  ..., -0.3367, -0.2732, -0.1060],\n",
      "        [-0.1597,  2.5445, -0.0135,  ...,  1.8931,  0.5358,  2.1305],\n",
      "        [ 4.0627, -0.2216,  1.6876,  ..., -0.3367, -0.2732, -0.1060]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 3.8285e-04, -1.4614e-03, -1.5461e-03,  ..., -1.6687e-03,\n",
      "         -8.9607e-05, -0.0000e+00],\n",
      "        [-8.2231e-02, -6.6602e-02, -0.0000e+00,  ..., -2.1615e-01,\n",
      "         -1.9503e-01, -2.9858e-04],\n",
      "        [-5.6621e-02,  1.3837e+00,  2.8006e+00,  ..., -0.0000e+00,\n",
      "         -3.2947e-01,  2.0394e+00],\n",
      "        ...,\n",
      "        [ 4.0507e+00, -2.1640e-01,  1.7160e+00,  ..., -0.0000e+00,\n",
      "         -2.7419e-01, -1.0892e-01],\n",
      "        [ 5.5782e-01,  0.0000e+00, -7.5658e-02,  ..., -9.0208e-01,\n",
      "          9.1972e-02,  5.4201e-02],\n",
      "        [ 4.0507e+00, -2.1640e-01,  1.7160e+00,  ..., -3.1335e-01,\n",
      "         -2.7419e-01, -1.0892e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0000, -0.0072,  0.0148,  ...,  0.0401,  0.0515,  0.0499],\n",
      "        [ 0.8729, -0.1191,  1.5305,  ..., -0.1085, -0.3618,  4.3360],\n",
      "        [-0.2009,  3.2599, -0.5423,  ...,  2.5377, -0.4758,  1.3745],\n",
      "        ...,\n",
      "        [-0.2844,  0.5313, -0.0000,  ..., -0.7127, -1.1687, -0.4647],\n",
      "        [ 3.1309, -0.2113, -1.1245,  ..., -1.2370, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.2395, -1.1693,  ..., -0.0000, -1.1509,  1.9424]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0205,  0.0000,  0.0618,  ..., -0.0053, -0.0148,  0.0125],\n",
      "        [-0.1787, -0.3218,  2.0726,  ..., -0.0659, -0.1213,  2.8402],\n",
      "        [-0.2493,  2.2170, -0.0287,  ...,  3.6381, -0.3990,  0.9999],\n",
      "        ...,\n",
      "        [-0.2028, -0.2135, -0.7367,  ..., -0.0000, -0.6593, -0.2055],\n",
      "        [-0.0000, -0.4563, -0.0000,  ..., -0.6288, -0.6340, -0.1923],\n",
      "        [ 0.0252, -0.4620, -0.3226,  ..., -0.4694, -0.9260,  1.0862]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-6.6582e-03,  2.0577e-02, -5.8428e-03,  ..., -3.8844e-03,\n",
      "         -4.8702e-03, -3.4890e-03],\n",
      "        [-5.0178e-01, -5.7605e-01, -9.5443e-04,  ..., -3.1364e-01,\n",
      "         -4.4064e-01,  2.3977e+00],\n",
      "        [-5.1896e-01,  3.5408e+00, -1.5772e-01,  ...,  4.9091e+00,\n",
      "         -3.9095e-01, -1.4752e-01],\n",
      "        ...,\n",
      "        [-4.0499e-01,  1.0263e+00, -1.9600e-01,  ..., -2.0661e-01,\n",
      "         -2.1437e-01, -1.7846e-02],\n",
      "        [-3.8449e-01, -4.4600e-01, -1.9681e-01,  ..., -4.5643e-01,\n",
      "          2.0217e+00,  1.1217e-01],\n",
      "        [-7.0455e-01, -7.5612e-02, -3.0289e-01,  ..., -2.6205e-01,\n",
      "         -6.3232e-02,  2.1470e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.2118e-02,  2.2868e-02, -4.4458e-03,  ..., -1.1033e-03,\n",
      "          1.3177e-02, -4.9777e-05],\n",
      "        [-7.9057e-02, -6.2105e-02, -1.4688e-01,  ..., -2.2360e-01,\n",
      "         -0.0000e+00, -1.4840e-01],\n",
      "        [ 1.9093e+00,  2.7419e+00,  0.0000e+00,  ..., -1.9573e-01,\n",
      "         -3.2576e-01, -3.4845e-02],\n",
      "        ...,\n",
      "        [ 4.9799e-01,  6.7782e-01,  5.9007e-02,  ..., -0.0000e+00,\n",
      "          3.9583e+00,  2.7906e-01],\n",
      "        [ 2.3651e-01,  5.7820e-01,  4.1334e-01,  ..., -3.5887e-01,\n",
      "          1.9767e+00, -2.7196e-01],\n",
      "        [ 2.1582e-01,  9.4262e-01,  4.2315e+00,  ...,  1.1973e+00,\n",
      "          2.8327e-01,  0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-4.0650e-03,  2.6762e-02, -3.0998e-03,  ...,  9.7083e-03,\n",
      "          1.1889e-02,  1.6687e-02],\n",
      "        [-8.8652e-02, -6.1132e-02, -0.0000e+00,  ..., -2.2007e-01,\n",
      "         -1.9918e-01, -1.4418e-01],\n",
      "        [ 1.9470e+00,  8.1038e-01,  2.5242e+00,  ..., -1.6760e-02,\n",
      "         -2.0565e-01,  2.9652e-01],\n",
      "        ...,\n",
      "        [-2.8314e-01, -0.0000e+00, -4.3767e-02,  ...,  1.6582e+00,\n",
      "          4.4920e+00, -2.7196e-01],\n",
      "        [ 9.4832e-02, -1.5615e-02, -8.7141e-02,  ..., -1.4935e-01,\n",
      "          4.2136e+00, -1.3295e-01],\n",
      "        [-1.6485e-01,  1.0719e+00,  3.3728e+00,  ...,  2.0254e+00,\n",
      "          0.0000e+00,  3.5769e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-3.2471e-04, -1.5158e-04, -6.5661e-04,  ..., -2.6970e-03,\n",
      "         -5.9255e-04, -1.2397e-03],\n",
      "        [-8.1885e-02, -0.0000e+00, -1.3800e-01,  ..., -2.1723e-01,\n",
      "         -1.9552e-01, -1.4428e-01],\n",
      "        [ 0.0000e+00,  1.3856e+00,  2.1606e+00,  ..., -3.3573e-01,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-1.8133e-01,  1.3675e+00, -6.7069e-02,  ...,  1.6928e-01,\n",
      "          3.0659e+00,  2.7239e-01],\n",
      "        [ 1.9308e-02, -1.4165e-01, -6.9372e-02,  ...,  0.0000e+00,\n",
      "          3.6898e+00, -2.6176e-01],\n",
      "        [-1.2746e-01,  0.0000e+00,  2.0034e+00,  ...,  0.0000e+00,\n",
      "          1.3387e+00,  2.5244e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 4.2031e-02, -2.0853e-03,  5.2743e-02,  ...,  5.7810e-02,\n",
      "          1.2614e-02,  1.4669e-02],\n",
      "        [ 0.0000e+00, -1.8786e-01,  2.4372e+00,  ...,  4.2831e-02,\n",
      "         -3.0769e-01,  4.4052e+00],\n",
      "        [ 4.4338e-01,  3.2829e+00, -3.9320e-01,  ...,  1.6373e+00,\n",
      "         -5.2328e-01,  0.0000e+00],\n",
      "        ...,\n",
      "        [-4.0309e-01, -3.6956e-01, -3.9324e-01,  ..., -4.5786e-01,\n",
      "         -7.3977e-01, -4.3935e-01],\n",
      "        [-0.0000e+00,  8.6603e-01, -2.6589e-01,  ..., -7.3804e-01,\n",
      "         -1.3311e+00, -6.4699e-01],\n",
      "        [-8.4857e-01,  1.3431e+00, -3.0668e-01,  ..., -0.0000e+00,\n",
      "         -1.2507e+00, -7.1200e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0811,  0.0000, -0.0131,  ...,  0.0309, -0.0109, -0.0057],\n",
      "        [-0.1313, -0.1857,  1.0701,  ...,  0.0821, -0.2129,  2.4678],\n",
      "        [ 0.5662,  2.5056, -0.3414,  ...,  2.4284, -0.3476,  1.5596],\n",
      "        ...,\n",
      "        [-0.0621, -0.5457, -0.6123,  ..., -0.1153, -0.6230,  0.2419],\n",
      "        [ 0.4777, -0.2911, -0.0000,  ..., -0.0679, -1.0236,  1.1359],\n",
      "        [ 0.2271,  0.2930, -1.2426,  ..., -0.4733, -0.0000,  0.2781]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-5.9258e-04,  1.1786e-02, -2.4014e-03,  ..., -3.3986e-04,\n",
      "         -1.4249e-04,  2.7944e-04],\n",
      "        [-4.6304e-01, -5.2445e-01,  0.0000e+00,  ..., -2.4078e-01,\n",
      "         -3.8021e-01,  2.8653e+00],\n",
      "        [-4.0040e-01,  4.0471e+00, -9.3325e-02,  ...,  3.7855e+00,\n",
      "         -0.0000e+00, -1.2903e-01],\n",
      "        ...,\n",
      "        [-5.3736e-01, -4.2579e-01, -2.8818e-01,  ..., -9.9104e-02,\n",
      "         -1.5070e-01, -2.9032e-01],\n",
      "        [-4.7007e-01,  0.0000e+00, -9.8613e-02,  ..., -2.6396e-02,\n",
      "         -2.7408e-01, -2.4905e-01],\n",
      "        [-3.6068e-01,  1.0789e+00,  5.4911e-01,  ..., -8.8526e-02,\n",
      "         -7.5882e-02, -1.8120e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.1211e-02,  2.0903e-02, -6.6694e-03,  ...,  1.9536e-02,\n",
      "         -0.0000e+00, -9.1173e-04],\n",
      "        [-7.8601e-02, -6.2785e-02, -1.4815e-01,  ..., -2.1843e-01,\n",
      "         -2.0496e-01, -1.4915e-01],\n",
      "        [ 2.9873e+00,  0.0000e+00,  2.3874e+00,  ..., -1.4055e-01,\n",
      "         -4.0441e-01, -2.1829e-01],\n",
      "        ...,\n",
      "        [-3.1301e-01,  1.5866e+00, -3.6928e-02,  ..., -4.1988e-01,\n",
      "          3.9097e-01, -1.4417e-01],\n",
      "        [-2.1422e-01,  1.4094e+00, -2.2197e-01,  ..., -1.1271e-01,\n",
      "          8.2269e-01, -1.6506e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00, -2.4385e-01,  ..., -3.8249e-01,\n",
      "          8.7402e-01,  1.9570e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.8848e-03,  5.2668e-03, -1.9035e-03,  ...,  1.0178e-02,\n",
      "          1.3599e-02,  1.3919e-02],\n",
      "        [-8.6788e-02, -6.6694e-02, -1.4338e-01,  ..., -2.2077e-01,\n",
      "         -1.9873e-01, -1.4476e-01],\n",
      "        [ 2.2041e+00,  1.0997e+00,  2.5155e+00,  ..., -2.5921e-01,\n",
      "         -0.0000e+00,  6.7085e-02],\n",
      "        ...,\n",
      "        [-0.0000e+00,  2.0829e+00, -2.3357e-01,  ..., -0.0000e+00,\n",
      "          1.9740e+00, -0.0000e+00],\n",
      "        [-1.0237e-01,  1.4214e+00, -0.0000e+00,  ..., -2.7043e-01,\n",
      "          0.0000e+00,  7.2686e-01],\n",
      "        [-2.1397e-01, -3.0544e-01, -4.9782e-01,  ..., -9.2816e-02,\n",
      "          1.6524e+00, -6.5711e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 4.4392e-03, -7.2081e-04, -1.4126e-03,  ..., -1.9740e-03,\n",
      "         -1.0430e-03,  1.1382e-03],\n",
      "        [-7.9834e-02, -6.6300e-02, -1.3784e-01,  ..., -2.1731e-01,\n",
      "         -1.9595e-01, -1.4267e-01],\n",
      "        [ 2.3740e+00,  1.5652e+00,  2.8212e+00,  ..., -2.5082e-01,\n",
      "         -2.4666e-01, -1.5012e-01],\n",
      "        ...,\n",
      "        [-4.2043e-01,  9.9463e-01, -1.3981e-01,  ..., -0.0000e+00,\n",
      "          1.5909e+00, -9.3198e-02],\n",
      "        [-3.3532e-01,  1.4082e+00, -2.0040e-01,  ..., -1.0211e-01,\n",
      "          1.1719e+00,  1.1666e-01],\n",
      "        [-4.0925e-01, -1.8978e-01, -2.4995e-01,  ..., -5.2527e-02,\n",
      "          1.4916e+00, -1.7905e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-8.8866e-04,  7.4535e-03,  1.3972e-02,  ...,  3.0190e-02,\n",
      "          1.4809e-03, -3.4291e-03],\n",
      "        [-1.2805e-01, -1.7111e-01,  1.8969e+00,  ...,  7.2285e-01,\n",
      "         -2.3689e-01,  2.4750e+00],\n",
      "        [-3.2333e-02, -0.0000e+00, -2.6759e-01,  ..., -4.3612e-01,\n",
      "         -6.9939e-01, -0.0000e+00],\n",
      "        ...,\n",
      "        [-2.8077e-01,  2.4561e+00, -3.5879e-01,  ..., -2.1146e-01,\n",
      "         -2.7296e-01,  3.3019e-01],\n",
      "        [ 2.1499e+00, -1.7175e-01, -6.5389e-01,  ...,  1.2947e+00,\n",
      "         -6.1959e-01,  3.2404e+00],\n",
      "        [ 1.6391e+00, -1.2336e-02, -3.0692e-01,  ...,  6.2266e-01,\n",
      "         -2.8401e-01,  3.6666e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0668,  0.0465,  0.0648,  ...,  0.0486, -0.0130,  0.0557],\n",
      "        [-0.4285, -0.2664,  2.5887,  ..., -0.2710, -0.6126,  3.3457],\n",
      "        [ 1.0206,  1.5396,  0.1925,  ..., -0.2917, -0.4332,  0.8305],\n",
      "        ...,\n",
      "        [ 1.6776,  0.0000, -0.2571,  ..., -0.1403, -0.2774,  1.9723],\n",
      "        [ 0.0000, -0.0331, -0.3744,  ...,  0.9266, -0.0000,  0.0000],\n",
      "        [ 0.9807, -0.2825, -0.7043,  ..., -0.6796, -0.6859,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-6.4422e-03,  1.4944e-02, -9.5324e-03,  ..., -4.0008e-03,\n",
      "         -5.8370e-03, -2.2902e-03],\n",
      "        [-3.1834e-01, -0.0000e+00,  9.6648e-01,  ...,  6.8065e-01,\n",
      "         -3.2864e-01,  2.6320e+00],\n",
      "        [-3.1067e-01, -3.5832e-01,  1.3087e+00,  ..., -1.0285e-01,\n",
      "         -1.8436e-01, -0.0000e+00],\n",
      "        ...,\n",
      "        [-3.0553e-01,  1.8811e+00, -4.1851e-01,  ..., -4.6958e-01,\n",
      "          1.2583e-01,  4.8633e-01],\n",
      "        [-7.2502e-01, -2.5317e-01, -2.6283e-01,  ...,  2.6946e+00,\n",
      "         -2.0218e-01,  1.6080e+00],\n",
      "        [-6.7086e-01, -1.7668e-01, -5.7118e-01,  ..., -1.0096e-01,\n",
      "          1.4577e+00,  2.8120e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.6958e-02,  8.3038e-03, -1.4173e-03,  ...,  8.7653e-03,\n",
      "          0.0000e+00, -2.8109e-03],\n",
      "        [-0.0000e+00, -0.0000e+00, -1.4385e-01,  ..., -2.2046e-01,\n",
      "         -1.9915e-01, -1.5105e-01],\n",
      "        [ 1.8859e-01,  2.2749e+00, -5.8535e-02,  ..., -2.0714e-01,\n",
      "         -1.8460e-01,  2.3832e-02],\n",
      "        ...,\n",
      "        [-3.8229e-01,  9.9848e-01,  7.0690e-01,  ..., -3.8772e-01,\n",
      "         -7.3343e-01, -1.5954e-01],\n",
      "        [-1.3179e-01, -1.4463e-01,  1.8403e+00,  ...,  2.3193e+00,\n",
      "          1.6167e+00, -5.2788e-02],\n",
      "        [-2.4225e-01,  2.9858e-02,  7.9285e-01,  ...,  3.8477e+00,\n",
      "          6.0304e-01, -6.0376e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-3.9905e-04,  1.6038e-02,  1.7106e-02,  ...,  3.0868e-02,\n",
      "          7.5654e-03,  6.1242e-03],\n",
      "        [-8.3175e-02, -6.5183e-02, -1.3816e-01,  ..., -2.1494e-01,\n",
      "         -2.0026e-01, -1.4670e-01],\n",
      "        [-1.6877e-01, -3.2810e-02, -1.1360e-01,  ..., -2.8790e-01,\n",
      "         -7.2272e-02,  1.7958e+00],\n",
      "        ...,\n",
      "        [-3.5921e-01,  8.2482e-02, -0.0000e+00,  ..., -5.2188e-01,\n",
      "         -5.4162e-02,  9.0300e-01],\n",
      "        [-3.0247e-01, -5.0328e-02,  1.5547e+00,  ...,  0.0000e+00,\n",
      "          1.4080e+00, -1.9725e-01],\n",
      "        [-2.5773e-01,  1.0574e+00,  1.0021e+00,  ...,  0.0000e+00,\n",
      "          1.0050e+00, -1.6515e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-6.4781e-04, -0.0000e+00, -2.8785e-04,  ..., -0.0000e+00,\n",
      "          7.0195e-03, -0.0000e+00],\n",
      "        [-8.0465e-02, -0.0000e+00, -1.3763e-01,  ..., -2.1675e-01,\n",
      "         -0.0000e+00, -1.4584e-01],\n",
      "        [-1.9340e-01,  1.1637e+00, -5.8968e-01,  ..., -5.0761e-01,\n",
      "         -1.0893e-01, -1.5252e-01],\n",
      "        ...,\n",
      "        [-4.5342e-01, -8.4873e-02, -3.1986e-01,  ..., -2.6178e-01,\n",
      "         -0.0000e+00,  1.0061e-01],\n",
      "        [-0.0000e+00, -1.4983e-01,  1.3801e+00,  ...,  3.5220e+00,\n",
      "          2.2181e+00, -2.7663e-01],\n",
      "        [-2.1694e-01,  6.9543e-01, -4.5381e-02,  ...,  0.0000e+00,\n",
      "          1.4072e+00, -0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 9.1131e-02,  4.1291e-02,  2.3216e-02,  ...,  5.5670e-02,\n",
      "          1.6269e-03,  7.3009e-02],\n",
      "        [-1.9570e-01, -3.7461e-01,  1.0179e+00,  ..., -4.3096e-02,\n",
      "         -5.4593e-01,  2.4344e+00],\n",
      "        [-8.5492e-02, -2.2976e-01, -4.4885e-01,  ..., -2.9729e-01,\n",
      "         -3.1762e-01, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 1.0806e+00,  2.9506e+00, -1.2117e-02,  ...,  4.4550e+00,\n",
      "         -2.8623e-01,  4.5164e+00],\n",
      "        [ 1.0020e+00, -3.3222e-02, -3.1074e-01,  ..., -3.5952e-02,\n",
      "         -4.7285e-01,  8.8486e-01],\n",
      "        [ 1.8331e-01, -0.0000e+00,  2.1033e+00,  ...,  1.9054e+00,\n",
      "         -1.9757e-01,  1.3489e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.2500,  0.1432,  0.0793,  ...,  0.0592, -0.0380, -0.0000],\n",
      "        [-0.4073, -0.0277,  2.0975,  ..., -0.2099, -0.5298,  3.0570],\n",
      "        [-0.3134,  1.0361,  0.0000,  ..., -0.5744, -0.6574, -0.1845],\n",
      "        ...,\n",
      "        [ 1.2988,  3.7567,  1.0588,  ...,  3.6328, -0.2112,  1.2300],\n",
      "        [ 0.1020,  0.9905,  0.3447,  ..., -0.2247, -0.0000, -0.1451],\n",
      "        [-0.0888, -0.0775,  2.7254,  ..., -0.0047, -0.3039,  0.7427]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 5.8883e-03,  1.4831e-02, -8.3076e-03,  ...,  0.0000e+00,\n",
      "         -1.7085e-03,  4.4979e-03],\n",
      "        [-3.7495e-01, -3.2911e-01,  1.5817e+00,  ..., -2.2436e-02,\n",
      "         -3.6195e-01,  2.4153e+00],\n",
      "        [-6.4286e-01, -2.5361e-01, -4.8615e-02,  ..., -1.3586e-01,\n",
      "         -0.0000e+00, -3.2229e-01],\n",
      "        ...,\n",
      "        [-9.1549e-01,  2.1450e+00,  1.6754e-01,  ...,  3.9620e+00,\n",
      "         -3.4362e-01,  5.4097e-01],\n",
      "        [-5.5989e-01, -4.9101e-01, -2.2185e-01,  ..., -1.4943e-01,\n",
      "         -0.0000e+00, -5.1558e-01],\n",
      "        [-3.0145e-01, -2.6793e-01,  1.4801e+00,  ...,  6.6379e-01,\n",
      "         -3.5594e-01,  1.4746e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0059, -0.0062,  0.0108,  ...,  0.0143,  0.0021,  0.0000],\n",
      "        [-0.0815, -0.0000, -0.1397,  ..., -0.0000, -0.2016, -0.1467],\n",
      "        [-0.2684,  1.5920, -0.1975,  ..., -0.1731, -0.3336, -0.1020],\n",
      "        ...,\n",
      "        [-0.2746,  1.7495, -0.2513,  ..., -0.2856, -0.4891, -0.0396],\n",
      "        [ 0.8829, -0.2517, -0.2737,  ..., -0.0095, -0.0428,  0.7519],\n",
      "        [-0.0815, -0.0753, -0.0000,  ..., -0.2192, -0.2016,  0.0109]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0023,  0.0317, -0.0058,  ...,  0.0319,  0.0000,  0.0069],\n",
      "        [-0.0853,  0.0343, -0.1482,  ..., -0.0000, -0.1876, -0.1465],\n",
      "        [-0.0974,  1.7839, -0.0694,  ..., -0.2236, -0.3098,  0.2850],\n",
      "        ...,\n",
      "        [-0.2873,  1.8114, -0.2576,  ..., -0.4158, -0.2810,  0.3373],\n",
      "        [-0.1771,  0.3735, -0.0705,  ...,  1.6427,  0.0643, -0.1004],\n",
      "        [-0.0853, -0.0612, -0.1482,  ..., -0.2148, -0.1876,  0.0120]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 4.0483e-03, -1.2719e-04, -2.0221e-03,  ..., -8.2568e-04,\n",
      "          3.2274e-03, -3.7180e-03],\n",
      "        [-7.9049e-02,  2.0393e-03, -1.3936e-01,  ..., -2.1560e-01,\n",
      "         -1.9414e-01, -1.4660e-01],\n",
      "        [-2.2420e-01,  1.9049e+00, -2.7283e-01,  ..., -0.0000e+00,\n",
      "         -6.2603e-02, -6.2036e-02],\n",
      "        ...,\n",
      "        [-2.5452e-01,  1.6993e+00, -3.2457e-01,  ..., -3.1466e-01,\n",
      "         -1.9519e-01,  1.9638e-01],\n",
      "        [-2.2087e-01,  1.3774e-01, -3.4953e-01,  ..., -4.9141e-02,\n",
      "         -1.8697e-01, -2.6928e-02],\n",
      "        [-7.9049e-02, -6.6765e-02, -1.3936e-01,  ..., -2.1560e-01,\n",
      "         -1.9414e-01, -2.4847e-03]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.4136e-02,  0.0000e+00, -7.8362e-04,  ...,  9.3369e-03,\n",
      "         -6.9652e-03, -2.3533e-03],\n",
      "        [-0.0000e+00, -1.5968e-01,  1.8057e+00,  ..., -2.9921e-02,\n",
      "         -6.4967e-01,  1.9140e+00],\n",
      "        [-1.3293e-01,  0.0000e+00,  4.5027e-01,  ..., -0.0000e+00,\n",
      "         -9.5553e-01, -7.1198e-01],\n",
      "        ...,\n",
      "        [-1.3542e-01,  2.2646e+00,  2.4996e-01,  ..., -2.5869e-01,\n",
      "         -8.0597e-01, -4.8272e-01],\n",
      "        [-1.4655e-02,  2.2768e+00,  1.3188e-01,  ..., -1.1738e-01,\n",
      "         -1.1768e+00, -5.5774e-01],\n",
      "        [-2.0558e-01,  1.5668e+00, -4.9124e-01,  ...,  0.0000e+00,\n",
      "         -8.0793e-01, -3.2918e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.1120,  0.0829,  0.0872,  ...,  0.0143, -0.0135,  0.0493],\n",
      "        [ 0.0000, -0.0000,  1.6487,  ..., -0.1326, -0.2727,  3.1940],\n",
      "        [ 0.1246, -0.0000, -0.8632,  ...,  0.3243, -0.5767,  0.0326],\n",
      "        ...,\n",
      "        [ 0.7975, -0.0000, -1.1590,  ...,  0.3755, -0.0000,  0.1815],\n",
      "        [ 1.8562, -0.2996, -0.6201,  ...,  0.4469, -0.6958,  0.1759],\n",
      "        [ 4.2267, -0.3734, -0.3289,  ...,  2.7673, -0.8937,  2.6225]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-4.4528e-05,  3.3132e-03, -2.2923e-03,  ..., -0.0000e+00,\n",
      "          4.3563e-03, -5.7798e-04],\n",
      "        [-3.7676e-01, -2.8753e-01,  1.6949e+00,  ..., -1.9605e-02,\n",
      "         -2.2831e-01,  2.6222e+00],\n",
      "        [-1.8593e-01, -2.2118e-01,  1.0706e+00,  ..., -1.8290e-01,\n",
      "         -1.2339e-01, -5.6112e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  1.3030e+00,  3.9640e-01,  ..., -5.9754e-02,\n",
      "         -3.5625e-01, -2.8545e-01],\n",
      "        [-1.3734e-01,  1.1671e+00, -1.5073e-01,  ...,  3.9367e-02,\n",
      "         -2.1288e-01, -4.5004e-01],\n",
      "        [-1.3856e-01, -4.3334e-01,  1.2928e+00,  ..., -7.3381e-02,\n",
      "          6.9535e-01, -1.8309e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.8355e-03, -6.7270e-04, -2.2833e-03,  ..., -3.2968e-03,\n",
      "         -2.5533e-03, -7.1346e-03],\n",
      "        [-8.3980e-02, -7.0393e-02, -3.2368e-03,  ..., -2.2606e-01,\n",
      "         -2.0478e-01, -5.8528e-03],\n",
      "        [-2.0838e-01,  2.7414e+00,  0.0000e+00,  ..., -2.1186e-01,\n",
      "         -5.9037e-01, -0.0000e+00],\n",
      "        ...,\n",
      "        [-1.8047e-01,  2.0848e+00, -3.0403e-01,  ..., -4.3601e-01,\n",
      "          8.7538e-01, -6.5115e-02],\n",
      "        [ 6.2166e-01,  1.8510e+00, -1.0122e-01,  ..., -2.6476e-01,\n",
      "          0.0000e+00, -1.6712e-01],\n",
      "        [-1.0736e-01,  1.4593e+00, -2.1242e-01,  ..., -5.8724e-01,\n",
      "          2.7958e+00, -3.6598e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0000e+00,  3.5155e-03,  1.4994e-02,  ...,  3.0539e-02,\n",
      "          1.8096e-02,  6.3181e-03],\n",
      "        [-0.0000e+00, -0.0000e+00,  1.1180e-02,  ..., -2.1513e-01,\n",
      "         -1.9770e-01,  1.1445e-02],\n",
      "        [-8.7831e-02,  2.5819e+00, -9.8066e-03,  ..., -2.1442e-01,\n",
      "         -9.4460e-02,  1.0485e-01],\n",
      "        ...,\n",
      "        [-2.5369e-01,  1.3103e+00, -0.0000e+00,  ..., -2.0862e-01,\n",
      "          1.7133e+00,  5.4855e-01],\n",
      "        [ 5.2602e-01,  1.6403e+00, -1.0560e-01,  ..., -2.9008e-01,\n",
      "          1.8495e+00,  2.4441e-03],\n",
      "        [-3.1792e-01, -1.9111e-01, -3.8885e-01,  ..., -5.0981e-02,\n",
      "          3.6546e+00, -6.6955e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 6.0979e-03, -2.6588e-03, -8.0824e-05,  ..., -1.1031e-03,\n",
      "          5.4732e-03, -1.3673e-03],\n",
      "        [-7.7737e-02, -6.9886e-02, -1.0002e-03,  ..., -2.1590e-01,\n",
      "         -1.9367e-01, -1.3136e-04],\n",
      "        [-1.8542e-01,  1.1481e+00, -3.1554e-01,  ...,  1.3900e+00,\n",
      "         -4.6365e-02, -3.8577e-01],\n",
      "        ...,\n",
      "        [-2.6185e-01,  9.1245e-01, -3.2235e-01,  ..., -1.3291e-01,\n",
      "          2.2983e+00, -6.4127e-02],\n",
      "        [-2.1832e-01,  1.1244e+00, -3.0781e-01,  ..., -1.1132e-01,\n",
      "          1.3705e+00, -1.0742e-01],\n",
      "        [-2.9298e-01,  0.0000e+00, -3.4102e-01,  ..., -9.8051e-02,\n",
      "          3.5115e+00, -3.4485e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0736,  0.0000, -0.0095,  ...,  0.0267,  0.0725,  0.0056],\n",
      "        [-0.3415, -0.3158,  1.0919,  ..., -0.0000, -0.5039, -0.3454],\n",
      "        [-0.1328, -0.0000, -0.0000,  ..., -0.2225, -0.0000, -0.3923],\n",
      "        ...,\n",
      "        [-0.1164,  2.0532,  0.2836,  ..., -0.0000, -0.7330, -0.4522],\n",
      "        [-0.1633,  1.0561, -0.2429,  ..., -0.4547, -0.8292, -0.4287],\n",
      "        [-0.0000,  1.4117, -0.1466,  ..., -0.6826, -0.8726, -0.6580]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0248,  0.0841,  0.0076,  ..., -0.0070, -0.0180,  0.0000],\n",
      "        [-0.3114, -0.0873,  1.8970,  ..., -0.1836, -0.3742,  0.8344],\n",
      "        [-0.6042,  1.0612,  1.6116,  ..., -0.3741, -0.6078,  1.0073],\n",
      "        ...,\n",
      "        [ 0.8720,  0.7798, -0.9348,  ...,  1.2511, -0.8239,  1.2591],\n",
      "        [-0.1248, -0.1847, -1.1900,  ..., -0.0000, -0.0000,  0.9432],\n",
      "        [ 2.2997,  0.6530, -0.9347,  ...,  1.3482, -0.0000,  1.8253]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.9341e-03, -6.0255e-04,  2.1659e-03,  ..., -2.1435e-03,\n",
      "         -4.0155e-04, -1.2077e-03],\n",
      "        [-4.6574e-01, -2.6045e-01,  9.0670e-01,  ..., -0.0000e+00,\n",
      "         -3.8783e-01, -5.1781e-03],\n",
      "        [-4.2643e-01, -2.0491e-01, -2.3273e-02,  ...,  6.1886e-01,\n",
      "         -2.2382e-01, -0.0000e+00],\n",
      "        ...,\n",
      "        [-1.9006e-01,  6.8106e-01, -4.6650e-02,  ..., -7.8305e-02,\n",
      "         -3.3884e-01, -3.5076e-01],\n",
      "        [-1.9700e-01,  1.0708e+00, -2.3825e-03,  ..., -4.3425e-02,\n",
      "         -2.3863e-01, -0.0000e+00],\n",
      "        [-2.8121e-01,  4.2737e-01,  0.0000e+00,  ...,  8.4387e-02,\n",
      "         -1.9617e-01, -3.7325e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0043, -0.0015,  0.0043,  ...,  0.0000,  0.0038, -0.0000],\n",
      "        [-0.0871, -0.0718, -0.1413,  ..., -0.2208, -0.2013, -0.0024],\n",
      "        [-0.2923,  1.2449, -0.1602,  ..., -0.0000, -0.3906, -0.2834],\n",
      "        ...,\n",
      "        [-0.1433,  0.7851, -0.1130,  ..., -0.1780,  0.0000, -0.3108],\n",
      "        [-0.2990,  0.0000, -0.0652,  ..., -0.0000,  0.6114, -0.2672],\n",
      "        [-0.1143, -0.4565, -0.2281,  ..., -0.2687,  1.4909, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.5652e-03, -3.5685e-04,  1.0104e-02,  ...,  1.2016e-02,\n",
      "         -7.3465e-04, -2.2400e-03],\n",
      "        [-8.2474e-02, -7.0653e-02, -1.3984e-01,  ..., -2.1976e-01,\n",
      "         -2.0299e-01, -2.2400e-03],\n",
      "        [-3.6347e-01,  1.2228e+00, -1.1630e-01,  ..., -3.2070e-01,\n",
      "         -5.8215e-01,  1.9022e-02],\n",
      "        ...,\n",
      "        [-4.5938e-01,  2.3409e+00, -1.6680e-01,  ..., -2.7352e-01,\n",
      "          1.4671e+00, -0.0000e+00],\n",
      "        [-3.7993e-01,  1.3191e+00, -2.5048e-01,  ..., -3.2074e-01,\n",
      "          6.7776e-01, -1.8059e-03],\n",
      "        [-3.1753e-01, -1.1735e-02, -6.7500e-02,  ..., -1.9635e-01,\n",
      "          4.9998e-02, -1.1997e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-4.0205e-04, -5.5324e-04, -1.7677e-03,  ..., -2.2478e-03,\n",
      "          2.9255e-03, -1.8137e-04],\n",
      "        [-8.0303e-02, -6.8335e-02, -1.3904e-01,  ..., -2.1705e-01,\n",
      "         -1.9431e-01, -0.0000e+00],\n",
      "        [-2.3870e-01,  1.9217e+00, -2.2698e-01,  ..., -2.8832e-01,\n",
      "         -2.1589e-01,  6.8606e-02],\n",
      "        ...,\n",
      "        [-2.9142e-01,  0.0000e+00, -3.5726e-01,  ..., -1.4683e-01,\n",
      "          1.9271e+00, -8.6302e-02],\n",
      "        [-4.1685e-01,  7.1038e-01, -3.7314e-01,  ..., -3.9841e-02,\n",
      "          1.7398e+00, -1.2098e-01],\n",
      "        [-3.1242e-01, -1.7180e-01, -4.9108e-01,  ..., -1.7585e-01,\n",
      "          2.0881e+00, -2.2304e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0281, -0.0071,  0.0175,  ...,  0.0000,  0.0730, -0.0057],\n",
      "        [-0.3670, -0.0457,  0.8393,  ..., -0.4393, -0.5968,  2.3196],\n",
      "        [-0.2716, -0.0273, -0.3054,  ..., -0.5852, -0.4507, -0.1811],\n",
      "        ...,\n",
      "        [-0.3710,  1.3186,  0.0763,  ..., -0.8320, -0.7832, -0.3203],\n",
      "        [ 0.1512, -0.4168,  0.3257,  ..., -0.9470, -0.4753, -0.0771],\n",
      "        [-0.3082, -0.1478,  0.0000,  ..., -0.5621, -0.5685,  2.7667]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000e+00,  7.5564e-02,  4.1235e-02,  ..., -7.7897e-03,\n",
      "         -1.5312e-02,  1.3119e-02],\n",
      "        [-1.4779e-01, -2.3555e-01,  8.1334e-01,  ..., -2.3799e-01,\n",
      "         -5.6318e-01,  3.4695e+00],\n",
      "        [-3.4102e-01, -1.8359e-01, -6.6656e-01,  ..., -3.2436e-01,\n",
      "         -1.7034e-01,  0.0000e+00],\n",
      "        ...,\n",
      "        [-2.8075e-01,  1.0902e+00, -5.7576e-01,  ..., -3.2724e-03,\n",
      "         -5.5255e-01, -1.4864e-01],\n",
      "        [-2.8396e-01, -1.3613e-01,  6.2853e-01,  ..., -1.7222e-01,\n",
      "          6.6982e-01, -0.0000e+00],\n",
      "        [-1.9256e-01, -2.6804e-01,  1.1322e+00,  ..., -1.7289e-01,\n",
      "         -4.7415e-01,  3.1225e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.9607e-03,  2.4891e-02, -7.3160e-03,  ...,  1.5075e-02,\n",
      "         -2.5066e-04,  1.3184e-02],\n",
      "        [-2.8735e-01, -7.4713e-02,  1.2300e+00,  ..., -8.5585e-02,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-1.8783e-01, -0.0000e+00,  1.2097e+00,  ..., -6.1062e-02,\n",
      "          1.0544e+00, -4.6650e-01],\n",
      "        ...,\n",
      "        [-4.7551e-01,  1.4725e-01, -3.2713e-01,  ..., -4.4111e-02,\n",
      "         -2.6349e-01, -2.9649e-01],\n",
      "        [-2.0422e-01, -2.6379e-02,  7.6762e-01,  ...,  5.9297e-01,\n",
      "          0.0000e+00, -6.1707e-01],\n",
      "        [-4.6176e-01, -3.8162e-01,  1.1025e+00,  ..., -1.0962e-01,\n",
      "         -2.1032e-01,  2.6822e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0149,  0.0183, -0.0046,  ...,  0.0100,  0.0031, -0.0000],\n",
      "        [-0.0788, -0.0664, -0.0000,  ..., -0.2203, -0.2016, -0.1508],\n",
      "        [-0.1623,  2.8698,  1.4608,  ..., -0.0000, -0.9004, -0.2648],\n",
      "        ...,\n",
      "        [-0.2480,  2.2942,  0.1692,  ..., -0.3934,  0.7700, -0.1998],\n",
      "        [ 0.6028,  2.2232,  1.0506,  ..., -0.6644, -0.3242, -0.1746],\n",
      "        [-0.0788, -0.0664, -0.1469,  ..., -0.2203, -0.2016, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-3.1924e-03,  1.2259e-02,  1.3485e-02,  ...,  5.1694e-03,\n",
      "          6.8013e-03,  4.8814e-03],\n",
      "        [-8.5689e-02, -6.7915e-02, -1.3895e-01,  ..., -2.2153e-01,\n",
      "         -2.0063e-01, -1.4686e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -1.6613e-01,  ..., -0.0000e+00,\n",
      "         -5.4476e-02,  4.5447e-01],\n",
      "        ...,\n",
      "        [-3.4553e-01,  0.0000e+00, -5.3475e-01,  ..., -2.9381e-01,\n",
      "          3.2437e+00, -2.9208e-01],\n",
      "        [ 4.7466e-01,  0.0000e+00, -2.8572e-01,  ..., -3.1496e-01,\n",
      "         -4.5533e-03,  2.1545e-02],\n",
      "        [-0.0000e+00, -6.7915e-02, -1.3895e-01,  ..., -2.2153e-01,\n",
      "         -2.0063e-01, -1.4686e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.2520e-03, -9.7895e-04, -5.0528e-04,  ..., -2.7269e-03,\n",
      "          4.9319e-03, -1.9630e-03],\n",
      "        [-7.9239e-02, -0.0000e+00, -1.3772e-01,  ..., -2.1757e-01,\n",
      "         -0.0000e+00, -1.4474e-01],\n",
      "        [-2.2033e-01,  1.3118e+00, -2.3204e-01,  ...,  3.5300e-01,\n",
      "          2.4651e-03, -2.5867e-01],\n",
      "        ...,\n",
      "        [-3.5085e-01,  1.2431e+00, -3.6207e-01,  ..., -9.3751e-02,\n",
      "          2.9674e+00, -2.7413e-01],\n",
      "        [-1.2057e-01,  1.5146e+00, -2.5751e-01,  ...,  3.4637e-01,\n",
      "          5.4423e-01, -0.0000e+00],\n",
      "        [-7.9239e-02, -6.9416e-02, -1.3772e-01,  ..., -0.0000e+00,\n",
      "         -1.9389e-01, -1.4474e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.1668e-02, -1.4991e-03, -8.7063e-03,  ...,  7.4572e-02,\n",
      "         -0.0000e+00, -2.4903e-03],\n",
      "        [-2.7553e-02,  1.8476e+00, -3.4553e-01,  ..., -1.4837e-03,\n",
      "          2.1593e+00,  1.9151e-01],\n",
      "        [ 2.7855e+00, -9.0590e-01, -5.3399e-01,  ...,  1.2573e-02,\n",
      "         -0.0000e+00,  2.7200e+00],\n",
      "        ...,\n",
      "        [-4.8938e-02, -7.6672e-02,  5.2448e-02,  ...,  3.7395e+00,\n",
      "         -2.7088e-01,  2.1635e+00],\n",
      "        [ 3.4344e+00,  2.3011e+00,  2.8156e-01,  ...,  0.0000e+00,\n",
      "         -2.8366e-01,  3.3136e-01],\n",
      "        [ 0.0000e+00, -0.0000e+00, -3.0613e-01,  ..., -1.3230e-01,\n",
      "          2.5972e+00,  1.0736e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-5.7339e-03, -1.0270e-03,  7.2938e-03,  ...,  5.8400e-03,\n",
      "          1.6376e-02,  0.0000e+00],\n",
      "        [-2.7733e-01,  1.5136e+00, -7.8302e-02,  ..., -3.1511e-01,\n",
      "          1.9578e+00, -6.3991e-02],\n",
      "        [ 0.0000e+00,  1.8083e-01, -8.6882e-02,  ...,  2.5815e+00,\n",
      "          4.0652e-01,  1.0566e+00],\n",
      "        ...,\n",
      "        [-3.6514e-01, -1.1513e-01,  1.3792e+00,  ...,  2.8028e+00,\n",
      "         -3.9783e-01,  8.8323e-01],\n",
      "        [ 1.0316e+00,  1.2634e+00, -3.8916e-01,  ...,  2.0395e-03,\n",
      "         -3.8718e-01, -2.8520e-01],\n",
      "        [-0.0000e+00, -2.1468e-01, -2.1479e-01,  ...,  2.3309e-02,\n",
      "          1.5241e+00, -3.5925e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-8.8050e-04, -1.0079e-03,  1.8950e-03,  ..., -9.7891e-04,\n",
      "         -2.3077e-04, -1.2286e-03],\n",
      "        [-5.7011e-01,  1.9657e+00, -4.7401e-01,  ..., -3.3839e-01,\n",
      "          0.0000e+00, -1.3291e-02],\n",
      "        [-4.3584e-01, -3.3831e-01, -4.0706e-01,  ...,  0.0000e+00,\n",
      "          1.8123e+00, -1.2683e-01],\n",
      "        ...,\n",
      "        [-2.3598e-01, -2.1814e-01,  1.1745e+00,  ...,  4.1111e+00,\n",
      "         -5.2278e-01, -0.0000e+00],\n",
      "        [-0.0000e+00, -1.5629e-02, -1.3766e-01,  ..., -3.0916e-01,\n",
      "         -5.4387e-01, -6.3441e-01],\n",
      "        [ 0.0000e+00, -9.8639e-02, -8.5067e-02,  ...,  3.2937e-01,\n",
      "          1.6461e+00, -4.8764e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-3.5354e-03,  1.6101e-02,  1.6429e-02,  ..., -1.7152e-03,\n",
      "          4.2635e-03, -3.8243e-03],\n",
      "        [ 0.0000e+00,  1.0724e+00,  1.2583e-02,  ..., -2.2388e-01,\n",
      "         -2.4829e-01, -2.5338e-03],\n",
      "        [ 7.8945e-01,  3.9184e+00,  2.1877e+00,  ..., -4.1787e-01,\n",
      "          2.4036e+00,  5.5336e-01],\n",
      "        ...,\n",
      "        [ 1.6977e+00,  1.9307e+00, -3.3543e-01,  ..., -4.5774e-01,\n",
      "         -3.1674e-01, -3.8177e-01],\n",
      "        [-2.9534e-02, -3.0117e-01, -3.2571e-01,  ..., -1.2529e-01,\n",
      "         -3.1337e-01,  6.1316e-01],\n",
      "        [-2.4617e-02, -5.5670e-02,  1.2303e+00,  ..., -2.6472e-01,\n",
      "         -2.3645e-01, -1.9368e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.2467e-03,  2.2872e-02,  3.2743e-03,  ...,  2.9816e-02,\n",
      "          1.9838e-02,  2.2483e-03],\n",
      "        [ 1.1239e-01,  1.0792e+00, -1.4301e-04,  ..., -2.1471e-01,\n",
      "         -2.4440e-01,  0.0000e+00],\n",
      "        [-1.0416e-01,  1.7638e+00,  3.9617e-01,  ...,  1.1232e+00,\n",
      "          1.4791e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 1.6290e-01,  2.8597e+00, -7.9341e-02,  ..., -3.2036e-01,\n",
      "         -7.7814e-02, -2.7130e-01],\n",
      "        [-1.2662e-01,  5.3868e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -1.9450e-01, -9.4518e-02],\n",
      "        [-2.2327e-02, -5.3977e-02,  1.2171e+00,  ..., -0.0000e+00,\n",
      "         -2.3256e-01, -1.8930e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000e+00, -1.2393e-03,  3.2780e-03,  ..., -1.3524e-03,\n",
      "          6.8957e-03, -1.3109e-03],\n",
      "        [ 1.1630e-01,  1.0512e+00, -1.3700e-04,  ..., -2.1554e-01,\n",
      "         -2.3875e-01, -6.7012e-05],\n",
      "        [-1.4017e-01,  2.4822e+00, -2.4963e-01,  ..., -3.2664e-01,\n",
      "         -3.7115e-03,  7.2189e-01],\n",
      "        ...,\n",
      "        [-1.3341e-01,  0.0000e+00, -3.2215e-01,  ..., -3.4229e-01,\n",
      "         -2.4528e-01, -2.0356e-01],\n",
      "        [-1.7693e-01, -1.7316e-02, -5.4324e-01,  ..., -4.0875e-01,\n",
      "         -1.4645e-01, -0.0000e+00],\n",
      "        [-2.0584e-02, -0.0000e+00,  1.2171e+00,  ..., -2.5492e-01,\n",
      "         -2.2733e-01, -1.8436e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.1379e-02,  2.3431e-02,  1.7399e-02,  ...,  2.1117e-02,\n",
      "          1.0822e-02, -1.5350e-03],\n",
      "        [-1.5927e-01,  7.3596e-02,  2.3191e+00,  ...,  7.1125e-01,\n",
      "         -2.3237e-01,  2.3595e+00],\n",
      "        [ 8.1715e-01, -3.8111e-02,  9.1270e-02,  ...,  0.0000e+00,\n",
      "         -5.1725e-01, -1.1846e-01],\n",
      "        ...,\n",
      "        [ 1.4190e+00,  1.9327e+00, -1.7453e-02,  ..., -2.8391e-01,\n",
      "          4.0878e-01,  6.3258e-01],\n",
      "        [ 1.4190e+00,  1.9327e+00,  9.9650e-01,  ..., -2.8391e-01,\n",
      "          0.0000e+00,  6.3258e-01],\n",
      "        [ 1.3449e+00,  9.8913e-01,  2.3749e+00,  ..., -4.2842e-01,\n",
      "         -7.3312e-01,  0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000,  0.0465,  0.0311,  ...,  0.0156, -0.0062,  0.0131],\n",
      "        [-0.5200,  0.5753,  2.5019,  ..., -0.2142, -0.3248,  2.9042],\n",
      "        [-0.3379,  0.6379,  0.8300,  ..., -0.6971, -0.2616, -0.1070],\n",
      "        ...,\n",
      "        [-0.3981,  0.5738, -0.0000,  ..., -0.2699,  2.4855,  0.0000],\n",
      "        [-0.3981,  0.5738, -0.1960,  ..., -0.0000,  2.4855,  0.0988],\n",
      "        [-0.4166, -0.0924,  0.5943,  ..., -0.4145, -0.0000,  3.7694]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.8301e-03,  6.7268e-03, -2.5786e-03,  ..., -5.9112e-04,\n",
      "         -1.3605e-03, -1.2197e-03],\n",
      "        [-3.0782e-01, -2.4164e-02,  9.5092e-01,  ..., -9.1425e-02,\n",
      "         -3.5380e-01,  2.6775e+00],\n",
      "        [-1.7019e-01, -1.5032e-01,  4.7896e-01,  ...,  4.1042e-01,\n",
      "         -0.0000e+00, -4.3137e-01],\n",
      "        ...,\n",
      "        [-8.2296e-01, -1.1966e-01, -7.2100e-01,  ...,  2.5255e+00,\n",
      "          1.9735e+00, -6.1868e-01],\n",
      "        [-8.2296e-01, -1.1966e-01, -4.6817e-01,  ...,  2.5255e+00,\n",
      "          1.9735e+00, -6.1868e-01],\n",
      "        [-8.4053e-01, -3.4341e-01, -1.4133e-01,  ...,  0.0000e+00,\n",
      "         -3.2432e-01,  1.0613e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0000e+00, -3.3333e-03,  4.6228e-03,  ..., -3.5216e-03,\n",
      "         -3.8747e-03, -1.0498e-03],\n",
      "        [-7.4666e-02, -7.6558e-02, -1.4112e-01,  ..., -2.2610e-01,\n",
      "         -2.0630e-01, -1.4911e-01],\n",
      "        [-3.2887e-01,  2.4916e+00, -9.3480e-02,  ..., -3.3298e-01,\n",
      "         -0.0000e+00, -1.0263e-01],\n",
      "        ...,\n",
      "        [ 8.5833e-01, -1.7361e-01,  1.3183e+00,  ..., -2.2155e-01,\n",
      "         -2.8853e-01, -0.0000e+00],\n",
      "        [ 3.6305e+00,  8.9865e-01,  2.4242e+00,  ..., -2.3595e-01,\n",
      "          1.1868e-01,  1.7304e+00],\n",
      "        [ 2.6007e-02, -7.6558e-02, -1.4112e-01,  ..., -2.7476e-03,\n",
      "         -0.0000e+00, -1.4911e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 5.3862e-02, -1.0670e-05, -7.1950e-03,  ...,  8.1377e-03,\n",
      "          7.1095e-02,  6.8221e-02],\n",
      "        [-0.0000e+00, -7.3236e-02, -0.0000e+00,  ..., -2.2055e-01,\n",
      "         -1.8465e-01, -1.3100e-01],\n",
      "        [-1.7994e-01,  1.2237e+00, -1.2168e-01,  ..., -4.7629e-01,\n",
      "         -3.5301e-01,  7.9874e-01],\n",
      "        ...,\n",
      "        [ 8.8302e-01, -1.7029e-01,  1.2849e+00,  ..., -2.1600e-01,\n",
      "         -2.6688e-01, -1.6023e-01],\n",
      "        [ 1.4664e+00, -1.8139e-01,  2.6008e+00,  ..., -2.3043e-01,\n",
      "          1.4203e+00,  3.0250e+00],\n",
      "        [ 5.0696e-02, -7.3236e-02, -1.4947e-01,  ...,  1.1234e-02,\n",
      "         -1.8465e-01, -1.3100e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 9.6408e-03, -0.0000e+00, -1.3333e-03,  ..., -3.2888e-03,\n",
      "          7.1859e-03, -1.0021e-03],\n",
      "        [-7.6687e-02, -7.3585e-02, -1.3849e-01,  ..., -2.1786e-01,\n",
      "         -1.9341e-01, -1.4373e-01],\n",
      "        [-2.6501e-01,  2.2580e+00, -4.9027e-02,  ..., -2.9711e-01,\n",
      "         -1.8707e-01, -3.4658e-02],\n",
      "        ...,\n",
      "        [ 8.3879e-01, -1.6714e-01,  1.3082e+00,  ..., -2.1348e-01,\n",
      "         -2.7268e-01, -1.7191e-01],\n",
      "        [ 8.0553e-01, -3.3131e-01,  2.3212e+00,  ..., -2.5318e-01,\n",
      "         -3.0535e-01,  0.0000e+00],\n",
      "        [ 6.4747e-03, -7.3585e-02, -1.3849e-01,  ..., -2.5427e-03,\n",
      "         -1.9341e-01, -1.4373e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 4.3729e-02, -1.1711e-03, -0.0000e+00,  ...,  1.2971e-02,\n",
      "          2.1006e-02,  8.8732e-03],\n",
      "        [ 7.7001e-03, -3.7415e-01,  1.5203e+00,  ...,  6.0006e-01,\n",
      "         -0.0000e+00,  2.4275e+00],\n",
      "        [-2.6682e-01, -2.8117e-01, -2.6099e-01,  ...,  3.7517e-01,\n",
      "         -5.1999e-01, -2.5513e-01],\n",
      "        ...,\n",
      "        [-3.7283e-01,  1.9285e+00, -4.4290e-01,  ...,  9.2596e-01,\n",
      "         -6.9925e-01, -2.3778e-01],\n",
      "        [-3.9132e-01,  1.0400e+00, -5.9868e-01,  ..., -6.6958e-01,\n",
      "         -1.1316e+00, -6.0431e-01],\n",
      "        [ 4.1637e-04, -3.2026e-01, -5.8685e-01,  ..., -7.4147e-01,\n",
      "         -6.3407e-01, -5.0304e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0066,  0.0611,  0.0122,  ..., -0.0056, -0.0087,  0.0233],\n",
      "        [-0.3744, -0.0000,  2.7615,  ..., -0.2217, -0.2755,  3.3288],\n",
      "        [-0.0000,  0.0000,  1.3463,  ..., -0.3061, -0.0000, -0.2674],\n",
      "        ...,\n",
      "        [-0.1742,  0.9879, -0.3927,  ...,  1.8927, -0.4906,  0.0074],\n",
      "        [ 0.3477, -0.3510, -0.4701,  ..., -0.3252, -0.8022, -0.2438],\n",
      "        [ 0.8718, -0.2996, -0.2868,  ..., -0.1439, -0.4559, -0.2428]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 3.9274e-03,  2.3550e-03, -2.0952e-03,  ..., -1.4948e-03,\n",
      "          1.2743e-04, -0.0000e+00],\n",
      "        [-2.4842e-01, -2.9622e-01,  1.5002e+00,  ...,  1.1640e-01,\n",
      "         -1.6910e-01,  2.4696e+00],\n",
      "        [-5.2499e-01, -1.9843e-01,  2.2167e-01,  ..., -0.0000e+00,\n",
      "          1.0980e-01, -4.4847e-01],\n",
      "        ...,\n",
      "        [-3.4292e-01,  2.2847e+00, -2.1077e-01,  ...,  0.0000e+00,\n",
      "         -2.4478e-01, -4.1760e-02],\n",
      "        [-0.0000e+00,  1.0681e+00, -2.9707e-01,  ...,  5.7416e-02,\n",
      "         -2.5028e-01, -6.3553e-02],\n",
      "        [-1.2113e-01, -0.0000e+00, -2.8537e-01,  ..., -9.5337e-02,\n",
      "          9.2966e-01, -1.4375e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.2367e-03,  0.0000e+00, -9.4992e-04,  ...,  2.1056e-02,\n",
      "         -4.9448e-03, -5.7609e-03],\n",
      "        [-8.2699e-02, -7.3135e-02, -1.9161e-03,  ..., -2.1726e-01,\n",
      "         -2.0747e-01, -1.5380e-01],\n",
      "        [-1.3212e-01,  7.8931e-01, -1.3184e-01,  ..., -2.2196e-01,\n",
      "         -4.6738e-01, -1.5136e-01],\n",
      "        ...,\n",
      "        [-2.8988e-01,  2.5399e+00, -1.8222e-01,  ..., -5.4737e-01,\n",
      "          1.0313e+00, -5.6241e-02],\n",
      "        [-8.8751e-02,  1.1153e+00, -0.0000e+00,  ..., -3.2910e-01,\n",
      "          1.2117e+00, -4.7734e-02],\n",
      "        [-5.8160e-01,  4.1900e-01,  9.6385e-01,  ..., -1.7743e-01,\n",
      "          3.1741e+00, -4.2908e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.4755e-02,  2.5206e-02,  1.5265e-02,  ...,  0.0000e+00,\n",
      "         -7.0194e-04,  1.0683e-02],\n",
      "        [-7.7774e-02, -6.7690e-02,  1.1401e-02,  ..., -2.2245e-01,\n",
      "         -2.0323e-01, -1.4537e-01],\n",
      "        [-2.9113e-01,  9.0207e-01, -3.3029e-01,  ..., -3.5033e-01,\n",
      "         -3.3680e-01,  4.4680e-01],\n",
      "        ...,\n",
      "        [-2.7826e-01,  1.2305e+00, -4.2086e-01,  ..., -1.8423e-01,\n",
      "          2.1865e+00, -0.0000e+00],\n",
      "        [-1.8828e-01,  9.5566e-01, -1.7345e-01,  ..., -1.2767e-01,\n",
      "          1.7982e+00, -2.2403e-01],\n",
      "        [-3.9295e-01, -1.3481e-01, -2.9765e-01,  ...,  3.8393e-01,\n",
      "          4.3327e+00, -1.1713e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-5.0365e-04, -6.8352e-04,  1.6098e-03,  ...,  1.6367e-03,\n",
      "          3.1783e-03, -3.8955e-03],\n",
      "        [-7.9025e-02, -7.2004e-02, -0.0000e+00,  ..., -2.1409e-01,\n",
      "         -1.9445e-01, -1.4659e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -3.3627e-01,  ..., -3.0562e-01,\n",
      "         -0.0000e+00, -2.8268e-02],\n",
      "        ...,\n",
      "        [-2.9154e-01,  0.0000e+00, -3.4080e-01,  ..., -1.5888e-01,\n",
      "          2.2347e+00, -0.0000e+00],\n",
      "        [-2.4470e-01,  8.6574e-01, -4.2709e-01,  ..., -1.4071e-01,\n",
      "          0.0000e+00, -1.0340e-01],\n",
      "        [-0.0000e+00,  1.6418e-01, -2.8037e-01,  ...,  3.0597e-01,\n",
      "          4.2627e+00, -4.3683e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0805,  0.0445,  0.0046,  ...,  0.0716,  0.0315, -0.0025],\n",
      "        [ 0.0701, -0.3266,  1.9117,  ...,  0.4130, -0.1111,  0.0000],\n",
      "        [ 0.7879, -0.0000, -0.0298,  ..., -0.1522, -0.6481, -0.0819],\n",
      "        ...,\n",
      "        [-0.2237, -0.1206, -0.5551,  ..., -0.5442, -0.7055, -0.3879],\n",
      "        [-0.1482,  0.9905, -0.6141,  ..., -0.0000, -1.0009, -0.6452],\n",
      "        [-0.4598,  0.8257, -0.4855,  ..., -0.5229, -0.0000, -0.8132]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0072,  0.0275,  0.0536,  ..., -0.0045, -0.0086,  0.0291],\n",
      "        [-0.2936, -0.1546,  2.9099,  ..., -0.2127, -0.3440,  2.4914],\n",
      "        [-0.5639, -0.0409,  0.4370,  ..., -0.0000, -0.2724, -0.4769],\n",
      "        ...,\n",
      "        [-0.2801, -0.2704, -0.4014,  ..., -0.0000, -0.3830,  0.8170],\n",
      "        [-0.4140, -0.0627, -0.7100,  ..., -0.2641, -0.6480,  0.8610],\n",
      "        [-0.1818, -0.2779, -0.6587,  ..., -0.4049, -0.7020,  0.9895]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-1.3227e-03,  7.1845e-03, -5.9482e-03,  ...,  1.0882e-02,\n",
      "         -1.1279e-03,  5.3672e-03],\n",
      "        [-2.3150e-01, -1.4970e-01,  1.3173e+00,  ...,  1.1742e-01,\n",
      "         -4.0869e-01,  2.5171e+00],\n",
      "        [-0.0000e+00, -2.1465e-01, -4.1848e-01,  ..., -2.0702e-01,\n",
      "         -2.8329e-01, -3.6453e-01],\n",
      "        ...,\n",
      "        [-4.8110e-01, -4.0568e-01, -1.0078e-01,  ..., -0.0000e+00,\n",
      "         -1.5061e-01, -8.0222e-02],\n",
      "        [-4.0127e-01, -1.3022e-01,  4.5446e-01,  ..., -1.6351e-01,\n",
      "         -1.8463e-01, -4.4180e-01],\n",
      "        [-5.8140e-01,  4.9479e-01, -4.6493e-02,  ...,  7.5416e-01,\n",
      "         -6.8286e-02, -3.5470e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.7998e-02,  2.6146e-02, -1.2262e-03,  ...,  1.5692e-02,\n",
      "         -3.8543e-03, -5.6104e-04],\n",
      "        [-7.6823e-02, -6.8607e-02, -1.4343e-01,  ..., -2.1853e-01,\n",
      "         -2.0644e-01, -1.4858e-01],\n",
      "        [ 1.7219e-01, -4.7931e-02, -0.0000e+00,  ..., -4.6437e-01,\n",
      "         -2.0376e-01,  5.8928e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  1.5019e+00, -4.8530e-02,  ..., -0.0000e+00,\n",
      "         -2.8258e-01, -1.5775e-01],\n",
      "        [-2.7671e-01,  1.8740e+00, -1.0783e-01,  ..., -2.0797e-01,\n",
      "          0.0000e+00, -7.1212e-02],\n",
      "        [-2.5515e-01,  1.0544e+00, -9.2164e-02,  ..., -2.6987e-01,\n",
      "          1.4396e+00,  4.5196e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0073,  0.0300, -0.0000,  ...,  0.0548,  0.0000,  0.0048],\n",
      "        [-0.0795, -0.0676, -0.1427,  ..., -0.2088, -0.1965, -0.1468],\n",
      "        [-0.2215,  1.9579, -0.2107,  ..., -0.0123, -0.0365, -0.0000],\n",
      "        ...,\n",
      "        [ 0.4692,  1.2741, -0.2259,  ..., -0.2917,  0.9190,  0.3374],\n",
      "        [-0.1293,  1.3463, -0.2130,  ..., -0.2486,  1.4981,  0.1026],\n",
      "        [-0.0845,  0.0000, -0.3300,  ..., -0.2313,  0.5675,  0.6344]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 8.5580e-03, -1.2285e-03, -6.9023e-05,  ..., -2.4356e-03,\n",
      "         -2.9878e-04, -1.5991e-03],\n",
      "        [-0.0000e+00, -7.3652e-02, -1.3712e-01,  ..., -2.1683e-01,\n",
      "         -0.0000e+00, -1.4426e-01],\n",
      "        [-2.5795e-01,  1.7555e+00, -2.7893e-01,  ..., -3.9777e-01,\n",
      "         -2.0559e-01,  2.6891e-01],\n",
      "        ...,\n",
      "        [-5.8900e-02,  1.1997e+00, -2.3227e-01,  ..., -7.7720e-02,\n",
      "          5.4715e-01, -1.0317e-01],\n",
      "        [-0.0000e+00,  1.1954e+00, -2.4987e-01,  ..., -8.5268e-02,\n",
      "          1.1639e+00, -1.8528e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -3.8745e-01,  ...,  4.6077e-02,\n",
      "          1.8683e+00, -5.4061e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0394,  0.0000,  0.0275,  ...,  0.0088,  0.0218, -0.0032],\n",
      "        [-0.0741, -0.0078, -0.7066,  ..., -0.2870, -0.5859, -0.4826],\n",
      "        [-0.1625,  0.2902, -0.0000,  ..., -1.0826, -1.2789, -0.3805],\n",
      "        ...,\n",
      "        [-0.0978, -0.0000, -0.0867,  ..., -0.4294, -0.9523, -0.4605],\n",
      "        [-0.0809,  0.0000, -0.0000,  ..., -0.4051, -0.9498, -0.4527],\n",
      "        [-0.1495,  0.7329, -0.0000,  ..., -0.0000, -1.3262, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0146,  0.0471,  0.0348,  ..., -0.0000, -0.0000,  0.0372],\n",
      "        [-0.3851, -0.4757, -0.5580,  ..., -0.3559, -0.5176, -0.0542],\n",
      "        [-0.2524, -0.6546, -0.8697,  ..., -0.0681, -0.4380,  1.3772],\n",
      "        ...,\n",
      "        [-0.2612, -0.5589, -0.8028,  ..., -0.4243, -0.4780, -0.1065],\n",
      "        [-0.4322,  0.6393, -0.9484,  ..., -0.0000, -0.6803,  0.0000],\n",
      "        [-0.2022, -0.1353, -0.6564,  ..., -0.0000, -0.7860,  1.0224]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-2.0792e-04,  0.0000e+00, -3.3934e-03,  ..., -1.0300e-03,\n",
      "          2.3203e-03, -1.0061e-03],\n",
      "        [-3.4053e-01, -4.5933e-01, -2.2079e-01,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -2.4155e-01],\n",
      "        [-2.4228e-01, -2.8585e-01,  9.3333e-01,  ...,  1.6278e+00,\n",
      "         -3.5150e-02, -5.0972e-01],\n",
      "        ...,\n",
      "        [-1.6637e-01, -8.0843e-02, -1.3211e-01,  ..., -6.4961e-02,\n",
      "         -2.6356e-01, -5.0421e-01],\n",
      "        [-8.2100e-02,  1.1997e+00,  0.0000e+00,  ..., -2.4985e-02,\n",
      "         -1.0237e-01, -3.2221e-01],\n",
      "        [-6.0769e-01,  3.6096e-01, -3.2263e-01,  ...,  3.5821e-01,\n",
      "         -2.3602e-01, -1.7931e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.0600e-03, -5.4481e-03,  2.7321e-02,  ..., -9.7284e-04,\n",
      "          6.2197e-03,  9.1918e-03],\n",
      "        [ 2.5783e+00, -5.4481e-03,  2.9371e+00,  ..., -3.1000e-01,\n",
      "          1.2147e-03,  8.6549e-01],\n",
      "        [ 6.1050e-01,  1.7736e+00,  1.2568e+00,  ..., -7.1694e-01,\n",
      "         -3.8376e-01,  2.1139e-02],\n",
      "        ...,\n",
      "        [-4.8801e-01,  1.7288e+00, -1.7917e-01,  ..., -3.8909e-01,\n",
      "          3.5528e-01, -9.2653e-02],\n",
      "        [-2.3539e-01,  1.4656e+00,  1.3141e+00,  ..., -0.0000e+00,\n",
      "         -8.3519e-02, -1.8466e-01],\n",
      "        [-0.0000e+00,  3.7610e-01, -8.3449e-02,  ..., -2.7545e-01,\n",
      "          1.2344e+00,  3.8721e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.5905e-02,  5.1141e-03,  1.2962e-02,  ..., -1.2803e-03,\n",
      "          1.7507e-02, -0.0000e+00],\n",
      "        [ 2.6022e+00,  0.0000e+00,  2.9227e+00,  ..., -3.1030e-01,\n",
      "          1.2503e-02,  0.0000e+00],\n",
      "        [ 6.0153e-01,  2.7327e+00, -1.0685e-01,  ..., -1.5877e-01,\n",
      "          8.6377e-01,  1.3409e-01],\n",
      "        ...,\n",
      "        [-2.9313e-01,  2.5715e+00, -3.0440e-01,  ..., -1.6858e-01,\n",
      "          2.0493e+00,  3.0915e-01],\n",
      "        [-1.7439e-01,  2.2461e+00,  6.2118e-01,  ..., -1.6222e-01,\n",
      "          1.9266e+00,  1.3507e-01],\n",
      "        [ 8.4655e-01,  1.1049e+00, -8.9561e-02,  ..., -9.8823e-02,\n",
      "          9.1047e-01, -6.8231e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-6.4041e-04,  2.0459e-03, -0.0000e+00,  ..., -1.5259e-03,\n",
      "          1.2701e-03, -0.0000e+00],\n",
      "        [ 2.5736e+00,  0.0000e+00,  2.9076e+00,  ..., -2.9935e-01,\n",
      "         -8.9975e-04,  0.0000e+00],\n",
      "        [-1.9614e-01,  1.0926e+00, -0.0000e+00,  ..., -8.0981e-02,\n",
      "         -5.1798e-02, -1.6167e-01],\n",
      "        ...,\n",
      "        [-4.0204e-01,  7.6856e-01, -0.0000e+00,  ...,  1.1409e-01,\n",
      "          1.8553e+00, -1.1228e-01],\n",
      "        [-4.4527e-01,  9.9417e-01, -1.1384e-03,  ...,  6.0007e-01,\n",
      "          1.6941e+00, -0.0000e+00],\n",
      "        [-4.5184e-02,  9.0619e-01, -2.9276e-01,  ..., -9.3703e-02,\n",
      "          2.1979e+00, -0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.9575e-02,  3.9399e-02, -2.2751e-03,  ...,  1.9198e-02,\n",
      "         -1.0860e-02, -4.4076e-03],\n",
      "        [-2.9333e-01, -2.5919e-01, -7.7906e-02,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  1.9494e+00],\n",
      "        [ 5.2182e-01,  8.2685e-01, -4.2820e-01,  ...,  6.1197e-01,\n",
      "         -1.6823e-01, -2.6922e-01],\n",
      "        ...,\n",
      "        [-2.4474e-01,  2.9943e+00, -0.0000e+00,  ..., -1.1585e-01,\n",
      "         -6.1554e-01,  1.2590e+00],\n",
      "        [ 6.9246e-01,  9.6460e-01, -0.0000e+00,  ..., -2.0968e-01,\n",
      "         -3.9482e-01, -1.9083e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -5.1552e-02,  ..., -1.1529e-01,\n",
      "         -0.0000e+00, -5.0608e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-6.0512e-03,  1.4017e-03, -3.1921e-03,  ...,  1.6122e-02,\n",
      "          0.0000e+00,  3.2602e-02],\n",
      "        [-2.9597e-01, -2.2532e-01, -4.6079e-02,  ..., -3.7086e-01,\n",
      "         -4.2781e-01,  3.5358e+00],\n",
      "        [-5.1020e-01, -9.5781e-02, -3.1445e-01,  ..., -8.9767e-02,\n",
      "          1.5247e+00,  1.4386e+00],\n",
      "        ...,\n",
      "        [-2.7509e-01,  1.9456e+00, -4.8488e-01,  ..., -1.9138e-01,\n",
      "         -2.3619e-02,  4.1332e+00],\n",
      "        [-1.3041e-01,  0.0000e+00, -7.8948e-01,  ...,  6.7493e-01,\n",
      "         -1.7164e-01,  1.0465e+00],\n",
      "        [-2.2451e-01,  1.3966e+00, -5.8938e-01,  ..., -1.4610e-01,\n",
      "         -2.3472e-01,  5.3548e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-3.8599e-03,  9.6249e-03, -3.1845e-03,  ..., -1.0958e-03,\n",
      "         -1.7032e-03, -7.9175e-05],\n",
      "        [-2.7505e-01, -4.6196e-01,  1.4782e-02,  ..., -2.0254e-01,\n",
      "         -3.0067e-01,  0.0000e+00],\n",
      "        [-3.3010e-01, -5.3361e-01, -2.4641e-01,  ..., -2.3436e-02,\n",
      "          5.6697e-01, -4.8373e-01],\n",
      "        ...,\n",
      "        [ 5.1155e-01,  0.0000e+00, -2.2224e-01,  ..., -1.4551e-01,\n",
      "         -2.1962e-01,  1.9115e+00],\n",
      "        [ 5.1646e-02, -6.8081e-02, -1.8121e-01,  ..., -5.4160e-02,\n",
      "         -3.5697e-01, -1.6103e-01],\n",
      "        [-1.7174e-01,  6.7761e-01, -1.7530e-01,  ...,  1.1265e-02,\n",
      "         -1.8315e-01, -2.9585e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.3962e-02,  1.8755e-02,  9.3319e-04,  ...,  2.9340e-03,\n",
      "          2.1836e-03, -2.7269e-03],\n",
      "        [-0.0000e+00, -7.2852e-02, -1.4184e-01,  ...,  6.0174e-03,\n",
      "         -2.0203e-01, -1.5062e-01],\n",
      "        [ 1.4275e+00,  3.1980e+00,  1.3220e-02,  ..., -3.7682e-01,\n",
      "         -0.0000e+00, -1.5699e-01],\n",
      "        ...,\n",
      "        [-3.2071e-01,  2.4580e+00,  1.5069e-01,  ..., -3.2468e-01,\n",
      "         -1.8613e-01, -0.0000e+00],\n",
      "        [-4.2670e-02,  1.5198e+00,  3.3191e-01,  ..., -1.3292e-01,\n",
      "          0.0000e+00,  5.9300e-02],\n",
      "        [-4.2268e-01,  1.3378e+00, -0.0000e+00,  ..., -2.0415e-01,\n",
      "          7.6206e-01,  2.4149e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0000,  0.0085, -0.0093,  ...,  0.0000,  0.0654,  0.0075],\n",
      "        [-0.0000, -0.0754, -0.1514,  ...,  0.0065, -0.1862, -0.1460],\n",
      "        [-0.2120,  0.0000, -0.0597,  ..., -0.0043, -0.2136, -0.2814],\n",
      "        ...,\n",
      "        [-0.3989,  1.1616, -0.3084,  ..., -0.2453,  0.7506,  0.0488],\n",
      "        [-0.2566,  0.5210, -0.0000,  ...,  0.6033,  1.5565, -0.0000],\n",
      "        [-0.3683,  0.2854, -0.3230,  ..., -0.1248,  2.2441,  0.0824]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.9417e-03, -6.8385e-04, -5.1305e-04,  ..., -1.6391e-03,\n",
      "          2.6756e-03, -1.3896e-03],\n",
      "        [-7.7407e-02, -7.5410e-02, -1.3743e-01,  ..., -8.9623e-04,\n",
      "         -1.9458e-01, -0.0000e+00],\n",
      "        [-0.0000e+00,  2.0924e+00, -5.4394e-02,  ..., -2.1485e-01,\n",
      "         -2.8754e-02, -2.8154e-01],\n",
      "        ...,\n",
      "        [-4.6194e-01,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          2.8269e-01, -0.0000e+00],\n",
      "        [-4.3043e-01,  1.3647e+00, -1.4863e-01,  ..., -1.8272e-01,\n",
      "          1.0603e+00,  0.0000e+00],\n",
      "        [-3.6646e-01,  0.0000e+00, -3.5511e-01,  ..., -0.0000e+00,\n",
      "          2.2794e+00, -1.5853e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 6.8574e-02, -1.5439e-03,  5.7194e-02,  ...,  7.1810e-02,\n",
      "          1.4287e-02, -1.1000e-02],\n",
      "        [-1.4357e-01, -2.1205e-01,  6.6818e-01,  ..., -3.1560e-01,\n",
      "         -3.7844e-01,  3.3496e+00],\n",
      "        [ 2.5244e+00,  1.9682e+00, -5.4668e-01,  ...,  1.6632e+00,\n",
      "         -6.8685e-01,  1.2865e+00],\n",
      "        ...,\n",
      "        [-2.0965e-01,  1.4073e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -8.6674e-01,  1.9507e+00],\n",
      "        [ 2.5570e+00, -6.4140e-01, -8.3316e-01,  ..., -1.0734e+00,\n",
      "         -8.6105e-01,  7.0225e-02],\n",
      "        [-2.8971e-01,  9.8352e-01, -8.6189e-01,  ..., -9.1675e-01,\n",
      "         -2.6830e-01, -2.4529e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.1074,  0.1187,  0.0896,  ...,  0.0618, -0.0177,  0.0442],\n",
      "        [-0.1399, -0.2644,  0.5047,  ..., -0.2118, -0.2272,  2.5178],\n",
      "        [ 1.1158, -0.2256, -0.0000,  ..., -0.1122, -1.0794, -0.2837],\n",
      "        ...,\n",
      "        [ 2.7884, -0.7017, -0.3336,  ..., -0.5920, -1.6367,  0.1853],\n",
      "        [ 4.3765, -0.1087, -0.7495,  ..., -0.5909, -0.7376,  2.7923],\n",
      "        [ 3.3768, -0.5135, -0.1024,  ..., -0.4797, -0.8431, -0.3460]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-3.2139e-03,  1.1726e-02, -4.2029e-03,  ..., -3.5335e-03,\n",
      "         -3.1139e-04, -2.5356e-04],\n",
      "        [-7.9515e-02, -5.5224e-01,  0.0000e+00,  ..., -1.8367e-01,\n",
      "         -4.6128e-01,  1.7208e+00],\n",
      "        [-6.3908e-01, -2.0761e-01, -4.8588e-01,  ...,  1.0172e+00,\n",
      "         -3.4565e-01, -4.9868e-01],\n",
      "        ...,\n",
      "        [-7.4505e-01, -4.7383e-01, -2.8413e-01,  ..., -3.9987e-01,\n",
      "          1.5361e-02,  1.9325e+00],\n",
      "        [-3.8296e-01, -8.5439e-01, -3.9857e-02,  ...,  1.8692e+00,\n",
      "          1.9552e+00, -5.8999e-02],\n",
      "        [-7.8610e-01, -4.3373e-01, -3.8869e-01,  ..., -3.8069e-01,\n",
      "          3.2672e+00,  3.5032e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.8924e-02,  4.6295e-03,  8.7985e-03,  ..., -4.2300e-03,\n",
      "          2.5087e-03, -1.5601e-03],\n",
      "        [ 1.5680e-02, -7.7230e-02, -1.3984e-01,  ..., -2.2708e-01,\n",
      "         -2.0197e-01, -1.4947e-01],\n",
      "        [-2.2759e-02,  3.2700e-01, -3.4671e-02,  ..., -2.9327e-01,\n",
      "         -8.6917e-01,  1.5622e+00],\n",
      "        ...,\n",
      "        [-2.1625e-01,  2.5208e+00,  9.6178e-01,  ..., -1.0834e-01,\n",
      "          4.2244e+00, -1.4654e-01],\n",
      "        [ 4.3496e-02,  1.1906e+00,  1.4223e+00,  ..., -1.0175e-02,\n",
      "          5.4225e+00,  5.6473e-01],\n",
      "        [-2.9142e-01, -0.0000e+00, -7.8996e-03,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  2.7998e-02]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 3.0926e-03,  3.5115e-02,  5.1290e-03,  ...,  2.9063e-02,\n",
      "          1.2194e-02,  1.4373e-02],\n",
      "        [-3.7559e-05, -6.9608e-02, -1.4076e-01,  ..., -2.1559e-01,\n",
      "         -1.9955e-01, -1.4432e-01],\n",
      "        [-8.4851e-02,  1.5171e+00, -2.7887e-02,  ..., -2.1178e-01,\n",
      "         -2.2086e-01,  1.5127e+00],\n",
      "        ...,\n",
      "        [-2.4030e-01, -1.0597e-01, -1.2524e-01,  ...,  0.0000e+00,\n",
      "          4.8396e+00,  1.2837e-01],\n",
      "        [-2.8311e-01,  6.3537e-01, -0.0000e+00,  ...,  1.9737e+00,\n",
      "          4.4695e+00, -5.3002e-02],\n",
      "        [-3.3610e-01, -1.2621e-01, -9.2682e-02,  ..., -5.5599e-02,\n",
      "          2.3322e+00, -1.3192e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 6.7652e-04, -1.7743e-03, -4.6521e-04,  ..., -0.0000e+00,\n",
      "          5.0704e-03,  3.2734e-03],\n",
      "        [-6.1835e-04, -7.7313e-02, -1.3735e-01,  ..., -2.1609e-01,\n",
      "         -1.9402e-01, -1.4175e-01],\n",
      "        [-2.8784e-01,  1.2848e+00, -3.3466e-01,  ..., -4.6732e-01,\n",
      "         -1.8443e-01,  1.0617e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00,  2.1982e-01, -2.6217e-01,  ...,  1.4039e+00,\n",
      "          4.6886e+00, -4.4586e-01],\n",
      "        [-1.7128e-01,  9.3944e-01, -2.3526e-01,  ...,  3.1337e-01,\n",
      "          4.5671e+00, -1.3234e-01],\n",
      "        [-3.6758e-01, -0.0000e+00, -3.9412e-01,  ...,  6.3522e-01,\n",
      "          2.7146e+00, -0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0188,  0.0133,  0.0120,  ...,  0.0403, -0.0047, -0.0045],\n",
      "        [-0.2062, -0.1943, -0.3217,  ..., -0.2262, -0.6390, -0.0000],\n",
      "        [-0.0631,  1.5474, -0.0341,  ..., -0.0336, -0.8276,  0.6068],\n",
      "        ...,\n",
      "        [-0.2130,  1.6654, -0.1136,  ..., -0.3527, -0.0000, -0.4831],\n",
      "        [-0.2084,  0.5038,  0.0000,  ...,  0.3068, -0.6483, -0.3332],\n",
      "        [-0.2562, -0.1615,  1.7914,  ..., -0.3770, -0.7360,  2.0621]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-9.5680e-03,  3.5528e-02,  3.1471e-02,  ..., -5.5176e-04,\n",
      "         -1.4099e-03,  3.6689e-02],\n",
      "        [ 0.0000e+00, -5.2545e-01, -1.5625e-01,  ..., -1.3251e-01,\n",
      "         -3.9506e-01, -3.9132e-01],\n",
      "        [-9.5397e-02,  9.6354e-01,  6.7529e-01,  ...,  2.4234e+00,\n",
      "         -1.0380e+00, -7.0584e-01],\n",
      "        ...,\n",
      "        [ 5.2530e-02,  8.1778e-01, -7.6223e-01,  ...,  7.6261e-02,\n",
      "         -4.5500e-01,  1.0462e+00],\n",
      "        [-2.2103e-01,  5.1047e-01, -5.5719e-01,  ..., -3.2087e-01,\n",
      "         -9.6137e-02, -9.7147e-03],\n",
      "        [-3.9648e-02, -1.7101e-01,  1.1781e+00,  ..., -0.0000e+00,\n",
      "         -2.8776e-01,  3.5716e+00]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0120,  0.0169, -0.0109,  ..., -0.0079, -0.0119, -0.0113],\n",
      "        [-0.5080, -0.4722, -0.2377,  ..., -0.4332, -0.0967, -0.0000],\n",
      "        [-0.8868, -0.4878,  0.0000,  ...,  0.5232, -0.3437, -1.0329],\n",
      "        ...,\n",
      "        [-0.2882,  1.0242, -0.0248,  ...,  0.0000, -0.0202, -0.0746],\n",
      "        [-0.4389, -0.1930,  0.5112,  ..., -0.1488, -0.0170, -0.5299],\n",
      "        [-0.2926, -0.3092,  1.5933,  ...,  0.0188, -0.2344,  2.9804]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 8.8453e-03, -2.8363e-03,  2.5994e-02,  ..., -5.7856e-03,\n",
      "         -4.7837e-03, -0.0000e+00],\n",
      "        [ 2.5874e+00, -2.3194e-01,  2.9425e+00,  ..., -0.0000e+00,\n",
      "         -3.2109e-01,  8.3888e-01],\n",
      "        [ 2.5821e+00, -2.2529e-02,  4.7124e+00,  ...,  6.1563e-01,\n",
      "         -6.1398e-01,  1.3334e+00],\n",
      "        ...,\n",
      "        [-2.8706e-01,  1.8400e+00,  2.9634e-01,  ..., -3.2276e-01,\n",
      "          1.4297e-01, -2.8291e-01],\n",
      "        [ 0.0000e+00,  1.6214e+00,  1.0299e-01,  ..., -2.5774e-01,\n",
      "         -5.5834e-01, -1.9867e-01],\n",
      "        [-7.8150e-02, -8.1972e-02, -0.0000e+00,  ..., -2.2776e-01,\n",
      "         -6.0314e-03, -1.5158e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0082, -0.0057,  0.0046,  ...,  0.0383,  0.0775, -0.0049],\n",
      "        [ 2.5867, -0.2348,  2.9212,  ..., -0.2993, -0.2969,  0.8338],\n",
      "        [ 2.1102,  1.5772,  0.0000,  ..., -0.6298,  0.0000,  2.4965],\n",
      "        ...,\n",
      "        [-0.2736,  2.0723, -0.2160,  ..., -0.2702,  1.3986, -0.2409],\n",
      "        [ 0.0823,  2.8256, -0.0596,  ...,  0.1680, -0.1810, -0.2194],\n",
      "        [-0.0783, -0.0848, -0.1398,  ..., -0.2124,  0.0725, -0.1529]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 1.9312e-04, -2.0026e-04, -1.8767e-03,  ..., -6.2532e-04,\n",
      "          4.0720e-03, -0.0000e+00],\n",
      "        [ 2.5787e+00, -2.2098e-01,  2.9088e+00,  ..., -2.9830e-01,\n",
      "         -3.0383e-01,  8.3811e-01],\n",
      "        [ 9.2725e-01,  1.0217e+00,  1.4118e+00,  ..., -1.8989e-01,\n",
      "         -3.6750e-01, -0.0000e+00],\n",
      "        ...,\n",
      "        [-3.8183e-01,  7.8467e-01, -4.4038e-01,  ..., -4.9510e-02,\n",
      "          3.3332e+00, -0.0000e+00],\n",
      "        [-2.7277e-01,  7.4702e-01, -4.0970e-01,  ...,  1.4258e+00,\n",
      "          3.4164e-01, -3.5294e-01],\n",
      "        [-7.7395e-02, -0.0000e+00, -1.3773e-01,  ..., -2.1453e-01,\n",
      "         -2.2120e-04, -1.4626e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 0.0667,  0.0154,  0.0140,  ...,  0.0658,  0.0255, -0.0039],\n",
      "        [-0.2160,  2.4430, -0.0939,  ...,  2.6901, -0.3127,  0.9507],\n",
      "        [ 3.6759,  2.5576,  1.4825,  ...,  3.4769, -0.1616,  2.8576],\n",
      "        ...,\n",
      "        [-0.1841,  1.2935, -0.3434,  ..., -0.7389, -1.2368, -0.6227],\n",
      "        [-0.0000,  0.2382, -0.8934,  ..., -1.1979, -1.0331, -0.0149],\n",
      "        [-0.1791, -0.2229, -0.6138,  ..., -0.4730, -0.7200, -0.5902]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-0.0095,  0.0670,  0.0360,  ...,  0.0296, -0.0062,  0.0373],\n",
      "        [-0.0934,  2.1446, -0.0375,  ...,  2.9810, -0.6730, -0.0652],\n",
      "        [-0.2031,  1.5790,  2.9485,  ...,  1.8513, -0.0106, -0.0000],\n",
      "        ...,\n",
      "        [-0.0000, -0.1196, -0.0000,  ..., -0.0000, -0.6581, -0.0833],\n",
      "        [-0.1047, -0.0857, -0.4422,  ...,  0.4630, -0.1647,  0.6775],\n",
      "        [-0.1356, -0.5110, -0.4655,  ...,  0.0996, -0.2027, -0.1728]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[-9.8359e-04, -1.8801e-03, -6.1338e-03,  ...,  7.5506e-03,\n",
      "         -6.6339e-05, -1.3895e-03],\n",
      "        [-5.3566e-01,  2.0223e+00, -0.0000e+00,  ...,  2.9351e+00,\n",
      "         -2.8303e-01, -2.1366e-01],\n",
      "        [-0.0000e+00, -0.0000e+00,  3.4143e+00,  ...,  1.8790e+00,\n",
      "          4.7820e-01, -1.1259e+00],\n",
      "        ...,\n",
      "        [-2.7629e-01,  5.5115e-02,  1.0965e+00,  ..., -4.1456e-02,\n",
      "         -0.0000e+00, -3.1600e-01],\n",
      "        [-4.6129e-01, -3.0283e-02,  1.0070e+00,  ...,  1.3363e+00,\n",
      "          0.0000e+00, -5.1564e-01],\n",
      "        [-3.1328e-01, -5.4606e-01, -2.6978e-01,  ...,  2.4680e-01,\n",
      "          2.9718e-01, -1.3474e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.6060e-02,  2.2869e-02, -2.1136e-03,  ...,  8.8671e-03,\n",
      "         -2.1184e-04, -6.2461e-04],\n",
      "        [ 5.4745e-02,  1.2557e+00, -1.7330e-01,  ..., -2.1575e-01,\n",
      "         -2.4039e-01,  2.7824e-03],\n",
      "        [ 3.0935e+00,  1.2920e+00, -2.2916e-01,  ..., -4.9160e-01,\n",
      "         -3.3310e-02, -6.0188e-02],\n",
      "        ...,\n",
      "        [ 7.0874e-02,  2.1463e+00, -2.0236e-01,  ..., -3.4601e-02,\n",
      "          1.8720e-01,  3.1332e-01],\n",
      "        [ 5.8255e-01,  2.3613e+00,  3.6922e-01,  ..., -3.5494e-01,\n",
      "         -4.1814e-01, -1.1492e-02],\n",
      "        [ 2.6064e+00, -2.2340e-01,  0.0000e+00,  ..., -3.0675e-01,\n",
      "         -3.1656e-01,  8.5154e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 2.2500e-02,  1.8603e-02,  0.0000e+00,  ...,  3.3077e-02,\n",
      "          1.0854e-02, -1.7311e-04],\n",
      "        [ 5.1185e-02,  1.2514e+00, -1.6168e-01,  ..., -2.0970e-01,\n",
      "         -2.3747e-01,  4.5886e-03],\n",
      "        [ 0.0000e+00,  1.7707e+00,  0.0000e+00,  ..., -4.8573e-01,\n",
      "          3.3424e+00, -1.5639e-01],\n",
      "        ...,\n",
      "        [-5.6542e-02,  2.1900e+00, -2.4115e-01,  ...,  1.5549e+00,\n",
      "         -6.5032e-02,  4.8638e-01],\n",
      "        [-4.9880e-02,  0.0000e+00, -2.0987e-01,  ..., -2.5787e-01,\n",
      "          7.1395e-01, -9.0959e-02],\n",
      "        [ 2.6028e+00, -2.2447e-01,  2.9537e+00,  ..., -3.0070e-01,\n",
      "         -0.0000e+00,  8.5335e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n",
      "tensor([[ 6.4486e-04, -1.7086e-03, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          1.2609e-03, -7.1636e-04],\n",
      "        [ 0.0000e+00,  1.2257e+00, -1.6570e-01,  ..., -2.1112e-01,\n",
      "         -0.0000e+00,  2.3074e-03],\n",
      "        [ 1.3875e-01,  1.5581e+00,  1.2035e-01,  ..., -3.8835e-01,\n",
      "          0.0000e+00, -2.0254e-01],\n",
      "        ...,\n",
      "        [-1.0695e-01,  3.1174e-01, -2.6591e-01,  ..., -3.0210e-02,\n",
      "         -2.1291e-02, -1.8610e-01],\n",
      "        [-2.5414e-01,  7.4046e-01, -3.2651e-01,  ...,  5.0701e-02,\n",
      "          2.9621e-01, -4.1584e-01],\n",
      "        [ 2.5810e+00, -2.2250e-01,  2.9126e+00,  ..., -2.9882e-01,\n",
      "         -3.0455e-01,  8.5107e-01]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14597/266398252.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# perform training and validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0ms_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mt_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0ms_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14597/3926526150.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# return self.mock_iter(epoch, self.train_data, train=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14597/3926526150.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tox/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tox/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tox/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tox/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tox/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform training.\n",
    "print(f'resume_from_epoch is {resume_from_epoch}')\n",
    "for epoch in range(resume_from_epoch + 1, args.epochs):\n",
    "    s_time = time.time()\n",
    "\n",
    "    # Data pre-loading.\n",
    "    if args.enable_multi_gpu:\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        train_data.clean_cache()\n",
    "        idxs = train_sampler.get_indices()\n",
    "        for local_gpu_idx in idxs:\n",
    "            train_data.load_data(local_gpu_idx)\n",
    "    d_time = time.time() - s_time\n",
    "\n",
    "    # perform training and validation.\n",
    "    s_time = time.time()\n",
    "    _, train_loss, _ = trainer.train(epoch)\n",
    "    t_time = time.time() - s_time\n",
    "    s_time = time.time()\n",
    "    _, val_loss, detailed_loss_val = trainer.test(epoch)\n",
    "    val_av_loss, val_bv_loss, val_fg_loss, _, _, _ = detailed_loss_val\n",
    "    v_time = time.time() - s_time\n",
    "\n",
    "    '''    # print information.\n",
    "    if master_worker:\n",
    "        print('Epoch: {:04d}'.format(epoch),\n",
    "              'loss_train: {:.6f}'.format(train_loss),\n",
    "              'loss_val: {:.6f}'.format(val_loss),\n",
    "              'loss_val_av: {:.6f}'.format(val_av_loss),\n",
    "              'loss_val_bv: {:.6f}'.format(val_bv_loss),\n",
    "              'loss_val_fg: {:.6f}'.format(val_fg_loss),\n",
    "              'cur_lr: {:.5f}'.format(trainer.scheduler.get_lr()[0]),\n",
    "              't_time: {:.4f}s'.format(t_time),\n",
    "              'v_time: {:.4f}s'.format(v_time),\n",
    "              'd_time: {:.4f}s'.format(d_time), flush=True)\n",
    "\n",
    "        if epoch % args.save_interval == 0:\n",
    "            trainer.save(epoch, model_dir)\n",
    "\n",
    "\n",
    "        trainer.save_tmp(epoch, model_dir, rank)'''\n",
    "\n",
    "# Only save final version.\n",
    "if master_worker:\n",
    "    trainer.save(args.epochs, model_dir, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0664dbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/rt/anaconda3/envs/tox/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_data_dl):\n",
    "    if i==0:\n",
    "        items = item\n",
    "        batch_graph = item[\"graph_input\"]\n",
    "        targets = item[\"targets\"]\n",
    "    else : break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213de4aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-1-4-1 배치 내부 구조\n",
    "- graph_input(graph상태를 의미)\n",
    "  - f_atoms\n",
    "  - f_bonds\n",
    "  - a2b\n",
    "  - b2a\n",
    "  - b2revb\n",
    "  - a_scope\n",
    "  - b_scope\n",
    "  - a2a\n",
    "- targets : Label값\n",
    "  - av_task : atom 맞추기\n",
    "  - bv_task : bond 맞추기\n",
    "  - fg_task : motif 맞추기(정확한걸 맞추는게 아니라 이 분자에 포함된 motif가 이거이거다 라는 식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b985810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14135/1623259844.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'items' is not defined"
     ]
    }
   ],
   "source": [
    "items['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcb7a8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0, 0],\n",
       "         [2, 4],\n",
       "         [1, 0],\n",
       "         [3, 0]]),\n",
       " tensor([0, 1, 2, 1, 3]),\n",
       " tensor([0, 2, 1, 4, 3]),\n",
       " tensor([[1, 3]]),\n",
       " tensor([[1, 4]]),\n",
       " tensor([[0, 0],\n",
       "         [2, 3],\n",
       "         [1, 0],\n",
       "         [1, 0]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol2graph(['C(O)O'], shared_dict, args).get_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94f06b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [2, 3],\n",
       "        [1, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol2graph(['C(O)O'], shared_dict, args).a2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcdebeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mol2graph(['C(O)O'], shared_dict, args).b2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53046a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0, 0],\n",
       "         [2, 4],\n",
       "         [1, 0],\n",
       "         [3, 0]]),\n",
       " tensor([0, 1, 2, 1, 3]),\n",
       " tensor([0, 2, 1, 4, 3]),\n",
       " tensor([[1, 3]]),\n",
       " tensor([[1, 4]]),\n",
       " tensor([[0, 0],\n",
       "         [2, 3],\n",
       "         [1, 0],\n",
       "         [1, 0]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4dcd585f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "642b5630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc90bb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3fbb640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e36afbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "092bf44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7f76b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8386e7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[6][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4f1c279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[7][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b650fb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets['av_task'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78056f2-0f57-4fc2-887a-0580b1b0331d",
   "metadata": {},
   "source": [
    "### 3.3. run_motif_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04140a21-a578-427a-899b-151f7e323402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_motif_training(args, logger):\n",
    "    \"\"\"\n",
    "    Run the pretrain task.\n",
    "    :param args:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # initalize the logger.\n",
    "    if logger is not None:\n",
    "        debug, _ = logger.debug, logger.info\n",
    "    else:\n",
    "        debug = print\n",
    "\n",
    "    # initialize the horovod library\n",
    "    if args.enable_multi_gpu:\n",
    "        mgw.init()\n",
    "\n",
    "    # binding training to GPUs.\n",
    "    master_worker = (mgw.rank() == 0) if args.enable_multi_gpu else True\n",
    "    # pin GPU to local rank. By default, we use gpu:0 for training.\n",
    "    local_gpu_idx = mgw.local_rank() if args.enable_multi_gpu else 0\n",
    "    with_cuda = args.cuda\n",
    "    if with_cuda:\n",
    "        torch.cuda.set_device(local_gpu_idx)\n",
    "\n",
    "    # get rank an  number of workers\n",
    "    rank = mgw.rank() if args.enable_multi_gpu else 0\n",
    "    num_replicas = mgw.size() if args.enable_multi_gpu else 1\n",
    "    # print(\"Rank: %d Rep: %d\" % (rank, num_replicas))\n",
    "\n",
    "    # load file paths of the data.\n",
    "    if master_worker:\n",
    "        print(args)\n",
    "        if args.enable_multi_gpu:\n",
    "            debug(\"Total workers: %d\" % (mgw.size()))\n",
    "        debug('Loading data')\n",
    "    data, sample_per_file = get_data(data_path=args.data_path)\n",
    "\n",
    "    # data splitting\n",
    "    if master_worker:\n",
    "        debug(f'Splitting data with seed 0.')\n",
    "    train_data, test_data, _ = split_data(data=data, sizes=(0.9, 0.1, 0.0), seed=0, logger=logger)\n",
    "\n",
    "    # Here the true train data size is the train_data divided by #GPUs\n",
    "    if args.enable_multi_gpu:\n",
    "        args.train_data_size = len(train_data) // mgw.size()\n",
    "    else:\n",
    "        args.train_data_size = len(train_data)\n",
    "    if master_worker:\n",
    "        debug(f'Total size = {len(data):,} | '\n",
    "              f'train size = {len(train_data):,} | val size = {len(test_data):,}')\n",
    "\n",
    "    # load atom and bond vocabulary and the semantic motif labels.\n",
    "    atom_vocab = MolVocab.load_vocab(args.atom_vocab_path)\n",
    "    bond_vocab = MolVocab.load_vocab(args.bond_vocab_path)\n",
    "    atom_vocab_size, bond_vocab_size = len(atom_vocab), len(bond_vocab)\n",
    "\n",
    "    # Load motif vocabulary for pretrain\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if args.parser_name == 'pretrain':\n",
    "        motif_vocab = [x.strip(\"\\r\\n \") for x in open(args.motif_vocab_path)]\n",
    "        motif_vocab = Motif_Vocab(motif_vocab)\n",
    "        #see below motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order).to(device)\n",
    "        \n",
    "        \n",
    "    # Hard coding here, since we haven't load any data yet!\n",
    "    fg_size = 85\n",
    "    shared_dict = {}\n",
    "    mol_collator = GroverCollator(shared_dict=shared_dict, atom_vocab=atom_vocab, bond_vocab=bond_vocab, args=args)\n",
    "    if master_worker:\n",
    "        debug(\"atom vocab size: %d, bond vocab size: %d, Number of FG tasks: %d\" % (atom_vocab_size,\n",
    "                                                                                    bond_vocab_size, fg_size))\n",
    "\n",
    "    # Define the distributed sampler. If using the single card, the sampler will be None.\n",
    "    train_sampler = None\n",
    "    test_sampler = None\n",
    "    shuffle = True\n",
    "    if args.enable_multi_gpu:\n",
    "        # If not shuffle, the performance may decayed.\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=True, sample_per_file=sample_per_file)\n",
    "        # Here sample_per_file in test_sampler is None, indicating the test sampler would not divide the test samples by\n",
    "        # rank. (TODO: bad design here.)\n",
    "        test_sampler = DistributedSampler(\n",
    "            test_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=False)\n",
    "        train_sampler.set_epoch(args.epochs)\n",
    "        test_sampler.set_epoch(1)\n",
    "        # if we enables multi_gpu training. shuffle should be disabled.\n",
    "        shuffle = False\n",
    "\n",
    "    # Pre load data. (Maybe unnecessary. )\n",
    "    pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
    "    pre_load_data(test_data, rank, num_replicas)\n",
    "    if master_worker:\n",
    "        # print(\"Pre-loaded training data: %d\" % train_data.count_loaded_datapoints())\n",
    "        print(\"Pre-loaded test data: %d\" % test_data.count_loaded_datapoints())\n",
    "\n",
    "    # Build dataloader\n",
    "    train_data_dl = DataLoader(train_data,\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=shuffle,\n",
    "                               num_workers=12,\n",
    "                               sampler=train_sampler,\n",
    "                               collate_fn=mol_collator)\n",
    "    test_data_dl = DataLoader(test_data,\n",
    "                              batch_size=args.batch_size,\n",
    "                              shuffle=shuffle,\n",
    "                              num_workers=10,\n",
    "                              sampler=test_sampler,\n",
    "                              collate_fn=mol_collator)\n",
    "\n",
    "    # Build the embedding model.\n",
    "    grover_model = GROVEREmbedding(args)\n",
    "\n",
    "    #  Build the trainer.\n",
    "    trainer = GROVERMotifTrainer(args=args,\n",
    "                            embedding_model=grover_model,\n",
    "                            atom_vocab_size=atom_vocab_size,\n",
    "                            bond_vocab_size=bond_vocab_size,\n",
    "                            fg_szie=fg_size,\n",
    "                            train_dataloader=train_data_dl,\n",
    "                            test_dataloader=test_data_dl,\n",
    "                            optimizer_builder=build_optimizer,\n",
    "                            scheduler_builder=build_lr_scheduler,\n",
    "                            logger=logger,\n",
    "                            with_cuda=with_cuda,\n",
    "                            enable_multi_gpu=args.enable_multi_gpu)\n",
    "\n",
    "    # Restore the interrupted training.\n",
    "    model_dir = os.path.join(args.save_dir, \"model\")\n",
    "    resume_from_epoch = 0\n",
    "    resume_scheduler_step = 0\n",
    "    if master_worker:\n",
    "        resume_from_epoch, resume_scheduler_step = trainer.restore(model_dir)\n",
    "    if args.enable_multi_gpu:\n",
    "        resume_from_epoch = mgw.broadcast(torch.tensor(resume_from_epoch), root_rank=0, name=\"resume_from_epoch\").item()\n",
    "        resume_scheduler_step = mgw.broadcast(torch.tensor(resume_scheduler_step),\n",
    "                                              root_rank=0, name=\"resume_scheduler_step\").item()\n",
    "        trainer.scheduler.current_step = resume_scheduler_step\n",
    "        print(\"Restored epoch: %d Restored scheduler step: %d\" % (resume_from_epoch, trainer.scheduler.current_step))\n",
    "    trainer.broadcast_parameters()\n",
    "\n",
    "    # Print model details.\n",
    "    if master_worker:\n",
    "        # Change order here.\n",
    "        print(grover_model)\n",
    "        print(\"Total parameters: %d\" % param_count(trainer.grover))\n",
    "\n",
    "    # Perform training.\n",
    "    for epoch in range(resume_from_epoch + 1, args.epochs):\n",
    "        s_time = time.time()\n",
    "\n",
    "        # Data pre-loading.\n",
    "        if args.enable_multi_gpu:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "            train_data.clean_cache()\n",
    "            idxs = train_sampler.get_indices()\n",
    "            for local_gpu_idx in idxs:\n",
    "                train_data.load_data(local_gpu_idx)\n",
    "        d_time = time.time() - s_time\n",
    "\n",
    "        # perform training and validation.\n",
    "        s_time = time.time()\n",
    "        _, train_loss, _ = trainer.train(epoch)\n",
    "        t_time = time.time() - s_time\n",
    "        s_time = time.time()\n",
    "        _, val_loss, detailed_loss_val = trainer.test(epoch)\n",
    "        val_av_loss, val_bv_loss, val_fg_loss, _, _, val_topo_loss, val_node_loss = detailed_loss_val\n",
    "        v_time = time.time() - s_time\n",
    "\n",
    "        # print information.\n",
    "        if master_worker:\n",
    "            print('Epoch: {:04d}'.format(epoch),\n",
    "                  'loss_train: {:.6f}'.format(train_loss),\n",
    "                  'loss_val: {:.6f}'.format(val_loss),\n",
    "                  'loss_val_av: {:.6f}'.format(val_av_loss),\n",
    "                  'loss_val_bv: {:.6f}'.format(val_bv_loss),\n",
    "                  'loss_val_fg: {:.6f}'.format(val_fg_loss),\n",
    "                  'loss_val_topo: {:.6f}'.format(val_topo_loss),\n",
    "                  'loss_val_node: {:.6f}'.format(val_node_loss),\n",
    "                  'cur_lr: {:.5f}'.format(trainer.scheduler.get_lr()[0]),\n",
    "                  't_time: {:.4f}s'.format(t_time),\n",
    "                  'v_time: {:.4f}s'.format(v_time),\n",
    "                  'd_time: {:.4f}s'.format(d_time), flush=True)\n",
    "\n",
    "            if epoch % args.save_interval == 0:\n",
    "                trainer.save(epoch, model_dir)\n",
    "\n",
    "\n",
    "            trainer.save_tmp(epoch, model_dir, rank)\n",
    "\n",
    "    # Only save final version.\n",
    "    if master_worker:\n",
    "        trainer.save(args.epochs, model_dir, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "96e9da1c-3407-4962-a085-d5fff8b66005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BatchMolDataset_motif at 0x7fe258304f10>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3bd10d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading data\n",
      "Splitting data with seed 0.\n",
      "Splitting data with seed 0.\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n",
      "Total size = 500,000 | train size = 450,000 | val size = 50,000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation='PReLU', atom_vocab_path='data/zinc10M/zinc10M_atom_vocab.pkl', backbone='gtrans', batch_size=100, bias=False, bond_drop_rate=0, bond_vocab_path='data/zinc10M/zinc10M_bond_vocab.pkl', cuda=True, data_path='data/zinc10M_0', dense=False, depth=3, dist_coff=0.1, dropout=0.1, embedding_output_type='both', enable_multi_gpu=False, epochs=20, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=1200, init_lr=0.0002, max_lr=0.0004, motif_hidden_size=1200, motif_latent_size=56, motif_order='dfs', motif_vocab_path='data/zinc10M/clique.txt', no_cache=True, num_attn_head=4, num_mt_block=1, parser_name='pretrain', save_dir='model/ChEMBL', save_interval=5, topology=True, undirected=False, wandb=False, wandb_name='pretrain', warmup_epochs=2.0, weight_decay=1e-07)\n",
      "Loading data:\n",
      "Number of files: 501\n",
      "Number of samples: 500000\n",
      "Samples/file: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n",
      "atom vocab size: 521, bond vocab size: 942, Number of FG tasks: 85\n"
     ]
    }
   ],
   "source": [
    "logger = create_logger(name='pretrain', save_dir=args.save_dir)\n",
    "if logger is not None:\n",
    "    debug, _ = logger.debug, logger.info\n",
    "else:\n",
    "    debug = print\n",
    "\n",
    "# initialize the horovod library\n",
    "if args.enable_multi_gpu:\n",
    "    mgw.init()\n",
    "\n",
    "# binding training to GPUs.\n",
    "master_worker = (mgw.rank() == 0) if args.enable_multi_gpu else True\n",
    "# pin GPU to local rank. By default, we use gpu:0 for training.\n",
    "local_gpu_idx = mgw.local_rank() if args.enable_multi_gpu else 0\n",
    "with_cuda = args.cuda\n",
    "if with_cuda:\n",
    "    torch.cuda.set_device(local_gpu_idx)\n",
    "\n",
    "# get rank an  number of workers\n",
    "rank = mgw.rank() if args.enable_multi_gpu else 0\n",
    "num_replicas = mgw.size() if args.enable_multi_gpu else 1\n",
    "# print(\"Rank: %d Rep: %d\" % (rank, num_replicas))\n",
    "\n",
    "# load file paths of the data.\n",
    "if master_worker:\n",
    "    print(args)\n",
    "    if args.enable_multi_gpu:\n",
    "        debug(\"Total workers: %d\" % (mgw.size()))\n",
    "    debug('Loading data')\n",
    "#data, sample_per_file = get_data(data_path=args.data_path)\n",
    "data, sample_per_file = get_motif_data(data_path=args.data_path)\n",
    "\n",
    "# data splitting\n",
    "if master_worker:\n",
    "    debug(f'Splitting data with seed 0.')\n",
    "train_data, test_data, _ = split_data(data=data, sizes=(0.9, 0.1, 0.0), seed=0, logger=logger)\n",
    "\n",
    "# Here the true train data size is the train_data divided by #GPUs\n",
    "if args.enable_multi_gpu:\n",
    "    args.train_data_size = len(train_data) // mgw.size()\n",
    "else:\n",
    "    args.train_data_size = len(train_data)\n",
    "if master_worker:\n",
    "    debug(f'Total size = {len(data):,} | '\n",
    "          f'train size = {len(train_data):,} | val size = {len(test_data):,}')\n",
    "\n",
    "# load atom and bond vocabulary and the semantic motif labels.\n",
    "atom_vocab = MolVocab.load_vocab(args.atom_vocab_path)\n",
    "bond_vocab = MolVocab.load_vocab(args.bond_vocab_path)\n",
    "atom_vocab_size, bond_vocab_size = len(atom_vocab), len(bond_vocab)\n",
    "\n",
    "# Load motif vocabulary for pretrain\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if args.parser_name == 'pretrain':\n",
    "    motif_vocab = [x.strip(\"\\r\\n \") for x in open(args.motif_vocab_path)]\n",
    "    motif_vocab = Motif_Vocab(motif_vocab)\n",
    "    #see below motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order).to(device)\n",
    "\n",
    "\n",
    "# Hard coding here, since we haven't load any data yet!\n",
    "fg_size = 85\n",
    "shared_dict = {}\n",
    "motif_collator = GroverMotifCollator(shared_dict=shared_dict, atom_vocab=atom_vocab, bond_vocab=bond_vocab, args=args)\n",
    "\n",
    "if master_worker:\n",
    "    debug(\"atom vocab size: %d, bond vocab size: %d, Number of FG tasks: %d\" % (atom_vocab_size,\n",
    "                                                                                bond_vocab_size, fg_size))\n",
    "\n",
    "# Define the distributed sampler. If using the single card, the sampler will be None.\n",
    "train_sampler = None\n",
    "test_sampler = None\n",
    "shuffle = True\n",
    "if args.enable_multi_gpu:\n",
    "    # If not shuffle, the performance may decayed.\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=True, sample_per_file=sample_per_file)\n",
    "    # Here sample_per_file in test_sampler is None, indicating the test sampler would not divide the test samples by\n",
    "    # rank. (TODO: bad design here.)\n",
    "    test_sampler = DistributedSampler(\n",
    "        test_data, num_replicas=mgw.size(), rank=mgw.rank(), shuffle=False)\n",
    "    train_sampler.set_epoch(args.epochs)\n",
    "    test_sampler.set_epoch(1)\n",
    "    # if we enables multi_gpu training. shuffle should be disabled.\n",
    "    shuffle = False\n",
    "\n",
    "# Pre load data. (Maybe unnecessary. )\n",
    "#pre_load_data(train_data, rank, num_replicas, sample_per_file)\n",
    "#pre_load_data(test_data, rank, num_replicas)\n",
    "#if master_worker:\n",
    "    # print(\"Pre-loaded training data: %d\" % train_data.count_loaded_datapoints())\n",
    "#    print(\"Pre-loaded test data: %d\" % test_data.count_loaded_datapoints())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa794e84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found %d\n",
      "GROVEREmbedding(\n",
      "  (encoders): GTransEncoder(\n",
      "    (edge_blocks): ModuleList(\n",
      "      (0): MTBlock(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (W_i): Linear(in_features=165, out_features=1200, bias=False)\n",
      "        (attn): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            (1): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            (2): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (W_o): Linear(in_features=4800, out_features=1200, bias=False)\n",
      "        (sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (node_blocks): ModuleList(\n",
      "      (0): MTBlock(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (W_i): Linear(in_features=151, out_features=1200, bias=False)\n",
      "        (attn): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            (1): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            (2): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (W_o): Linear(in_features=4800, out_features=1200, bias=False)\n",
      "        (sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ffn_atom_from_atom): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=1351, out_features=4800, bias=True)\n",
      "      (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_atom_from_bond): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=1351, out_features=4800, bias=True)\n",
      "      (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_bond_from_atom): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=1365, out_features=4800, bias=True)\n",
      "      (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_bond_from_bond): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=1365, out_features=4800, bias=True)\n",
      "      (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (atom_from_atom_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (atom_from_bond_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (bond_from_atom_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (bond_from_bond_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (act_func_node): PReLU(num_parameters=1)\n",
      "    (act_func_edge): PReLU(num_parameters=1)\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Total parameters: 107143232\n"
     ]
    }
   ],
   "source": [
    "# Build dataloader\n",
    "train_data_dl = DataLoader(train_data,\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=0,\n",
    "                           sampler=train_sampler,\n",
    "                           collate_fn=motif_collator)\n",
    "test_data_dl = DataLoader(test_data,\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=0,\n",
    "                          sampler=test_sampler,\n",
    "                          collate_fn=motif_collator)\n",
    "args.train_data_size=len(train_data)\n",
    "# Build the embedding model.\n",
    "grover_model = GROVEREmbedding(args)\n",
    "\n",
    "# build the topology predict model.\n",
    "motif_model = Motif_Generation(motif_vocab, args.motif_hidden_size, args.motif_latent_size, 3, device, args.motif_order).to(device)\n",
    "\n",
    "#  Build the trainer.\n",
    "trainer = GROVERMotifTrainer(args=args,\n",
    "                        embedding_model=grover_model,\n",
    "                        topology_model = motif_model,\n",
    "                        atom_vocab_size=atom_vocab_size,\n",
    "                        bond_vocab_size=bond_vocab_size,\n",
    "                        fg_size=fg_size,\n",
    "                        train_dataloader=train_data_dl,\n",
    "                        test_dataloader=test_data_dl,\n",
    "                        optimizer_builder=build_optimizer,\n",
    "                        scheduler_builder=build_lr_scheduler,\n",
    "                        logger=logger,\n",
    "                        with_cuda=with_cuda,\n",
    "                        enable_multi_gpu=args.enable_multi_gpu)\n",
    "\n",
    "# Restore the interrupted training.\n",
    "model_dir = os.path.join(args.save_dir, \"model\")\n",
    "resume_from_epoch = 0\n",
    "resume_scheduler_step = 0\n",
    "if master_worker:\n",
    "    resume_from_epoch, resume_scheduler_step = trainer.restore(model_dir)\n",
    "if args.enable_multi_gpu:\n",
    "    resume_from_epoch = mgw.broadcast(torch.tensor(resume_from_epoch), root_rank=0, name=\"resume_from_epoch\").item()\n",
    "    resume_scheduler_step = mgw.broadcast(torch.tensor(resume_scheduler_step),\n",
    "                                          root_rank=0, name=\"resume_scheduler_step\").item()\n",
    "    trainer.scheduler.current_step = resume_scheduler_step\n",
    "    print(\"Restored epoch: %d Restored scheduler step: %d\" % (resume_from_epoch, trainer.scheduler.current_step))\n",
    "trainer.broadcast_parameters()\n",
    "\n",
    "# Print model details.\n",
    "if master_worker:\n",
    "    # Change order here.\n",
    "    print(grover_model)\n",
    "    print(\"Total parameters: %d\" % param_count(trainer.grover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5279d45-38a7-4c25-ac81-76b103de0e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3945/501167332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_graph_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"graph_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtargets_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmoltree_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"moltree\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/grover/data/groverdataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mdp_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_per_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mreal_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_per_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreal_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3945/3388227146.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapoints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "for i, item in enumerate(train_data_dl):\n",
    "    batch_graph_test = item[\"graph_input\"]\n",
    "    targets_test = item[\"targets\"]\n",
    "    moltree_test = item[\"moltree\"]\n",
    "\n",
    "    if next(embed_model.parameters()).is_cuda:\n",
    "        targets_test[\"av_task\"] = targets_test[\"av_task\"].cuda()\n",
    "        targets_test[\"bv_task\"] = targets_test[\"bv_task\"].cuda()\n",
    "        targets_test[\"fg_task\"] = targets_test[\"fg_task\"].cuda()\n",
    "    preds_test = embed_model_test(batch_graph)\n",
    "    emb_test = preds_test['emb_vec']\n",
    "    #_, motif_loss, _ = motif_model(emb)\n",
    "    print(time.time()-stime)\n",
    "    if i == 0 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98ce84af-82d9-4aae-a6a5-59372b4aeade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter start\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/grover/grover/data/groverdataset.py\", line 158, in __getitem__\n    return self.data[dp_idx][real_idx]\n  File \"/tmp/ipykernel_1191/3388227146.py\", line 49, in __getitem__\n    assert self.datapoints is not None\nAssertionError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1191/974610269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# perform training and validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ms_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mt_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0ms_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1191/4013720795.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \"\"\"\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# return self.mock_iter(epoch, self.train_data, train=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1191/4013720795.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# loss_func = self.model.get_loss_func(self.args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mbatch_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"graph_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/grover/grover/data/groverdataset.py\", line 158, in __getitem__\n    return self.data[dp_idx][real_idx]\n  File \"/tmp/ipykernel_1191/3388227146.py\", line 49, in __getitem__\n    assert self.datapoints is not None\nAssertionError\n"
     ]
    }
   ],
   "source": [
    "# Perform training.\n",
    "best_val_loss = 0\n",
    "best_val_epoch = 0\n",
    "best_model_dir = os.path.join(args.save_dir, \"model_best\")\n",
    "for epoch in range(args.epochs):\n",
    "    s_time = time.time()\n",
    "\n",
    "    # Data pre-loading.\n",
    "    if args.enable_multi_gpu:\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        train_data.clean_cache()\n",
    "        idxs = train_sampler.get_indices()\n",
    "        for local_gpu_idx in idxs:\n",
    "            train_data.load_data(local_gpu_idx)\n",
    "    d_time = time.time() - s_time\n",
    "\n",
    "    # perform training and validation.\n",
    "    s_time = time.time()\n",
    "    _, train_loss, _ = trainer.train(epoch)\n",
    "    t_time = time.time() - s_time\n",
    "    s_time = time.time()\n",
    "    _, val_loss, detailed_loss_val = trainer.test(epoch)\n",
    "    val_av_loss, val_bv_loss, val_fg_loss, _, _, _, val_topo_loss, val_node_loss = detailed_loss_val\n",
    "    v_time = time.time() - s_time\n",
    "    \n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_epoch = epoch\n",
    "        trainer.save(epoch, best_model_dir)\n",
    "    \n",
    "    wandb.log({\"train_loss\" : train_loss, \"val_loss\" : val_loss, \"topo_loss\" : val_topo_loss, \"epochs\" : epoch})\n",
    "    \n",
    "    # print information.\n",
    "    if master_worker:\n",
    "        print('Epoch: {:04d}\\n'.format(epoch),\n",
    "              'loss_train: {:.6f}'.format(train_loss),\n",
    "              'loss_val: {:.6f}'.format(val_loss),\n",
    "              'loss_val_av: {:.6f}'.format(val_av_loss),\n",
    "              'loss_val_bv: {:.6f}'.format(val_bv_loss),\n",
    "              'loss_val_fg: {:.6f}'.format(val_fg_loss),\n",
    "              'loss_val_topo: {:.6f}'.format(val_topo_loss),\n",
    "              'loss_val_node: {:.6f}'.format(val_node_loss),\n",
    "              'cur_lr: {:.5f}'.format(trainer.scheduler.get_lr()[0]),\n",
    "              't_time: {:.4f}s'.format(t_time),\n",
    "              'v_time: {:.4f}s'.format(v_time),\n",
    "              'd_time: {:.4f}s'.format(d_time), flush=True)\n",
    "\n",
    "        if epoch % args.save_interval == 0:\n",
    "            trainer.save(epoch, model_dir)\n",
    "\n",
    "\n",
    "        trainer.save_tmp(epoch, model_dir, rank)\n",
    "\n",
    "# Only save final version.\n",
    "if master_worker:\n",
    "    trainer.save(args.epochs, model_dir, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce52afd9-3feb-4fb4-9877-0c7583424050",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0748a503-7d2b-4a9a-869e-0077223a8d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.672407817840576,\n",
       " 5.451661777496338,\n",
       " 1.284084439277649,\n",
       " 0.4274427592754364,\n",
       " 0.47347733080387117,\n",
       " 0.00922257686033845,\n",
       " 10.510714721679687)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d6877f8f-69ea-4428-aa23-aa1f7b000402",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r model/tryout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37111a82",
   "metadata": {},
   "source": [
    "## 2-2. pretrain_model() 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba276d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_model(args: Namespace, logger: Logger = None):\n",
    "    \"\"\"\n",
    "    The entrey of pretrain.\n",
    "    :param args: the argument.\n",
    "    :param logger: the logger.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # avoid auto optimized import by pycharm.\n",
    "    a = MolVocab\n",
    "    s_time = time.time()\n",
    "    run_training(args=args, logger=logger)\n",
    "    e_time = time.time()\n",
    "    print(\"Total Time: %.3f\" % (e_time - s_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897836d",
   "metadata": {},
   "source": [
    "# main.py 실행코드"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d20ae015",
   "metadata": {},
   "source": [
    "# setup random seed\n",
    "setup(seed=42)\n",
    "# Avoid the pylint warning.\n",
    "a = MolVocab\n",
    "# supress rdkit logger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# Initialize MolVocab\n",
    "mol_vocab = MolVocab\n",
    "\n",
    "args = parse_args()\n",
    "logger = create_logger(name='pretrain', save_dir=args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d848b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading data\n",
      "Splitting data with seed 0.\n",
      "Splitting data with seed 0.\n",
      "Total size = 5,970 | train size = 5,400 | val size = 570\n",
      "Total size = 5,970 | train size = 5,400 | val size = 570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation='PReLU', atom_vocab_path='data/pretrain/tryout_atom_vocab.pkl', backbone='gtrans', batch_size=32, bias=False, bond_drop_rate=0, bond_vocab_path='data/pretrain/tryout_bond_vocab.pkl', cuda=True, data_path='data/pretrain/tryout', dense=False, depth=5, dist_coff=0.1, dropout=0.1, embedding_output_type='both', enable_multi_gpu=False, epochs=3, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=100, init_lr=0.0001, max_lr=0.001, motif_hidden_size=300, motif_latent_size=56, motif_order='bfs', motif_vocab_path='data/pretrain/clique.txt', no_cache=True, num_attn_head=1, num_mt_block=1, parser_name='pretrain', save_dir='model/tryout', save_interval=9999999999, train_data_size=5400, undirected=False, warmup_epochs=2.0, weight_decay=0.0)\n",
      "Loading data:\n",
      "Number of files: 60\n",
      "Number of samples: 5970\n",
      "Samples/file: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "atom vocab size: 324, bond vocab size: 353, Number of FG tasks: 85\n",
      "atom vocab size: 324, bond vocab size: 353, Number of FG tasks: 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loaded test data: 6\n",
      "Restore checkpoint, current epoch: 1\n",
      "GROVEREmbedding(\n",
      "  (encoders): GTransEncoder(\n",
      "    (edge_blocks): ModuleList(\n",
      "      (0): MTBlock(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
      "        (attn): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "            (1): Linear(in_features=100, out_features=100, bias=True)\n",
      "            (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
      "        (sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (node_blocks): ModuleList(\n",
      "      (0): MTBlock(\n",
      "        (heads): ModuleList(\n",
      "          (0): Head(\n",
      "            (mpn_q): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "            )\n",
      "            (mpn_k): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "            )\n",
      "            (mpn_v): MPNEncoder(\n",
      "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "              (act_func): PReLU(num_parameters=1)\n",
      "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
      "        (attn): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "            (1): Linear(in_features=100, out_features=100, bias=True)\n",
      "            (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
      "        (sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ffn_atom_from_atom): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
      "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_atom_from_bond): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
      "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_bond_from_atom): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
      "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (ffn_bond_from_bond): PositionwiseFeedForward(\n",
      "      (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
      "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (atom_from_atom_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (atom_from_bond_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (bond_from_atom_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (bond_from_bond_sublayer): SublayerConnection(\n",
      "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (act_func_node): PReLU(num_parameters=1)\n",
      "    (act_func_edge): PReLU(num_parameters=1)\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Total parameters: 768614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
      "/home/rt/tox/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002 loss_train: 3.373643 loss_val: 2.182687 loss_val_av: 0.507859 loss_val_bv: 0.846167 loss_val_fg: 0.828661 cur_lr: 0.00017 t_time: 10.2503s v_time: 0.8160s d_time: 0.0000s\n",
      "EP:3 Model Saved on: model/tryout/model.ep3\n",
      "Total Time: 87.221\n"
     ]
    }
   ],
   "source": [
    "pretrain_model(args, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06e3b837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(activation='PReLU', atom_vocab_path='data/pretrain/CO2_atom_vocab.pkl', backbone='gtrans', batch_size=1, bias=False, bond_drop_rate=0, bond_vocab_path='data/pretrain/CO2_bond_vocab.pkl', cuda=True, data_path='data/pretrain/CO2', dense=False, depth=5, dist_coff=0.1, dropout=0.0, embedding_output_type='both', enable_multi_gpu=False, epochs=3, fg_label_path=None, final_lr=0.0001, fine_tune_coff=1, hidden_size=3, init_lr=0.0001, max_lr=0.001, motif_hidden_size=300, motif_latent_size=56, motif_order='bfs', motif_vocab_path='data/pretrain/clique.txt', no_cache=True, num_attn_head=4, num_mt_block=1, parser_name='pretrain', save_dir='model/tryout', save_interval=9999999999, train_data_size=10, undirected=False, warmup_epochs=2.0, weight_decay=0.0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b52b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load('data/pretrain/tryout/feature/0.npz')\n",
    "npzfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e29686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = Chem.MolFromSmiles('C1CCC23C4=C5C6=C2C2=C7C8=C9C%10=C(C%11=C%12C%13=C%10C%10=C%14C%15=C%16C%17=C%18C%19=C%15C(=C%149)C7=C7C2=C2C6=C6C9=C%14C%15=C%20C(=C%12C(=C%114)C%15=C59)C4=C%13C%10C%16C5=C%17C9=C(C%20=C45)C%14=C4C9=C%18C(=C7%19)C2=C64)C83C1')\n",
    "mol2 = Chem.MolFromSmiles('CC(C)(C)C1=CC(O)=CC=C1O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6428c63-1eff-4943-8e6e-241a249f1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles='CC(C)(C)C1=CC(O)=CC=C1O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8603d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MolTree' object has no attribute 'savez_compressed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34/1146342262.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmol_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmol_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmol_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'MolTree' object has no attribute 'savez_compressed'"
     ]
    }
   ],
   "source": [
    "mol_tree = MolTree(smiles)\n",
    "mol_tree.recover()\n",
    "mol_tree.assemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e6906e-f351-4084-9928-d6916c49cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "mol_tree = MolTree(smiles)\n",
    "mol_tree.recover()\n",
    "mol_tree.assemble()\n",
    "with open('moltree.p', 'wb') as file: \n",
    "    pickle.dump(mol_tree, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f5ee931-e3bc-48ec-84f0-bbb7b5e66377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<grover.topology.mol_tree.MolTree at 0x7f04defd7550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d6558-fe23-4a5f-b8a7-f30062c3e9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55f20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moltree.p', 'wb') as file: \n",
    "    pickle.dump(mol_tree, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba7aeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moltree.p', 'rb') as file:\n",
    "    mol_tree2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "960f71a0-cf2b-4ce2-b2a8-dbb895b0fd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<grover.topology.mol_tree.MolTree at 0x7f04df042590>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol_tree2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c4584-596e-4fda-9b87-1867bcaa8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "sys.path.append('./')\n",
    "from grover.topology.chemutils import *\n",
    "from grover.topology.mol_tree import *\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--datapath', type=str, default='./data/zinc/all.txt',\n",
    "                        help='root directory of dataset. For now, only classification.')\n",
    "parser.add_argument('--output_clique', type=str, default='./clique.txt',\n",
    "                        help='filename to output the pre-trained model')\n",
    "parser.add_argument('--output_data', type=str, default='./data/zinc/all_edit.txt',\n",
    "                        help='filename to output deleted data')\n",
    "args = parser.parse_args(['--datapath','data/pretraindata])\n",
    "\n",
    "lg = rdkit.RDLogger.logger() \n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "data = pd.read_csv(args.datapath)\n",
    "data_len = len(data)\n",
    "print(data_len)\n",
    "\n",
    "num=0\n",
    "cset = set()\n",
    "counts = {}\n",
    "\n",
    "print(\"start\")\n",
    "s_time = time.time()\n",
    "\n",
    "for i in range(data_len):\n",
    "    if num%10000==0:print(f'process : {num} / {data_len}')\n",
    "    smiles = data.smiles[num]\n",
    "    try : \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        mol.GetNumHeavyAtoms()\n",
    "        moltree = MolTree(smiles)\n",
    "        for node in moltree.nodes:\n",
    "            cset.add(node.smiles)\n",
    "            if node.smiles not in counts:\n",
    "                counts[node.smiles] = 1\n",
    "            else:\n",
    "                counts[node.smiles] += 1\n",
    "    except : \n",
    "        print(f'error smiles is {smiles}')\n",
    "        data=data.drop(num)\n",
    "    num += 1\n",
    "\n",
    "\n",
    "print(\"Preprocessing Completed!\")\n",
    "t_time = time.time() - s_time\n",
    "print(f'total time is {t_time:.4f}s, data length is {data_len} -> {len(data)}')\n",
    "\n",
    "clique_list = list(cset)\n",
    "\n",
    "data.to_csv(args.output_data, index=False)\n",
    "\n",
    "with open(args.output_clique, 'w') as file:\n",
    "    for c in clique_list:\n",
    "        file.write(c)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc801ef2-abdf-4cec-b175-c9e95cfc11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77a6d07-b3a6-4e20-ab84-d11f872e4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_features('data/zinc10M/feature/1.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bb47ff-b3d0-490f-b127-b39d75f79a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea0a674-4f8c-4dfc-ba0b-c8305f4fd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv = pd.read_csv('data/mgssl_test/graph/0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cf04187-82f1-487b-ac9d-298bd50d24d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Cc1cc(Cl)ccc1OCC(=O)N/N=C/c1ccccn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>O=C1NC(=S)NC(=O)C1=CNc1ccc([N+](=O)[O-])cc1O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Cc1c(C(=O)N2CCOCC2)oc2c1-c1nn(CC(=O)NCc3ccco3)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>CCc1ccc(CNC(=O)c2ccc(-c3nccnc3N3CCCCC3)cc2)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>COc1ccc([C@H]2C[C@@H](C(F)(F)F)n3nc(C(=O)NC4CC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               smiles\n",
       "0             CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1\n",
       "1        C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1\n",
       "2   N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...\n",
       "3   CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...\n",
       "4   N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...\n",
       "..                                                ...\n",
       "95                 Cc1cc(Cl)ccc1OCC(=O)N/N=C/c1ccccn1\n",
       "96       O=C1NC(=S)NC(=O)C1=CNc1ccc([N+](=O)[O-])cc1O\n",
       "97  Cc1c(C(=O)N2CCOCC2)oc2c1-c1nn(CC(=O)NCc3ccco3)...\n",
       "98     CCc1ccc(CNC(=O)c2ccc(-c3nccnc3N3CCCCC3)cc2)cc1\n",
       "99  COc1ccc([C@H]2C[C@@H](C(F)(F)F)n3nc(C(=O)NC4CC...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c65cfa6-1e9a-4c4f-8b9a-f673f3123304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f2300-78d3-44a5-aedc-e0039967f160",
   "metadata": {},
   "source": [
    "# test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88f8947d-d5ee-4a14-b93d-79dd4bc1c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_tracker():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.num_subset = 0\n",
    "        self.now_subset = 0\n",
    "        self.now_iter = 0\n",
    "        self.origin_data_path = args.data_path\n",
    "        \n",
    "        \n",
    "    def save_process(self):\n",
    "        path = os.path.join(self.args.save_dir, \"process.txt\")\n",
    "        txt = open(path, 'w')\n",
    "        txt.write(\"num_subset:%d\\n\" % (self.num_subset))\n",
    "        txt.write(\"now_subset:%d\\n\" % (self.now_subset))\n",
    "        txt.write(\"now_iter:%d\\n\" % (self.now_iter))\n",
    "        txt.close()\n",
    "        print('process saved')\n",
    "        \n",
    "    def load_process(self):\n",
    "        '''\n",
    "        if you don't have saved data, you must make txt file like below\n",
    "        \n",
    "        num_subset:0\n",
    "        now_subset:0\n",
    "        now_iter:0\n",
    "        '''\n",
    "        path = os.path.join(self.args.save_dir, \"process.txt\")\n",
    "        f = open(path, 'r')\n",
    "        lines = f.readlines()\n",
    "        self.num_subset = np.int(lines[0].split(':')[1].split('\\n')[0])\n",
    "        self.now_subset = np.int(lines[1].split(':')[1].split('\\n')[0])\n",
    "        self.now_iter = np.int(lines[2].split(':')[1].split('\\n')[0])\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73e56e4f-67ff-4cd9-95d9-d63bcf98a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = process_tracker(args)\n",
    "pt.load_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ced88d9-73d6-4094-b532-03573bcf3128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.now_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0cce15a-3cbc-4e8b-a0c0-57d2dd137d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c25aa2e-90d1-462b-ae93-efb9b21fdd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(args.epochs % args.each_epochs) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fec867a8-0333-44a8-9abb-8f59e3a87f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_learning(args: Namespace, logger: Logger = None, process = None):\n",
    "    a = MolVocab\n",
    "    assert (args.epochs % args.each_epochs) == 0, 'you must make args.epochs % args.each_epochs = 0'\n",
    "    \n",
    "    left_iters = args.epochs/args.each_epochs - process.now_iter\n",
    "    for iters in range(left_iters):\n",
    "        left_subsets = process.num_subset - process.now_subset\n",
    "        \n",
    "        # run_motif_training until all subset\n",
    "        for num_subset in range(left_subsets):\n",
    "            args.data_path = process.origin_data_path+f'_{process.now_subset}'\n",
    "            run_motif_training(args=args, logger=logger)\n",
    "            process.now_subset += 1\n",
    "            process.save_process()\n",
    "            \n",
    "        process.now_subset = 0\n",
    "        process.now_iter += 1\n",
    "        process.save_process()\n",
    "        \n",
    "    print('all process is end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "927b14b5-8f49-420f-997e-f74be3cd44f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n",
      "process saved\n"
     ]
    }
   ],
   "source": [
    "a = MolVocab\n",
    "assert (args.epochs % args.each_epochs) == 0, 'you must make args.epochs % args.each_epochs = 0'\n",
    "\n",
    "left_iters = np.int(args.epochs/args.each_epochs - pt.now_iter)\n",
    "for iters in range(left_iters):\n",
    "    left_subsets = pt.num_subset - pt.now_subset\n",
    "\n",
    "    # run_motif_training until all subset\n",
    "    for num_subset in range(left_subsets):\n",
    "        args.data_path = pt.origin_data_path+f'_{pt.now_subset}'\n",
    "        #run_motif_training(args=args, logger=logger)\n",
    "        pt.now_subset += 1\n",
    "        pt.save_process()\n",
    "\n",
    "    pt.now_subset = 0\n",
    "    pt.now_iter += 1\n",
    "    pt.save_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08dcfa01-e787-48e1-9409-122c1e547189",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-a78492bf780b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msubset_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-5823f300c41f>\u001b[0m in \u001b[0;36msubset_learning\u001b[0;34m(args, logger, process)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mleft_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meach_epochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mleft_subsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_subset\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow_subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "logger = create_logger(name='pretrain', save_dir=args.save_dir)\n",
    "pt = process_tracker(args)\n",
    "pt.load_process()\n",
    "subset_learning(args, logger, pt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
