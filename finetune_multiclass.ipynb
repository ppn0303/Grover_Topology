{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3754146-e986-4b43-93d6-a3d5891fdabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Horovod for multi-GPU training\n"
     ]
    }
   ],
   "source": [
    "import random, os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from grover.util.parsing import parse_args, get_newest_train_args\n",
    "from grover.util.utils import create_logger\n",
    "from task.cross_validate import cross_validate, randomsearch, gridsearch, make_confusion_matrix\n",
    "from task.fingerprint import generate_fingerprints, generate_embvec\n",
    "from task.predict import make_predictions, write_prediction\n",
    "from task.pretrain import pretrain_model, subset_learning\n",
    "from grover.data.torchvocab import MolVocab\n",
    "\n",
    "from grover.topology.mol_tree import *\n",
    "\n",
    "#add for gridsearch\n",
    "from argparse import ArgumentParser, Namespace\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a506430a-2710-44d6-8a6e-851710fc9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from grover.data import MolCollator\n",
    "from grover.data import StandardScaler\n",
    "from grover.util.metrics import get_metric_func\n",
    "from grover.util.nn_utils import initialize_weights, param_count\n",
    "from grover.util.scheduler import NoamLR\n",
    "from grover.util.utils import build_optimizer, build_lr_scheduler, makedirs, load_checkpoint, get_loss_func, \\\n",
    "    save_checkpoint, build_model\n",
    "from grover.util.utils import get_class_sizes, get_data, split_data, get_task_names\n",
    "from task.predict import predict, evaluate, evaluate_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae477852-7baa-4132-bdbc-e66ce0c2a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(seed):\n",
    "    # frozen random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75bacae4-f07c-4a33-8724-93d0dd2eb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.parsing import *\n",
    "def parse_args() -> Namespace:\n",
    "    \"\"\"\n",
    "    Parses arguments for training and testing (includes modifying/validating arguments).\n",
    "\n",
    "    :return: A Namespace containing the parsed, modified, and validated args.\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "                                      dest=\"parser_name\",\n",
    "                                      help=\"Subcommands for fintune, prediction, and fingerprint.\")\n",
    "    parser_finetune = subparser.add_parser('finetune', help=\"Fine tune the pre-trained model.\")\n",
    "    add_finetune_args(parser_finetune)\n",
    "    parser_eval = subparser.add_parser('eval', help=\"Evaluate the results of the pre-trained model.\")\n",
    "    add_finetune_args(parser_eval)\n",
    "    parser_predict = subparser.add_parser('predict', help=\"Predict results from fine tuned model.\")\n",
    "    add_predict_args(parser_predict)\n",
    "    parser_fp = subparser.add_parser('fingerprint', help=\"Get the fingerprints of SMILES.\")\n",
    "    add_fingerprint_args(parser_fp)\n",
    "    parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "    add_pretrain_args(parser_pretrain)\n",
    "\n",
    "    #args = parser.parse_args(['eval','--data_path','data/tg407.csv','--features_path','data/tg407.npz','--save_dir','model/test','--checkpoint_path','grover_large.pt','--self_attention','--no_features_scaling','--split_type','scaffold_balanced','--epochs','3','--ffn_hidden_size','900','--num_folds','3','--batch_size','32'])\n",
    "    args = parser.parse_args(['eval', '--data_path', 'data/tg423.csv', '--features_path', 'data/tg423.npz', '--dataset_type', 'classification','--split_type', 'scaffold_balanced', '--no_features_scaling' ,'--num_folds', '3','--checkpoint_dir', 'model/tg423_RS1/iter_16', '--confusionmatrix'])\n",
    "\n",
    "    if args.parser_name == 'finetune' or args.parser_name == 'eval':\n",
    "        modify_train_args(args)\n",
    "    elif args.parser_name == \"pretrain\":\n",
    "        modify_pretrain_args(args)\n",
    "    elif args.parser_name == 'predict':\n",
    "        modify_predict_args(args)\n",
    "    elif args.parser_name == 'fingerprint':\n",
    "        modify_fingerprint_args(args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95c1f8-964c-4aae-b2d8-9d8ccf57f88a",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37856cce-0864-45a1-99e8-f1e9283470c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "from grover.data.scaler import StandardScaler\n",
    "from grover.util.utils import get_class_sizes, get_data, split_data, get_task_names, get_loss_func\n",
    "from grover.util.utils import load_checkpoint\n",
    "from task.predict import evaluate_predictions, evaluate_predictions_cfm\n",
    "from grover.util.metrics import get_metric_func\n",
    "from grover.util.nn_utils import param_count\n",
    "from task.predict import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bab25ed-2620-49a7-9eb7-0ac3edf17175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup random seed\n",
    "setup(seed=42)\n",
    "# Avoid the pylint warning.\n",
    "a = MolVocab\n",
    "# supress rdkit logger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# Initialize MolVocab\n",
    "mol_vocab = MolVocab\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "logger = create_logger(name='eval', save_dir=args.save_dir, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a15f608-b181-41a7-9b14-331d3b4a0460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading data\n",
      "Number of tasks = 1\n",
      "Number of tasks = 1\n",
      "Splitting data with seed 0\n",
      "Splitting data with seed 0\n",
      "100%|##########| 1577/1577 [00:00<00:00, 6148.33it/s]\n",
      "Total scaffolds = 418 | train scaffolds = 216 | val scaffolds = 102 | test scaffolds = 100\n",
      "Total scaffolds = 418 | train scaffolds = 216 | val scaffolds = 102 | test scaffolds = 100\n",
      "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([0.25888325]), array([197])), (array([0.18830243]), array([701])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([2])), (array([0.]), array([1])), (array([0.]), array([1]))]\n",
      "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([0.25888325]), array([197])), (array([0.18830243]), array([701])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([2])), (array([0.]), array([1])), (array([0.]), array([1]))]\n",
      "Class sizes\n",
      "Class sizes\n",
      "0th class size 0: 83.51%\n",
      "0th class size 0: 83.51%\n",
      "1th class size 0: 13.76%\n",
      "1th class size 0: 13.76%\n",
      "2th class size 0: 2.73%\n",
      "2th class size 0: 2.73%\n",
      "Total size = 1,577 | train size = 1,261 | val size = 157 | test size = 159\n",
      "Total size = 1,577 | train size = 1,261 | val size = 157 | test size = 159\n",
      "Loading model 0 from model/tg423_RS1/iter_16/fold_0/model_0/model.pt\n",
      "Loading model 0 from model/tg423_RS1/iter_16/fold_0/model_0/model.pt\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
      "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
      "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.5.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.5.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.7.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.7.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.7.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.7.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.5.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.5.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.7.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.7.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.7.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.7.bias\".\n",
      "Moving model to cuda\n",
      "Moving model to cuda\n",
      "Number of parameters = 112,654,240\n",
      "Number of parameters = 112,654,240\n"
     ]
    }
   ],
   "source": [
    "if logger is not None:\n",
    "    debug, info = logger.debug, logger.info\n",
    "else:\n",
    "    debug = info = print\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Get data\n",
    "debug('Loading data')\n",
    "args.task_names = get_task_names(args.data_path)\n",
    "data = get_data(path=args.data_path, args=args, logger=logger)\n",
    "args.num_tasks = data.num_tasks()\n",
    "args.features_size = data.features_size()\n",
    "debug(f'Number of tasks = {args.num_tasks}')\n",
    "\n",
    "# Split data\n",
    "debug(f'Splitting data with seed {args.seed}')\n",
    "\n",
    "train_data, val_data, test_data = split_data(data=data,\n",
    "                                             split_type=args.split_type,\n",
    "                                             sizes=[0.8, 0.1, 0.1],\n",
    "                                             seed=args.seed,\n",
    "                                             args=args,\n",
    "                                             logger=logger)\n",
    "\n",
    "if args.dataset_type == 'classification':\n",
    "    class_sizes = get_class_sizes(data, args)\n",
    "    debug('Class sizes')\n",
    "    if args.multi_class : \n",
    "        for i, task_class_sizes in enumerate(class_sizes):\n",
    "            debug(f'{i}th class size '\n",
    "                  f'{\", \".join(f\"{cls}: {size * 100:.2f}%\" for cls, size in enumerate(task_class_sizes))}')\n",
    "\n",
    "    else : \n",
    "        for i, task_class_sizes in enumerate(class_sizes):\n",
    "            debug(f'{args.task_names[i]} '\n",
    "                  f'{\", \".join(f\"{cls}: {size * 100:.2f}%\" for cls, size in enumerate(task_class_sizes))}')\n",
    "\n",
    "if args.features_scaling:\n",
    "    features_scaler = train_data.normalize_features(replace_nan_token=0)\n",
    "    val_data.normalize_features(features_scaler)\n",
    "    test_data.normalize_features(features_scaler)\n",
    "else:\n",
    "    features_scaler = None\n",
    "\n",
    "args.train_data_size = len(train_data)\n",
    "\n",
    "debug(f'Total size = {len(data):,} | '\n",
    "      f'train size = {len(train_data):,} | val size = {len(val_data):,} | test size = {len(test_data):,}')\n",
    "\n",
    "# Initialize scaler  (regression only)\n",
    "scaler = None\n",
    "if args.dataset_type == 'regression':\n",
    "    debug('Fitting scaler')\n",
    "    _, train_targets = train_data.smiles(), train_data.targets()\n",
    "    scaler = StandardScaler().fit(train_targets)\n",
    "    scaled_targets = scaler.transform(train_targets).tolist()\n",
    "    train_data.set_targets(scaled_targets)\n",
    "\n",
    "    val_targets = val_data.targets()\n",
    "    scaled_val_targets = scaler.transform(val_targets).tolist()\n",
    "    val_data.set_targets(scaled_val_targets)\n",
    "\n",
    "metric_func = get_metric_func(metric=args.metric)\n",
    "\n",
    "# Set up test set evaluation\n",
    "test_smiles, test_targets = test_data.smiles(), test_data.targets()\n",
    "sum_test_preds = np.zeros((len(test_smiles), args.num_tasks))\n",
    "\n",
    "# Load/build model\n",
    "if args.checkpoint_paths is not None:\n",
    "    cur_model = args.seed\n",
    "    target_path = []\n",
    "    for path in args.checkpoint_paths:\n",
    "        if \"fold_%d\" % cur_model in path:\n",
    "            target_path = path\n",
    "    debug(f'Loading model {args.seed} from {target_path}')\n",
    "    model = load_checkpoint(target_path, current_args=args, cuda=args.cuda, logger=logger)\n",
    "    # Get loss and metric functions\n",
    "    loss_func = get_loss_func(args, model)\n",
    "\n",
    "debug(f'Number of parameters = {param_count(model):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160d0577-affc-4c03-b5b9-6dabaa9cd560",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, _ = predict(\n",
    "        model=model,\n",
    "        data=test_data,\n",
    "        batch_size=args.batch_size,\n",
    "        loss_func=loss_func,\n",
    "        logger=logger,\n",
    "        shared_dict={},\n",
    "        scaler=scaler,\n",
    "        args=args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9b2f4c7-8574-4a69-8565-abbb0b07e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=test_preds\n",
    "targets=test_targets\n",
    "num_tasks=args.num_tasks\n",
    "dataset_type=args.dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a662fb7f-5e6e-478e-b41c-f8aa964ecc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8670040612295921"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_targets, test_preds, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "668c1217-1312-461c-8759-e98c9119cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e333648c-9435-4881-af93-83f12692f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e8036094-93c7-4c91-b718-71c7219a4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c9f2d28-cd9a-4daa-8887-d382a0aee458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([159, 3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c99593e-2d36-4ce5-a1ac-6afb28f535ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, mean_absolute_error, r2_score, \\\n",
    "    precision_recall_curve, auc, recall_score, confusion_matrix, f1_score, precision_score, classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "012e540f-c8b7-4e84-873d-232f93afa480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([159, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(targets).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "afdb0508-bc94-4798-8749-5f2c3b8c104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_preds = [np.argmax(x) for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8763378c-7dbf-47bb-90ab-68b08b90eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = multilabel_confusion_matrix(targets, hard_preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3c10002a-911a-48a3-8403-fef723370595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tn=[]\n",
    "fp=[]\n",
    "fn=[]\n",
    "tp=[]\n",
    "for i in range(args.multi_class_num):\n",
    "    tn.append(result[4*i])\n",
    "    fp.append(result[4*i+1])\n",
    "    fn.append(result[4*i+2])\n",
    "    tp.append(result[4*i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7e0e80ec-a22f-4768-82c7-0dd4ad8d1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(targets, hard_preds)\n",
    "rec = recall_score(targets, hard_preds, average='macro')\n",
    "prec = precision_score(targets, hard_preds, average='macro')\n",
    "f1s = f1_score(targets, hard_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "566078fe-1dae-4d87-a935-02334d01a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat_multi(targets: List[int], preds: List[float], threshold: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Computes the specificity of a binary prediction task using a given threshold for generating hard predictions.\n",
    "\n",
    "    :param targets: A list of binary targets.\n",
    "    :param preds: A list of prediction probabilities.\n",
    "    :param threshold: The threshold above which a prediction is a 1 and below which (inclusive) a prediction is a 0\n",
    "    :return: The computed specificity.\n",
    "    \"\"\"\n",
    "    hard_preds = [np.argmax(x) for x in preds]\n",
    "    result = multilabel_confusion_matrix(targets, hard_preds).ravel()\n",
    "    tn=[]\n",
    "    fp=[]\n",
    "    fn=[]\n",
    "    tp=[]\n",
    "    for i in range(args.multi_class_num):\n",
    "        tn.append(result[4*i])\n",
    "        fp.append(result[4*i+1])\n",
    "        fn.append(result[4*i+2])\n",
    "        tp.append(result[4*i+3])\n",
    "    acc = accuracy_score(targets, hard_preds)\n",
    "    rec = recall_score(targets, hard_preds, average='macro')\n",
    "    prec = precision_score(targets, hard_preds, average='macro')\n",
    "    f1s = f1_score(targets, hard_preds, average='macro')\n",
    "    return acc, rec, prec, f1s, tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11c2b42a-fc48-4a0d-b5cb-e8363fcbd050",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-53ec43bf5f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mresult_AUC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mresult_AUC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass-multioutput format is not supported"
     ]
    }
   ],
   "source": [
    "# Compute metric\n",
    "result_AUC = []\n",
    "result_ACC = []\n",
    "result_REC = []\n",
    "result_PREC = []\n",
    "result_SPEC = []\n",
    "result_F1 = []\n",
    "result_BA = []\n",
    "result_TP = []\n",
    "result_FP = []\n",
    "result_TN = []\n",
    "result_FN = []\n",
    "for i in range(num_tasks):\n",
    "    # # Skip if all targets or preds are identical, otherwise we'll crash during classification\n",
    "    if dataset_type == 'classification':\n",
    "        nan = False\n",
    "        if all(target == 0 for target in valid_targets[i]) or all(target == 1 for target in valid_targets[i]):\n",
    "            nan = True\n",
    "            # info('Warning: Found a task with targets all 0s or all 1s')\n",
    "        if all(pred == 0 for pred in valid_preds[i]) or all(pred == 1 for pred in valid_preds[i]):\n",
    "            nan = True\n",
    "            # info('Warning: Found a task with predictions all 0s or all 1s')\n",
    "\n",
    "        if nan:\n",
    "            result_AUC.append(float('nan'))\n",
    "            result_ACC.append(float('nan'))\n",
    "            result_REC.append(float('nan'))\n",
    "            result_PREC.append(float('nan'))\n",
    "            result_SPEC.append(float('nan'))\n",
    "            result_F1.append(float('nan'))\n",
    "            result_BA.append(float('nan'))\n",
    "            result_TP.append(float('nan'))\n",
    "            result_FP.append(float('nan'))\n",
    "            result_TN.append(float('nan'))\n",
    "            result_FN.append(float('nan'))\n",
    "            continue\n",
    "\n",
    "    if len(valid_targets[i]) == 0:\n",
    "        continue\n",
    "\n",
    "    result_AUC.append(metric_func(valid_targets[i], valid_preds[i]))\n",
    "    acc, rec, prec, spe, f1s, BA, tp, fp, tn, fn = confusion_mat(valid_targets[i], valid_preds[i])\n",
    "    result_ACC.append(acc)\n",
    "    result_REC.append(rec)\n",
    "    result_PREC.append(prec)\n",
    "    result_SPEC.append(spe)\n",
    "    result_F1.append(f1s)\n",
    "    result_BA.append(BA)\n",
    "    result_TP.append(tp)\n",
    "    result_FP.append(fp)\n",
    "    result_TN.append(tn)\n",
    "    result_FN.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff6586-8b4a-41df-88dd-b254bf2f487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4fbf02f-89d5-4bed-b5ea-e08671a26f77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216a73db-34d7-4162-a760-c9ea4195a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.data import MoleculeDatapoint, MoleculeDataset, StandardScaler\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "from logging import Logger\n",
    "from typing import List, Set, Tuple, Union, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from torch import nn as nn\n",
    "from tqdm import tqdm as core_tqdm\n",
    "\n",
    "from grover.data import MoleculeDatapoint, MoleculeDataset, StandardScaler\n",
    "from grover.model.models import GroverFpGeneration, GroverFinetuneTask, GroverEmbvecGeneration\n",
    "from grover.util.nn_utils import initialize_weights\n",
    "from grover.util.scheduler import NoamLR\n",
    "\n",
    "from grover.topology.mol_tree import *\n",
    "from grover.util.utils import *\n",
    "from task.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f061392a-a870-4b79-931e-2c87d6827791",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup(seed=42)\n",
    "# Avoid the pylint warning.\n",
    "a = MolVocab\n",
    "# supress rdkit logger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "# Initialize MolVocab\n",
    "mol_vocab = MolVocab\n",
    "args = parse_args()\n",
    "logger = create_logger(name='train', save_dir=args.save_dir, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce1d1820-f8d0-4401-9aa1-3602220f09f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Number of tasks = 1\n",
      "Splitting data with seed 0\n",
      "100%|##########| 1019/1019 [00:00<00:00, 4268.18it/s]\n",
      "Total scaffolds = 281 | train scaffolds = 137 | val scaffolds = 69 | test scaffolds = 75\n",
      "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([1.05369128]), array([149])), (array([0.73318386]), array([446])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([1.]), array([1])), (array([2.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([0.]), array([3]))]\n",
      "Class sizes\n",
      "Total size = 1,019 | train size = 815 | val size = 101 | test size = 103\n",
      "Loading model 0 from grover_large.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.44\n",
      "1 : 0.33\n",
      "2 : 0.19\n",
      "3 : 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
      "Moving model to cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if logger is not None:\n",
    "    debug, info = logger.debug, logger.info\n",
    "else:\n",
    "    debug = info = print\n",
    "\n",
    "\n",
    "# pin GPU to local rank.\n",
    "idx = args.gpu\n",
    "if args.gpu is not None:\n",
    "    torch.cuda.set_device(idx)\n",
    "\n",
    "features_scaler, scaler, shared_dict, test_data, train_data, val_data = load_data(args, debug, logger)\n",
    "\n",
    "metric_func = get_metric_func(metric=args.metric)\n",
    "\n",
    "# Set up test set evaluation\n",
    "test_smiles, test_targets = test_data.smiles(), test_data.targets()\n",
    "if args.multi_class:\n",
    "    sum_test_preds = np.zeros((len(test_smiles), args.multi_class_num))\n",
    "else : \n",
    "    sum_test_preds = np.zeros((len(test_smiles), args.num_tasks))\n",
    "    \n",
    "# Train ensemble of models\n",
    "for model_idx in range(args.ensemble_size):\n",
    "    # Tensorboard writer\n",
    "    save_dir = os.path.join(args.save_dir, f'model_{model_idx}')\n",
    "    makedirs(save_dir)\n",
    "\n",
    "    # Load/build model\n",
    "    if args.checkpoint_paths is not None:\n",
    "        if len(args.checkpoint_paths) == 1:\n",
    "            cur_model = 0\n",
    "        else:\n",
    "            cur_model = model_idx\n",
    "        debug(f'Loading model {cur_model} from {args.checkpoint_paths[cur_model]}')\n",
    "        model = load_checkpoint(args.checkpoint_paths[cur_model], current_args=args, logger=logger)\n",
    "    else:\n",
    "        debug(f'Building model {model_idx}')\n",
    "        model = build_model(model_idx=model_idx, args=args)\n",
    "\n",
    "    if args.fine_tune_coff != 1 and args.checkpoint_paths is not None:\n",
    "        debug(\"Fine tune fc layer with different lr\")\n",
    "        initialize_weights(model_idx=model_idx, model=model.ffn, distinct_init=args.distinct_init)\n",
    "\n",
    "    # Get loss and metric functions\n",
    "    loss_func = get_loss_func(args, model)\n",
    "\n",
    "    optimizer = build_optimizer(model, args)\n",
    "    \n",
    "    if args.cuda:\n",
    "        debug('Moving model to cuda')\n",
    "        model = model.cuda()\n",
    "    # Ensure that model is saved in correct location for evaluation if 0 epochs\n",
    "    save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "\n",
    "    # Learning rate schedulers\n",
    "    scheduler = build_lr_scheduler(optimizer, args)\n",
    "\n",
    "    # Bulid data_loader\n",
    "    shuffle = True\n",
    "    mol_collator = MolCollator(shared_dict={}, args=args)\n",
    "    train_data = DataLoader(train_data,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=10,\n",
    "                            collate_fn=mol_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eff561b-0217-49f9-bea6-f07d2379f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "# data.shuffle()\n",
    "\n",
    "loss_sum, iter_count = 0, 0\n",
    "cum_loss_sum, cum_iter_count = 0, 0\n",
    "\n",
    "\n",
    "mol_collator = MolCollator(shared_dict=shared_dict, args=args)\n",
    "\n",
    "num_workers = 4\n",
    "if type(train_data) == DataLoader:\n",
    "    mol_loader = train_data\n",
    "else:\n",
    "    mol_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\n",
    "                        num_workers=num_workers, collate_fn=mol_collator)\n",
    "\n",
    "for _, item in enumerate(mol_loader):\n",
    "    _, batch, features_batch, mask, targets = item\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        mask, targets = mask.cuda(), targets.cuda()\n",
    "    class_weights = torch.ones(targets.shape)\n",
    "\n",
    "    if args.cuda:\n",
    "        class_weights = class_weights.cuda()\n",
    "\n",
    "    # Run model\n",
    "    model.zero_grad()\n",
    "    preds = model(batch, features_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4631cbd6-bb8b-4dcd-b403-5f93ffe264a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3956,  1.0293, -2.0984,  1.2230],\n",
       "        [ 0.3215,  0.4533, -1.3846,  1.3530],\n",
       "        [-0.1450,  0.6166, -2.1879,  1.9209],\n",
       "        [-1.4344,  0.9410, -2.3566,  1.5307],\n",
       "        [ 0.2092,  0.8772, -1.3095,  1.3412],\n",
       "        [ 1.2265,  0.4093, -0.9116,  1.1034],\n",
       "        [-0.2924,  0.8900, -1.0518,  0.4056],\n",
       "        [-0.5966,  0.6896,  0.3310,  1.6052],\n",
       "        [ 0.5463,  0.2386, -1.1186,  0.9212],\n",
       "        [-0.0789,  1.7566, -1.0784,  0.5090],\n",
       "        [-0.1155,  0.1876, -2.4077,  0.7775],\n",
       "        [-0.3261,  0.3426, -2.3022,  0.4938],\n",
       "        [-0.8970,  1.0712, -2.8677,  1.6920],\n",
       "        [ 0.4045,  0.0566, -1.3239,  1.3198],\n",
       "        [-0.0325,  0.9464, -1.9318,  1.3852]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8015778c-6f57-4d63-8fc7-65de04daadc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "310569b4-c528-43d0-a05b-ce569e2d0c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efdb3ae1-235f-4f9a-9bd0-645671aced4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loss1 = pred_loss(preds[0], targets.squeeze().to(torch.long))\n",
    "pred_loss2 = pred_loss(preds[1], targets.squeeze().to(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77b51d77-cc37-4b20-86a0-b81671d24b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3404, 1.5029, 4.4555, 3.4516, 1.8360, 0.8935, 1.9080, 1.4981, 1.5254,\n",
       "        3.2441, 1.2857, 3.6551, 3.0733, 1.8252, 1.0929], device='cuda:0',\n",
       "       grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42c37248-4cd1-400a-962a-321f9e644103",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_loss = nn.MSELoss(reduction='none')\n",
    "dist = dist_loss(preds[0], preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d406bdc-f563-4d15-9be8-cd5273b214dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pred_loss1 + pred_loss2 + dist.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0c8e7f7-7808-4ce7-b347-e76f1daabe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b35314e-166e-4d07-9fd7-4eeb20665e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fd2f21b-34cb-48eb-ad4d-b9217f64c986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "326c7c2c-186d-4803-a972-f2d0625789c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input1 = torch.randn(3, 5, requires_grad=True)\n",
    "target1 = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output1 = loss(input1, target1)\n",
    "output1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe466ebb-5ea5-401e-883d-96d3f566ecff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee4dd3bf-252f-4ec2-bfff-5f138e28c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class probabilities\n",
    "input2 = torch.randn(3, 5, requires_grad=True)\n",
    "target2 = torch.randn(3, 5).softmax(dim=1)\n",
    "output2 = loss(input2, target2)\n",
    "output2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e441b3b0-fdb3-44ec-a711-79678213c7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "079ac9ba-07ae-4deb-b2dd-bca37762b909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c258a4c-5d84-4e8a-91c1-6441393e199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 loss_train: 69.688618 loss_val: 26.584674 auc_val: 0.5686 cur_lr: 0.00059 t_time: 4.8184s v_time: 0.4729s\n",
      "Epoch: 0001 loss_train: 65.542918 loss_val: 26.671691 auc_val: 0.5558 cur_lr: 0.00076 t_time: 4.2996s v_time: 0.4478s\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "best_score = float('inf') if args.minimize_score else -float('inf')\n",
    "best_epoch, n_iter = 0, 0\n",
    "min_val_loss = float('inf')\n",
    "for epoch in range(2):\n",
    "    s_time = time.time()\n",
    "    n_iter, train_loss = train(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        data=train_data,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        args=args,\n",
    "        n_iter=n_iter,\n",
    "        shared_dict=shared_dict,\n",
    "        logger=logger\n",
    "    )\n",
    "    t_time = time.time() - s_time\n",
    "    s_time = time.time()\n",
    "    val_scores, val_loss = evaluate(\n",
    "        model=model,\n",
    "        data=val_data,\n",
    "        loss_func=loss_func,\n",
    "        num_tasks=args.num_tasks,\n",
    "        metric_func=metric_func,\n",
    "        batch_size=args.batch_size,\n",
    "        dataset_type=args.dataset_type,\n",
    "        scaler=scaler,\n",
    "        shared_dict=shared_dict,\n",
    "        logger=logger,\n",
    "        args=args\n",
    "    )\n",
    "    v_time = time.time() - s_time\n",
    "\n",
    "\n",
    "    # Average validation score\n",
    "    avg_val_score = np.nanmean(val_scores)\n",
    "\n",
    "\n",
    "    # Logged after lr step\n",
    "    if isinstance(scheduler, ExponentialLR):\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.show_individual_scores:\n",
    "        # Individual validation scores\n",
    "        for task_name, val_score in zip(args.task_names, val_scores):\n",
    "            debug(f'Validation {task_name} {args.metric} = {val_score:.6f}')\n",
    "    print('Epoch: {:04d}'.format(epoch),\n",
    "          'loss_train: {:.6f}'.format(train_loss),\n",
    "          'loss_val: {:.6f}'.format(val_loss),\n",
    "          f'{args.metric}_val: {avg_val_score:.4f}',\n",
    "          # 'auc_val: {:.4f}'.format(avg_val_score),\n",
    "          'cur_lr: {:.5f}'.format(scheduler.get_lr()[-1]),\n",
    "          't_time: {:.4f}s'.format(t_time),\n",
    "          'v_time: {:.4f}s'.format(v_time))\n",
    "\n",
    "    if args.tensorboard:\n",
    "        writer.add_scalar('loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('loss/val', val_loss, epoch)\n",
    "        writer.add_scalar(f'{args.metric}_val', avg_val_score, epoch)\n",
    "\n",
    "    if args.wandb :         \n",
    "        wandb.log({\"val_loss\" : val_loss, \"val_metrics\" : val_scores})\n",
    "\n",
    "\n",
    "    # Save model checkpoint if improved validation score\n",
    "    if args.select_by_loss:\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss, best_epoch = val_loss, epoch\n",
    "            save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "    else:\n",
    "        if args.minimize_score and avg_val_score < best_score or \\\n",
    "                not args.minimize_score and avg_val_score > best_score:\n",
    "            best_score, best_epoch = avg_val_score, epoch\n",
    "            save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "\n",
    "    if epoch - best_epoch > args.early_stop_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00788ed8-f40b-4a75-a2d6-60fce1de6276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 0 best val loss = 26.584674 on epoch 0\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
      "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
      "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
      "Moving model to cuda\n"
     ]
    }
   ],
   "source": [
    "ensemble_scores = 0.0\n",
    "\n",
    "# Evaluate on test set using model with best validation score\n",
    "if args.select_by_loss:\n",
    "    info(f'Model {model_idx} best val loss = {min_val_loss:.6f} on epoch {best_epoch}')\n",
    "else:\n",
    "    info(f'Model {model_idx} best validation {args.metric} = {best_score:.6f} on epoch {best_epoch}')\n",
    "model = load_checkpoint(os.path.join(save_dir, 'model.pt'), cuda=args.cuda, logger=logger)\n",
    "\n",
    "test_preds, _ = predict(\n",
    "    model=model,\n",
    "    data=test_data,\n",
    "    loss_func=loss_func,\n",
    "    batch_size=args.batch_size,\n",
    "    logger=logger,\n",
    "    shared_dict=shared_dict,\n",
    "    scaler=scaler,\n",
    "    args=args\n",
    ")\n",
    "\n",
    "test_scores = evaluate_predictions(\n",
    "    preds=test_preds,\n",
    "    targets=test_targets,\n",
    "    num_tasks=args.num_tasks,\n",
    "    metric_func=metric_func,\n",
    "    dataset_type=args.dataset_type,\n",
    "    arg=args,\n",
    "    logger=logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8732c426-5b25-459c-a440-1e6f8ca1277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 0 test auc = 0.600428\n"
     ]
    }
   ],
   "source": [
    "if len(test_preds) != 0:\n",
    "    sum_test_preds += np.array(test_preds, dtype=float)\n",
    "\n",
    "# Average test score\n",
    "avg_test_score = np.nanmean(test_scores)\n",
    "info(f'Model {model_idx} test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "if args.show_individual_scores:\n",
    "    # Individual test scores\n",
    "    for task_name, test_score in zip(args.task_names, test_scores):\n",
    "        info(f'Model {model_idx} test {task_name} {args.metric} = {test_score:.6f}')\n",
    "\n",
    "# Evaluate ensemble on test set\n",
    "avg_test_preds = (sum_test_preds / args.ensemble_size).tolist()\n",
    "\n",
    "ensemble_scores = evaluate_predictions(\n",
    "    preds=avg_test_preds,\n",
    "    targets=test_targets,\n",
    "    num_tasks=args.num_tasks,\n",
    "    metric_func=metric_func,\n",
    "    dataset_type=args.dataset_type,\n",
    "    arg=args,\n",
    "    logger=logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7eb3fd26-0466-43e9-9b6f-c5381e5ff9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [['preds'] * args.num_tasks + ['targets'] * args.num_tasks, args.task_names * 2]\n",
    "ind = pd.MultiIndex.from_tuples(list(zip(*ind)))\n",
    "if args.multi_class:\n",
    "    data = np.concatenate([np.array([np.argmax(x) for x in avg_test_preds]).reshape(-1,1), np.array(test_targets)], 1)\n",
    "else:\n",
    "    data = np.concatenate([np.array(avg_test_preds), np.array(test_targets)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a249d302-1516-4b1f-aa0a-d75027c6e9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 2.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 0.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 2.],\n",
       "       [1., 0.],\n",
       "       [2., 1.],\n",
       "       [1., 0.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [1., 1.],\n",
       "       [2., 0.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 2.],\n",
       "       [1., 0.],\n",
       "       [2., 0.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 0.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 0.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 0.],\n",
       "       [2., 2.],\n",
       "       [1., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 0.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 2.],\n",
       "       [1., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [1., 1.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 1.],\n",
       "       [2., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69d41b9e-dab9-49ce-88d6-d67de80d7d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_targets).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7713c09-7b39-4ea1-a79b-42fcec108ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame(data, index=test_smiles, columns=ind)\n",
    "test_result.to_csv(os.path.join(args.save_dir, 'test_result.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ab826f9-ef29-4661-a9d4-83d317a9b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a04300b1-dda3-4cb4-98f6-453958214b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a925f44-3d18-4cb1-be6b-f3206617d2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 1., 2., 0., 1., 2., 1., 1., 2., 1., 2., 1., 2., 1., 2., 0., 1.,\n",
       "        1., 1., 2., 1., 2., 2., 2., 1., 2., 2., 2., 2., 0., 1.],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.squeeze().to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67a68efb-ac34-401a-bd05-9fbd11243c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3708, 2.2572, 0.8642, 2.5354, 1.3634, 1.9989, 0.2592, 0.3828, 0.8414,\n",
       "        1.5310, 1.2287, 2.2028, 0.2887, 0.4733, 1.0140, 2.8086, 1.5943, 0.8936,\n",
       "        0.9096, 0.7167, 1.6020, 1.5150, 0.4911, 1.2378, 2.2452, 0.7885, 1.2347,\n",
       "        2.8424, 3.4238, 0.9983, 1.2706, 0.4381], device='cuda:0',\n",
       "       grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_loss(preds[0], targets.squeeze().to(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "654550bc-8d5a-4b6f-9b0e-31f38dcf8368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(82, device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.(torch.softmax(torch.tensor(preds[0]),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5482528-9dfa-4505-aa66-fc67a245f497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
