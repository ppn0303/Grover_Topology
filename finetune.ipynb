{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c2b7b5-8afc-4b6a-accc-dc430d937977",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 위에서부터 내려가는 순이지만, 맨 밑이 실제 실행 코드이다.\n",
    "- 무척 복잡해질 수 있다... 그러나 실행 코드의 흐름의 순서에 따라 한 것이고, 부분부분 나누지 않아서 심히 복잡하다 ㅜㅜ 이해하자... 나는 처음이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cffc7840-4d3e-4189-9a2b-1faa6abd226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import RDLogger\n",
    "\n",
    "#add for gridsearch\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "from grover.util.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32866d-73a2-4536-83b4-e99db87d1e46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. MolVocab\n",
    "- 논문에서 말하는 Contextual Property에 대한 내용을 담은 사전이다. 원자, 결합에 대한 정보를 담는 클래스다. 여기서는 일단 선포만 해둔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285e59b7-dd19-46af-8a01-d38cd518b575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def atom_to_vocab(mol, atom):\n",
    "    \"\"\"\n",
    "    Convert atom to vocabulary. The convention is based on atom type and bond type.\n",
    "    :param mol: the molecular.\n",
    "    :param atom: the target atom.\n",
    "    :return: the generated atom vocabulary with its contexts.\n",
    "    \"\"\"\n",
    "    nei = Counter()\n",
    "    for a in atom.GetNeighbors():\n",
    "        bond = mol.GetBondBetweenAtoms(atom.GetIdx(), a.GetIdx())\n",
    "        nei[str(a.GetSymbol()) + \"-\" + str(bond.GetBondType())] += 1\n",
    "    keys = nei.keys()\n",
    "    keys = list(keys)\n",
    "    keys.sort()\n",
    "    output = atom.GetSymbol()\n",
    "    for k in keys:\n",
    "        output = \"%s_%s%d\" % (output, k, nei[k])\n",
    "\n",
    "    # The generated atom_vocab is too long?\n",
    "    return output\n",
    "\n",
    "\n",
    "def bond_to_vocab(mol, bond):\n",
    "    \"\"\"\n",
    "    Convert bond to vocabulary. The convention is based on atom type and bond type.\n",
    "    Considering one-hop neighbor atoms\n",
    "    :param mol: the molecular.\n",
    "    :param atom: the target atom.\n",
    "    :return: the generated bond vocabulary with its contexts.\n",
    "    \"\"\"\n",
    "    nei = Counter()\n",
    "    two_neighbors = (bond.GetBeginAtom(), bond.GetEndAtom())\n",
    "    two_indices = [a.GetIdx() for a in two_neighbors]\n",
    "    for nei_atom in two_neighbors:\n",
    "        for a in nei_atom.GetNeighbors():\n",
    "            a_idx = a.GetIdx()\n",
    "            if a_idx in two_indices:\n",
    "                continue\n",
    "            tmp_bond = mol.GetBondBetweenAtoms(nei_atom.GetIdx(), a_idx)\n",
    "            nei[str(nei_atom.GetSymbol()) + '-' + get_bond_feature_name(tmp_bond)] += 1\n",
    "    keys = list(nei.keys())\n",
    "    keys.sort()\n",
    "    output = get_bond_feature_name(bond)\n",
    "    for k in keys:\n",
    "        output = \"%s_%s%d\" % (output, k, nei[k])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce25b8a-be45-45db-99a8-c148187f997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The contextual property.\n",
    "\"\"\"\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "\n",
    "#from grover.data.task_labels import atom_to_vocab\n",
    "#from grover.data.task_labels import bond_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20af1ab6-2e73-44d8-95f0-3c1876a7e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchVocab(object):\n",
    "    \"\"\"\n",
    "    Defines the vocabulary for atoms/bonds in molecular.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=('<pad>', '<other>'), vocab_type='atom'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param counter:\n",
    "        :param max_size:\n",
    "        :param min_freq:\n",
    "        :param specials:\n",
    "        :param vocab_type: 'atom': atom atom_vocab; 'bond': bond atom_vocab.\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "        if vocab_type in ('atom', 'bond'):\n",
    "            self.vocab_type = vocab_type\n",
    "        else:\n",
    "            raise ValueError('Wrong input for vocab_type!')\n",
    "        self.itos = list(specials)\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "        self.other_index = 1\n",
    "        self.pad_index = 0\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        # if self.vectors != other.vectors:\n",
    "        #    return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "                self.freqs[w] = 0\n",
    "            self.freqs[w] += v.freqs[w]\n",
    "\n",
    "    def mol_to_seq(self, mol, with_len=False):\n",
    "        mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "        if self.vocab_type == 'atom':\n",
    "            seq = [self.stoi.get(atom_to_vocab(mol, atom), self.other_index) for i, atom in enumerate(mol.GetAtoms())]\n",
    "        else:\n",
    "            seq = [self.stoi.get(bond_to_vocab(mol, bond), self.other_index) for i, bond in enumerate(mol.GetBonds())]\n",
    "        return (seq, len(seq)) if with_len else seq\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf25bd8-fe2a-4416-9d9f-8e364af4985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolVocab(TorchVocab):\n",
    "    def __init__(self, smiles, max_size=None, min_freq=1, vocab_type='atom'):\n",
    "        if vocab_type in ('atom', 'bond'):\n",
    "            self.vocab_type = vocab_type\n",
    "        else:\n",
    "            raise ValueError('Wrong input for vocab_type!')\n",
    "\n",
    "        print(\"Building %s vocab from smiles: %d\" % (self.vocab_type, len(smiles)))\n",
    "        counter = Counter()\n",
    "\n",
    "        for smi in tqdm.tqdm(smiles):\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if self.vocab_type == 'atom':\n",
    "                for _, atom in enumerate(mol.GetAtoms()):\n",
    "                    v = atom_to_vocab(mol, atom)\n",
    "                    counter[v] += 1\n",
    "            else:\n",
    "                for _, bond in enumerate(mol.GetBonds()):\n",
    "                    v = bond_to_vocab(mol, bond)\n",
    "                    counter[v] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq, vocab_type=vocab_type)\n",
    "\n",
    "    def __init__(self, file_path, max_size=None, min_freq=1, num_workers=1, total_lines=None, vocab_type='atom'):\n",
    "        if vocab_type in ('atom', 'bond'):\n",
    "            self.vocab_type = vocab_type\n",
    "        else:\n",
    "            raise ValueError('Wrong input for vocab_type!')\n",
    "        print(\"Building %s vocab from file: %s\" % (self.vocab_type, file_path))\n",
    "\n",
    "        from rdkit import RDLogger\n",
    "        lg = RDLogger.logger()\n",
    "        lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "        if total_lines is None:\n",
    "            def file_len(fname):\n",
    "                f_len = 0\n",
    "                with open(fname) as f:\n",
    "                    for f_len, _ in enumerate(f):\n",
    "                        pass\n",
    "                return f_len + 1\n",
    "\n",
    "            total_lines = file_len(file_path)\n",
    "\n",
    "        counter = Counter()\n",
    "        pbar = tqdm.tqdm(total=total_lines)\n",
    "        pool = Pool(num_workers)\n",
    "        res = []\n",
    "        batch = 50000\n",
    "        callback = lambda a: pbar.update(batch)\n",
    "        for i in range(int(total_lines / batch + 1)):\n",
    "            start = int(batch * i)\n",
    "            end = min(total_lines, batch * (i + 1))\n",
    "            # print(\"Start: %d, End: %d\"%(start, end))\n",
    "            res.append(pool.apply_async(MolVocab.read_smiles_from_file,\n",
    "                                        args=(file_path, start, end, vocab_type,),\n",
    "                                        callback=callback))\n",
    "            # read_smiles_from_file(lock, file_path, start, end)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        for r in res:\n",
    "            sub_counter = r.get()\n",
    "            for k in sub_counter:\n",
    "                if k not in counter:\n",
    "                    counter[k] = 0\n",
    "                counter[k] += sub_counter[k]\n",
    "        # print(counter)\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq, vocab_type=vocab_type)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_smiles_from_file(file_path, start, end, vocab_type):\n",
    "        # print(\"start\")\n",
    "        smiles = open(file_path, \"r\")\n",
    "        smiles.readline()\n",
    "        sub_counter = Counter()\n",
    "        for i, smi in enumerate(smiles):\n",
    "            if i < start:\n",
    "                continue\n",
    "            if i >= end:\n",
    "                break\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if vocab_type == 'atom':\n",
    "                for atom in mol.GetAtoms():\n",
    "                    v = atom_to_vocab(mol, atom)\n",
    "                    sub_counter[v] += 1\n",
    "            else:\n",
    "                for bond in mol.GetBonds():\n",
    "                    v = bond_to_vocab(mol, bond)\n",
    "                    sub_counter[v] += 1\n",
    "        # print(\"end\")\n",
    "        return sub_counter\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'MolVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0bb38-9a56-4979-87a5-db1ccc112ebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. parse_args()\n",
    "- argument들에 대한 정보를 담은 코드들이다. 이건 나중에 모델 세부 구조에 미치는 영향을 분석할때 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d160f69-3147-4527-a144-f7cf3c89ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.parsing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aab4d301-d40f-4b32-9687-5389687b1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args() -> Namespace:\n",
    "    \"\"\"\n",
    "    Parses arguments for training and testing (includes modifying/validating arguments).\n",
    "\n",
    "    :return: A Namespace containing the parsed, modified, and validated args.\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "                                      dest=\"parser_name\",\n",
    "                                      help=\"Subcommands for fintune, prediction, and fingerprint.\")\n",
    "    parser_finetune = subparser.add_parser('finetune', help=\"Fine tune the pre-trained model.\")\n",
    "    add_finetune_args(parser_finetune)\n",
    "    parser_eval = subparser.add_parser('eval', help=\"Evaluate the results of the pre-trained model.\")\n",
    "    add_finetune_args(parser_eval)\n",
    "    parser_predict = subparser.add_parser('predict', help=\"Predict results from fine tuned model.\")\n",
    "    add_predict_args(parser_predict)\n",
    "    #parser_fp = subparser.add_parser('fingerprint', help=\"Get the fingerprints of SMILES.\")\n",
    "    #add_fingerprint_args(parser_fp)\n",
    "    parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "    add_pretrain_args(parser_pretrain)\n",
    "\n",
    "    args = parser.parse_args(['finetune','--data_path', 'data/tox21.csv', '--features_path', 'data/tox21.npz', '--save_dir', 'model/CodeStudy/', '--checkpoint_path', 'model/zinc10M_0_75.pt', '--split_type', 'scaffold_balanced','--ffn_hidden_size','1200'])\n",
    "\n",
    "    if args.parser_name == 'finetune' or args.parser_name == 'eval':\n",
    "        modify_train_args(args)\n",
    "    elif args.parser_name == \"pretrain\":\n",
    "        modify_pretrain_args(args)\n",
    "    elif args.parser_name == 'predict':\n",
    "        modify_predict_args(args)\n",
    "    #elif args.parser_name == 'fingerprint':\n",
    "#        modify_fingerprint_args(args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d214088-c380-4eeb-b77e-7c04552be41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "001bb89d-80c9-4fd1-bab6-0a8927502d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(activation='ReLU', attn_hidden=128, attn_out=4, batch_size=32, bond_drop_rate=0, checkpoint_dir=None, checkpoint_path='model/zinc10M_0_75.pt', checkpoint_paths=['model/zinc10M_0_75.pt'], confusionmatrix=False, crossval_index_dir=None, crossval_index_file=None, cuda=True, data_path='data/tox21.csv', dataset_type='classification', dist_coff=0.1, distinct_init=False, dropout=0.0, early_stop_epoch=1000, embedding_output_type='atom', enbl_multi_gpu=False, ensemble_size=1, epochs=30, features_generator=None, features_only=False, features_path=['data/tox21.npz'], features_scaling=True, ffn_hidden_size=1200, ffn_last_size=None, ffn_mid_size=None, ffn_num_layers=2, final_lr=0.0001, fine_tune_coff=1, fingerprint=False, folds_file=None, gpu=0, gridsearch=False, init_lr=0.0001, max_data_size=None, max_lr=0.001, metric='auc', minimize_score=False, n_iters=1, no_cache=True, num_folds=1, num_lrs=1, parser_name='finetune', randomsearch=False, save_dir='model/CodeStudy/', save_smiles_splits=False, seed=0, select_by_loss=False, self_attention=False, separate_test_features_path=None, separate_test_path=None, separate_val_features_path=None, separate_val_path=None, show_individual_scores=False, smote=False, smote_rate=1, split_sizes=[0.8, 0.1, 0.1], split_type='scaffold_balanced', tensorboard=False, test_fold_index=None, use_compound_names=False, use_input_features=['data/tox21.npz'], val_fold_index=None, wandb=False, wandb_name='finetune', warmup_epochs=2.0, weight_decay=0.0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205c7e4-3ca4-4f6b-a53d-c4571a1ad40f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. create_logger()\n",
    "- verbose는 세세한 정보까지 다 저장하는 로그, quiet는 꼭 필요한 것만 저장하게끔 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2113d10f-173a-463c-bdea-784e6231e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6158fb65-fe6f-40ec-8c3e-c9b9a2692ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(name: str, save_dir: str = None, quiet: bool = False) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Creates a logger with a stream handler and two file handlers.\n",
    "\n",
    "    The stream handler prints to the screen depending on the value of `quiet`.\n",
    "    One file handler (verbose.log) saves all logs, the other (quiet.log) only saves important info.\n",
    "\n",
    "    :param name: The name of the logger.\n",
    "    :param save_dir: The directory in which to save the logs.\n",
    "    :param quiet: Whether the stream handler should be quiet (i.e. print only important info).\n",
    "    :return: The logger.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.propagate = False\n",
    "\n",
    "    # Set logger depending on desired verbosity\n",
    "    ch = logging.StreamHandler()\n",
    "    if quiet:\n",
    "        ch.setLevel(logging.INFO)\n",
    "    else:\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    if save_dir is not None:\n",
    "        makedirs(save_dir)\n",
    "        fh_v = logging.FileHandler(os.path.join(save_dir, 'verbose.log'))\n",
    "        fh_v.setLevel(logging.DEBUG)\n",
    "        fh_q = logging.FileHandler(os.path.join(save_dir, 'quiet.log'))\n",
    "        fh_q.setLevel(logging.INFO)\n",
    "\n",
    "        logger.addHandler(fh_v)\n",
    "        logger.addHandler(fh_q)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "869904b7-d2bb-4f9a-93ef-618b05f7a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = RDLogger.logger()\n",
    "logger = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18ac12-e7fe-4795-ada3-55b99edf557c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **_4. cross_validate()_**  (핵심!!!)\n",
    "- 우리가 아는 fit()과 같다고 보면 된다.\n",
    "- 교차검증이라고 이름은 되어있고, 설명에서 k-fold cross validation이라고 하지만,,, 실제로 우리가 아는 그것과는 다르다.\n",
    "- 그냥 seed를 다르게해서 여러번 실험을 돌릴 뿐이다. 여기서train으로 넘어가는 코드와 학습 결과를 종합하고, 출력해주는 역할을 한다.\n",
    "- 그러므로 gridsearch, randomsearch, confusionmatrix 모두 여기서 처음 수정을 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f45cbb2a-01fb-4459-8284-8ba30a5504ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe478932-768a-41d3-bc5d-ef2455120eae",
   "metadata": {},
   "source": [
    "from grover.util.utils import get_task_names\n",
    "from grover.util.utils import makedirs\n",
    "from task.run_evaluation import run_evaluation, run_evaluation_cfm\n",
    "from task.train import run_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aebb6f9-e096-4d28-9c85-0c06986a44ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4-1. get_task_names()\n",
    "- csv파일로부터 task의 이름들을 불러온다. 통상 1번째 행의 열들의 이름 중 2번째부터 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85967db0-9510-4a41-a629-ce3150a3ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea8afe65-8310-402d-82ed-f39a372a7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns the header of a data CSV file.\n",
    "\n",
    "    :param path: Path to a CSV file.\n",
    "    :return: A list of strings containing the strings in the comma-separated header.\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        header = next(csv.reader(f))\n",
    "\n",
    "    return header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a29564c1-0a5b-48d5-9be7-eebc9cd9d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_names(path: str, use_compound_names: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Gets the task names from a data CSV file.\n",
    "\n",
    "    :param path: Path to a CSV file.\n",
    "    :param use_compound_names: Whether file has compound names in addition to smiles strings.\n",
    "    :return: A list of task names.\n",
    "    \"\"\"\n",
    "    index = 2 if use_compound_names else 1\n",
    "    task_names = get_header(path)[index:]\n",
    "\n",
    "    return task_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3af91c-d429-40fe-a16c-c7e0d4c1375d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4-2. makedirs()\n",
    "- 왜 이걸 굳이 함수화 시켰을까... 기존 파일이 있으면 그 이름을 받고, 아니면 새로 만들라는 함수다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75ebab08-efda-4b98-a53f-7a2390952b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path: str, isfile: bool = False):\n",
    "    \"\"\"\n",
    "    Creates a directory given a path to either a directory or file.\n",
    "\n",
    "    If a directory is provided, creates that directory. If a file is provided (i.e. isfiled == True),\n",
    "    creates the parent directory for that file.\n",
    "\n",
    "    :param path: Path to a directory or file.\n",
    "    :param isfile: Whether the provided path is a directory or file.\n",
    "    \"\"\"\n",
    "    if isfile:\n",
    "        path = os.path.dirname(path)\n",
    "    if path != '':\n",
    "        os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144650e-85da-487e-b8fc-c8e02396325c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4-3. **_run_training_** 핵심!!\n",
    "- 이게 가장 중요한 코드겠지. 훈련으로 넘어가는 코드다\n",
    "- 데이터를 불러들이고\n",
    "- metric함수를 선정하고\n",
    "- 체크포인트가 있으면 부르고\n",
    "- 각 레이어별 학습률을 다르게 할 수도 있고(이건 뭐지?)\n",
    "- 손실함수, optimizer, 학습률 스케쥴러, MolCollator와 연결된 data_loader를 생성\n",
    "- 학습을 진행, (손실함수 계산, 역전파, 최적화, 학습률 갱신)\n",
    "- 결과에 따라 최고 성능일시 저장, 얼리스탑 여부 확인\n",
    "- 최고 성능의 결과 출력 후 test set으로 최종 검증\n",
    "- 예측 결과, 저장, 최종 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8aa63f29-07b9-4333-a5f0-4f5c173c75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5756f6d3-3a48-4e5c-ad94-799847c24ae1",
   "metadata": {},
   "source": [
    "from grover.data import MolCollator\n",
    "from grover.data import StandardScaler\n",
    "from grover.util.metrics import get_metric_func\n",
    "from grover.util.nn_utils import initialize_weights, param_count\n",
    "from grover.util.scheduler import NoamLR\n",
    "from grover.util.utils import build_optimizer, build_lr_scheduler, makedirs, load_checkpoint, get_loss_func, \\\n",
    "    save_checkpoint, build_model\n",
    "from grover.util.utils import get_class_sizes, get_data, split_data, get_task_names\n",
    "from task.predict import predict, evaluate, evaluate_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ec38b-db22-4f7d-8ae5-aae4ee16f9c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4-3-1. load_data()\n",
    "- get_data로 데이터셋 전체에 대한 MoleculeDataset을 생성한다. 여기엔 Smiles, Feature와 target을 포함한다. 그리고 이에 대한 정보들을 변수로 설정한다.\n",
    "- shared_dict을 만든다. 근데 단 한순간도 이를 써먹거나 내용이 추가되지는 않는다. 그냥 다른 task와 공통으로 사용되어야해서 추가되는 것 같다...\n",
    "- get_data로 얻은 전체 데이터셋을 split한다.\n",
    "- features_scaling이 필요시 스케일링을 한다.(기본은 미실시)\n",
    "- (추가) smote를 시행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21a068-1235-4b63-9308-03c435ab384a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4-3-1-1. get_data()\n",
    "- load_features() : feature를 사전에 만든게 있을경우 불러온다. -> load_features() 아니면 None으로 할당\n",
    "- Load data : csv를 읽어서 smiles를 받는다\n",
    "- 분자들을 배치단위의 MoleculeDataset을 만들기 위해 각 분자들은 MoleculeDatapoint로 만들고, 이때 csv파일을 line으로, 불러들인 각 분자의 feature를 feature로 넣는다.\n",
    "- 여기서 나온 결과물은 데이터셋에 대한 총 MoleculeDataset이며, feature는 200차원이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8bac0-984a-45b3-8a2a-f0fb96f5c4d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### MoluculeDataset()과 Datapoint()\n",
    "- 데이터포인트는 단일 분자의 feature와 target을 담고 있다. feature는 우리가 사전에 만든 npz파일에서부터 불러들이는 200차원의 feature이며, target, conpound_name이 포함된다. 그리고 사전에 안만든경우 즉석에서 만들거나 추가 feature를 추출할 수 있으나 기본 코드는 미적용(False)다.\n",
    "- moleculeDataset은 앞의 데이터포인트들을 배치단위로 묶어서 갖고 있으며, smiles 또한 포함한다. 필요시 standardscaler가 가능하다(이미 RDKit 2D_normalized가 되어있기 때문에 안쓴다.)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a12dafc-ef82-45e4-9348-0485794b7dc1",
   "metadata": {},
   "source": [
    "아래는 해당 코드\n",
    "data = MoleculeDataset([\n",
    "            MoleculeDatapoint(\n",
    "                line=line,\n",
    "                args=args,\n",
    "                features=features_data[i] if features_data is not None else None,\n",
    "                use_compound_names=use_compound_names\n",
    "            ) for i, line in tqdm(enumerate(lines), total=len(lines), disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "103a3d2e-8301-4fb2-a067-c0072d72ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "856e914e-a434-46e7-8a95-125983be3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfe7f72a-1030-4fd2-b761-9ae973977df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"A StandardScaler normalizes a dataset.\n",
    "\n",
    "    When fit on a dataset, the StandardScaler learns the mean and standard deviation across the 0th axis.\n",
    "    When transforming a dataset, the StandardScaler subtracts the means and divides by the standard deviations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, means: np.ndarray = None, stds: np.ndarray = None, replace_nan_token: Any = None):\n",
    "        \"\"\"\n",
    "        Initialize StandardScaler, optionally with means and standard deviations precomputed.\n",
    "\n",
    "        :param means: An optional 1D numpy array of precomputed means.\n",
    "        :param stds: An optional 1D numpy array of precomputed standard deviations.\n",
    "        :param replace_nan_token: The token to use in place of nans.\n",
    "        \"\"\"\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "        self.replace_nan_token = replace_nan_token\n",
    "\n",
    "    def fit(self, X: List[List[float]]) -> 'StandardScaler':\n",
    "        \"\"\"\n",
    "        Learns means and standard deviations across the 0th axis.\n",
    "\n",
    "        :param X: A list of lists of floats.\n",
    "        :return: The fitted StandardScaler.\n",
    "        \"\"\"\n",
    "        X = np.array(X).astype(float)\n",
    "        self.means = np.nanmean(X, axis=0)\n",
    "        self.stds = np.nanstd(X, axis=0)\n",
    "        self.means = np.where(np.isnan(self.means), np.zeros(self.means.shape), self.means)\n",
    "        self.stds = np.where(np.isnan(self.stds), np.ones(self.stds.shape), self.stds)\n",
    "        self.stds = np.where(self.stds == 0, np.ones(self.stds.shape), self.stds)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: List[List[float]]):\n",
    "        \"\"\"\n",
    "        Transforms the data by subtracting the means and dividing by the standard deviations.\n",
    "\n",
    "        :param X: A list of lists of floats.\n",
    "        :return: The transformed data.\n",
    "        \"\"\"\n",
    "        X = np.array(X).astype(float)\n",
    "        transformed_with_nan = (X - self.means) / self.stds\n",
    "        transformed_with_none = np.where(np.isnan(transformed_with_nan), self.replace_nan_token, transformed_with_nan)\n",
    "\n",
    "        return transformed_with_none\n",
    "\n",
    "    def inverse_transform(self, X: List[List[float]]):\n",
    "        \"\"\"\n",
    "        Performs the inverse transformation by multiplying by the standard deviations and adding the means.\n",
    "\n",
    "        :param X: A list of lists of floats.\n",
    "        :return: The inverse transformed data.\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray) or isinstance(X, list):\n",
    "            X = np.array(X).astype(float)\n",
    "            transformed_with_nan = X * self.stds + self.means\n",
    "            transformed_with_none = np.where(np.isnan(transformed_with_nan),\n",
    "                                             self.replace_nan_token, transformed_with_nan)\n",
    "        return transformed_with_none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a23b361-220d-4213-8077-dffcdf1b6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDatapoint:\n",
    "    \"\"\"A MoleculeDatapoint contains a single molecule and its associated features and targets.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 line: List[str],\n",
    "                 args: Namespace = None,\n",
    "                 features: np.ndarray = None,\n",
    "                 use_compound_names: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes a MoleculeDatapoint, which contains a single molecule.\n",
    "\n",
    "        :param line: A list of strings generated by separating a line in a data CSV file by comma.\n",
    "        :param args: Arguments.\n",
    "        :param features: A numpy array containing additional features (ex. Morgan fingerprint).\n",
    "        :param use_compound_names: Whether the data CSV includes the compound name on each line.\n",
    "        \"\"\"\n",
    "        self.features_generator = None   # 기본이 None맞고, csv파일로부터 feature 추출할때는 features_generator를 지정한다.\n",
    "        self.args = None\n",
    "        if args is not None:\n",
    "            if hasattr(args, \"features_generator\"):\n",
    "                self.features_generator = args.features_generator\n",
    "            self.args = args\n",
    "\n",
    "        if features is not None and self.features_generator is not None:\n",
    "            raise ValueError('Currently cannot provide both loaded features and a features generator.')\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "        if use_compound_names:\n",
    "            self.compound_name = line[0]  # str\n",
    "            line = line[1:]\n",
    "        else:\n",
    "            self.compound_name = None\n",
    "\n",
    "        self.smiles = line[0]  # str\n",
    "\n",
    "\n",
    "        # Generate additional features if given a generator -> 즉 평소에는 안쓴다는 것.\n",
    "        if self.features_generator is not None:\n",
    "            self.features = []\n",
    "            mol = Chem.MolFromSmiles(self.smiles)\n",
    "            for fg in self.features_generator:\n",
    "                features_generator = get_features_generator(fg)\n",
    "                if mol is not None and mol.GetNumHeavyAtoms() > 0:\n",
    "                    if fg in ['morgan', 'morgan_count']:\n",
    "                        self.features.extend(features_generator(mol, num_bits=args.num_bits))\n",
    "                    else:\n",
    "                        self.features.extend(features_generator(mol))\n",
    "\n",
    "            self.features = np.array(self.features)\n",
    "\n",
    "        # Fix nans in features\n",
    "        if self.features is not None:\n",
    "            replace_token = 0\n",
    "            self.features = np.where(np.isnan(self.features), replace_token, self.features)\n",
    "\n",
    "        # Create targets\n",
    "        self.targets = [float(x) if x != '' else None for x in line[1:]]\n",
    "\n",
    "    def set_features(self, features: np.ndarray):\n",
    "        \"\"\"\n",
    "        Sets the features of the molecule.\n",
    "\n",
    "        :param features: A 1-D numpy array of features for the molecule.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "\n",
    "    def num_tasks(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of prediction tasks.\n",
    "\n",
    "        :return: The number of tasks.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def set_targets(self, targets: List[float]):\n",
    "        \"\"\"\n",
    "        Sets the targets of a molecule.\n",
    "\n",
    "        :param targets: A list of floats containing the targets.\n",
    "        \"\"\"\n",
    "        self.targets = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9cf21e5-5631-41e9-9c03-56ef00e2b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset(Dataset):\n",
    "    \"\"\"A MoleculeDataset contains a list of molecules and their associated features and targets.\"\"\"\n",
    "\n",
    "    def __init__(self, data: List[MoleculeDatapoint]):\n",
    "        \"\"\"\n",
    "        Initializes a MoleculeDataset, which contains a list of MoleculeDatapoints (i.e. a list of molecules).\n",
    "\n",
    "        :param data: A list of MoleculeDatapoints.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.args = self.data[0].args if len(self.data) > 0 else None\n",
    "        self.scaler = None\n",
    "\n",
    "    def compound_names(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the compound names associated with the molecule (if they exist).\n",
    "\n",
    "        :return: A list of compound names or None if the dataset does not contain compound names.\n",
    "        \"\"\"\n",
    "        if len(self.data) == 0 or self.data[0].compound_name is None:\n",
    "            return None\n",
    "\n",
    "        return [d.compound_name for d in self.data]\n",
    "\n",
    "    def smiles(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the smiles strings associated with the molecules.\n",
    "\n",
    "        :return: A list of smiles strings.\n",
    "        \"\"\"\n",
    "        return [d.smiles for d in self.data]\n",
    "\n",
    "    def features(self) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns the features associated with each molecule (if they exist).\n",
    "\n",
    "        :return: A list of 1D numpy arrays containing the features for each molecule or None if there are no features.\n",
    "        \"\"\"\n",
    "        if len(self.data) == 0 or self.data[0].features is None:\n",
    "            return None\n",
    "\n",
    "        return [d.features for d in self.data]\n",
    "\n",
    "    def targets(self) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Returns the targets associated with each molecule.\n",
    "\n",
    "        :return: A list of lists of floats containing the targets.\n",
    "        \"\"\"\n",
    "        return [d.targets for d in self.data]\n",
    "\n",
    "    def num_tasks(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of prediction tasks.\n",
    "\n",
    "        :return: The number of tasks.\n",
    "        \"\"\"\n",
    "        if self.args.dataset_type == 'multiclass':\n",
    "            return int(max([i[0] for i in self.targets()])) + 1\n",
    "        else:\n",
    "            return self.data[0].num_tasks() if len(self.data) > 0 else None\n",
    "\n",
    "    def features_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the size of the features array associated with each molecule.\n",
    "\n",
    "        :return: The size of the features.\n",
    "        \"\"\"\n",
    "        return len(self.data[0].features) if len(self.data) > 0 and self.data[0].features is not None else None\n",
    "\n",
    "    def shuffle(self, seed: int = None):\n",
    "        \"\"\"\n",
    "        Shuffles the dataset.\n",
    "\n",
    "        :param seed: Optional random seed.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def normalize_features(self, scaler: StandardScaler = None, replace_nan_token: int = 0) -> StandardScaler:\n",
    "        \"\"\"\n",
    "        Normalizes the features of the dataset using a StandardScaler (subtract mean, divide by standard deviation).\n",
    "\n",
    "        If a scaler is provided, uses that scaler to perform the normalization. Otherwise fits a scaler to the\n",
    "        features in the dataset and then performs the normalization.\n",
    "\n",
    "        :param scaler: A fitted StandardScaler. Used if provided. Otherwise a StandardScaler is fit on\n",
    "        this dataset and is then used.\n",
    "        :param replace_nan_token: What to replace nans with.\n",
    "        :return: A fitted StandardScaler. If a scaler is provided, this is the same scaler. Otherwise, this is\n",
    "        a scaler fit on this dataset.\n",
    "        \"\"\"\n",
    "        if len(self.data) == 0 or self.data[0].features is None:\n",
    "            return None\n",
    "\n",
    "        if scaler is not None:\n",
    "            self.scaler = scaler\n",
    "\n",
    "        elif self.scaler is None:\n",
    "            features = np.vstack([d.features for d in self.data])\n",
    "            self.scaler = StandardScaler(replace_nan_token=replace_nan_token)\n",
    "            self.scaler.fit(features)\n",
    "\n",
    "        for d in self.data:\n",
    "            d.set_features(self.scaler.transform(d.features.reshape(1, -1))[0])\n",
    "\n",
    "        return self.scaler\n",
    "\n",
    "    def set_targets(self, targets: List[List[float]]):\n",
    "        \"\"\"\n",
    "        Sets the targets for each molecule in the dataset. Assumes the targets are aligned with the datapoints.\n",
    "\n",
    "        :param targets: A list of lists of floats containing targets for each molecule. This must be the\n",
    "        same length as the underlying dataset.\n",
    "        \"\"\"\n",
    "        assert len(self.data) == len(targets)\n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i].set_targets(targets[i])\n",
    "\n",
    "    def sort(self, key: Callable):\n",
    "        \"\"\"\n",
    "        Sorts the dataset using the provided key.\n",
    "\n",
    "        :param key: A function on a MoleculeDatapoint to determine the sorting order.\n",
    "        \"\"\"\n",
    "        self.data.sort(key=key)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset (i.e. the number of molecules).\n",
    "\n",
    "        :return: The length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx) -> Union[MoleculeDatapoint, List[MoleculeDatapoint]]:\n",
    "        \"\"\"\n",
    "        Gets one or more MoleculeDatapoints via an index or slice.\n",
    "\n",
    "        :param item: An index (int) or a slice object.\n",
    "        :return: A MoleculeDatapoint if an int is provided or a list of MoleculeDatapoints if a slice is provided.\n",
    "        \"\"\"\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc94b7-7f61-41c6-b8b1-ccdf2db835e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### get_data함수"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ce5cfd7-ea30-47ee-a8d9-a19392a89a84",
   "metadata": {},
   "source": [
    "get_data(path=args.data_path, args=args, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2636f96d-08f0-4568-b9ee-42c10dadcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_invalid_smiles(data: MoleculeDataset) -> MoleculeDataset:\n",
    "    \"\"\"\n",
    "    Filters out invalid SMILES.\n",
    "\n",
    "    :param data: A MoleculeDataset.\n",
    "    :return: A MoleculeDataset with only valid molecules.\n",
    "    \"\"\"\n",
    "    datapoint_list = []\n",
    "    for idx, datapoint in enumerate(data):\n",
    "        if datapoint.smiles == '':\n",
    "            print(f'invalid smiles {idx}: {datapoint.smiles}')\n",
    "            continue\n",
    "        mol = Chem.MolFromSmiles(datapoint.smiles)\n",
    "        if mol.GetNumHeavyAtoms() == 0:\n",
    "            print(f'invalid heavy {idx}')\n",
    "            continue\n",
    "        datapoint_list.append(datapoint)\n",
    "    return MoleculeDataset(datapoint_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40acd2d4-3f98-4d26-bee8-eda4f1a5b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loads features saved in a variety of formats.\n",
    "\n",
    "    Supported formats:\n",
    "    - .npz compressed (assumes features are saved with name \"features\")\n",
    "\n",
    "    All formats assume that the SMILES strings loaded elsewhere in the code are in the same\n",
    "    order as the features loaded here.\n",
    "\n",
    "    :param path: Path to a file containing features.\n",
    "    :return: A 2D numpy array of size (num_molecules, features_size) containing the features.\n",
    "    \"\"\"\n",
    "    extension = os.path.splitext(path)[1]\n",
    "\n",
    "    if extension == '.npz':\n",
    "        features = np.load(path)['features']\n",
    "    else:\n",
    "        raise ValueError(f'Features path extension {extension} not supported.')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b520e3f0-9162-4933-b2ab-9b9722997bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: str,\n",
    "             skip_invalid_smiles: bool = True,\n",
    "             args: Namespace = None,\n",
    "             features_path: List[str] = None,\n",
    "             max_data_size: int = None,\n",
    "             use_compound_names: bool = None,\n",
    "             logger: Logger = None) -> MoleculeDataset:\n",
    "    \"\"\"\n",
    "    Gets smiles string and target values (and optionally compound names if provided) from a CSV file.\n",
    "\n",
    "    :param path: Path to a CSV file.\n",
    "    :param skip_invalid_smiles: Whether to skip and filter out invalid smiles.\n",
    "    :param args: Arguments.\n",
    "    :param features_path: A list of paths to files containing features. If provided, it is used\n",
    "    in place of args.features_path.\n",
    "    :param max_data_size: The maximum number of data points to load.\n",
    "    :param use_compound_names: Whether file has compound names in addition to smiles strings.\n",
    "    :param logger: Logger.\n",
    "    :return: A MoleculeDataset containing smiles strings and target values along\n",
    "    with other info such as additional features and compound names when desired.\n",
    "    \"\"\"\n",
    "    debug = logger.debug if logger is not None else print\n",
    "\n",
    "    if args is not None:\n",
    "        # Prefer explicit function arguments but default to args if not provided\n",
    "        features_path = features_path if features_path is not None else args.features_path\n",
    "        max_data_size = max_data_size if max_data_size is not None else args.max_data_size\n",
    "        use_compound_names = use_compound_names if use_compound_names is not None else args.use_compound_names\n",
    "    else:\n",
    "        use_compound_names = False\n",
    "\n",
    "    max_data_size = max_data_size or float('inf')\n",
    "\n",
    "    # Load features\n",
    "    if features_path is not None:\n",
    "        features_data = []\n",
    "        for feat_path in features_path:\n",
    "            features_data.append(load_features(feat_path))  # each is num_data x num_features\n",
    "        features_data = np.concatenate(features_data, axis=1)\n",
    "        args.features_dim = len(features_data[0])\n",
    "    else:\n",
    "        features_data = None\n",
    "        if args is not None:\n",
    "            args.features_dim = 0\n",
    "\n",
    "    skip_smiles = set()\n",
    "\n",
    "    # Load data\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "\n",
    "        lines = []\n",
    "        for line in reader:\n",
    "            smiles = line[0]\n",
    "\n",
    "            if smiles in skip_smiles:\n",
    "                continue\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "            if len(lines) >= max_data_size:\n",
    "                break\n",
    "\n",
    "        data = MoleculeDataset([\n",
    "            MoleculeDatapoint(\n",
    "                line=line,\n",
    "                args=args,\n",
    "                features=features_data[i] if features_data is not None else None,\n",
    "                use_compound_names=use_compound_names\n",
    "            ) for i, line in tqdm(enumerate(lines), total=len(lines), disable=True)\n",
    "        ])\n",
    "\n",
    "    # Filter out invalid SMILES\n",
    "    if skip_invalid_smiles:\n",
    "        original_data_len = len(data)\n",
    "        data = filter_invalid_smiles(data)\n",
    "\n",
    "        if len(data) < original_data_len:\n",
    "            debug(f'Warning: {original_data_len - len(data)} SMILES are invalid.')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef63c1-dcf8-485a-9f2b-e74d8868f2bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### get_data, DataSet예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05fca776-ced6-46f8-bd83-557a1b47cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tox21_get_data = get_data(path=args.data_path, args=args, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c2fe075-f196-4527-bcec-e76cb81c4230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7831"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tox21_get_data.features())\n",
    "# get_data는 한마디로 tox21데이터셋의 전체에 대한 총 데이터셋?이라고 보면 된다. split 되기 전 단계!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da1f99c7-7bed-40e2-a172-780f2be49dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tox21_get_data.features()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f446a0f-b55e-4aae-969a-18dd436fd730",
   "metadata": {},
   "source": [
    "#### 4-3-1-2. split_data()\n",
    "- split_data는 train/valid/test로 나누는 것이며, 여러개가 지원은 되나 실제로는 random, scaffold_balanced를 사용한다.\n",
    "- 그리고 나는 여기에 smote가 적용되게끔 했다. smiles식이 실제 학습에는 사용이 안되는 것과 dataset에 feature와 target만 있기에 feature, target을 기준으로 smote, smiles식들은 임의로 넣었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd36874-ae3a-46f0-bbed-3bcc71df236c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### scaffold_split()\n",
    "- scaffold_split은 MurckoScaffold 방식으로 Mol구조로부터 scaffold에 해당하는 smiles를 추출하고\n",
    "- 같은 splitted dataset 내에서 scaffold가 공유되지 않게끔, 즉 train set에 valid, test에 해당하는 scaffold가 안들어가게끔하는 역할을 한다.\n",
    "- 방식은 valid, test셋의 최대허용량의 절반 이상이 포함되는 scaffold는 무조건 train에 넣고, 나머지는 random하게 넣는 방식이다.\n",
    "- 이로 인해서 모르는 분자 구조에 대한 대응 능력이 향상된다고 보면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee19cd13-cadd-4d92-a323-30bca5654694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Set, Tuple, Union, Dict\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a05c3d9-da07-4c7c-88e0-b3b8380e772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scaffold(mol: Union[str, Chem.Mol], include_chirality: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Compute the Bemis-Murcko scaffold for a SMILES string.\n",
    "\n",
    "    :param mol: A smiles string or an RDKit molecule.\n",
    "    :param include_chirality: Whether to include chirality.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol\n",
    "    scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=include_chirality)\n",
    "\n",
    "    return scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "decb9169-72ab-43ad-be33-f61ce03a016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_to_smiles(mols: Union[List[str], List[Chem.Mol]],\n",
    "                       use_indices: bool = False) -> Dict[str, Union[Set[str], Set[int]]]:\n",
    "    \"\"\"\n",
    "    Computes scaffold for each smiles string and returns a mapping from scaffolds to sets of smiles.\n",
    "\n",
    "    :param mols: A list of smiles strings or RDKit molecules.\n",
    "    :param use_indices: Whether to map to the smiles' index in all_smiles rather than mapping\n",
    "    to the smiles string itself. This is necessary if there are duplicate smiles.\n",
    "    :return: A dictionary mapping each unique scaffold to all smiles (or smiles indices) which have that scaffold.\n",
    "    \"\"\"\n",
    "    scaffolds = defaultdict(set)\n",
    "    for i, mol in tqdm(enumerate(mols), total=len(mols)):\n",
    "        scaffold = generate_scaffold(mol)\n",
    "        if use_indices:\n",
    "            scaffolds[scaffold].add(i)\n",
    "        else:\n",
    "            scaffolds[scaffold].add(mol)\n",
    "\n",
    "    return scaffolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c819c463-b801-4e10-9fa9-0d25e12ade9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scaffold_stats(data: MoleculeDataset,\n",
    "                       index_sets: List[Set[int]],\n",
    "                       num_scaffolds: int = 10,\n",
    "                       num_labels: int = 20,\n",
    "                       logger: logging.Logger = None) -> List[Tuple[List[float], List[int]]]:\n",
    "    \"\"\"\n",
    "    Logs and returns statistics about counts and average target values in molecular scaffolds.\n",
    "\n",
    "    :param data: A MoleculeDataset.\n",
    "    :param index_sets: A list of sets of indices representing splits of the data.\n",
    "    :param num_scaffolds: The number of scaffolds about which to display statistics.\n",
    "    :param num_labels: The number of labels about which to display statistics.\n",
    "    :param logger: A Logger.\n",
    "    :return: A list of tuples where each tuple contains a list of average target values\n",
    "    across the first num_labels labels and a list of the number of non-zero values for\n",
    "    the first num_scaffolds scaffolds, sorted in decreasing order of scaffold frequency.\n",
    "    \"\"\"\n",
    "    # print some statistics about scaffolds\n",
    "    target_avgs = []\n",
    "    counts = []\n",
    "    for index_set in index_sets:\n",
    "        data_set = [data[i] for i in index_set]\n",
    "        targets = [d.targets for d in data_set]\n",
    "        targets = np.array(targets, dtype=np.float)\n",
    "        target_avgs.append(np.nanmean(targets, axis=0))\n",
    "        counts.append(np.count_nonzero(~np.isnan(targets), axis=0))\n",
    "    stats = [(target_avgs[i][:num_labels], counts[i][:num_labels]) for i in range(min(num_scaffolds, len(target_avgs)))]\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.debug('Label averages per scaffold, in decreasing order of scaffold frequency,'\n",
    "                     f'capped at {num_scaffolds} scaffolds and {num_labels} labels: {stats}')\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "731cbfbe-ce32-4294-9b85-84d96ec57d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_split(data: MoleculeDataset,\n",
    "                   sizes: Tuple[float, float, float] = (0.8, 0.1, 0.1),\n",
    "                   balanced: bool = False,\n",
    "                   seed: int = 0,\n",
    "                   args: Namespace = None,\n",
    "                   logger: logging.Logger = None) -> Tuple[MoleculeDataset,\n",
    "                                                           MoleculeDataset,\n",
    "                                                           MoleculeDataset]:\n",
    "    \"\"\"\n",
    "    Split a dataset by scaffold so that no molecules sharing a scaffold are in the same split.\n",
    "\n",
    "    :param data: A MoleculeDataset.\n",
    "    :param sizes: A length-3 tuple with the proportions of data in the\n",
    "    train, validation, and test sets.\n",
    "    :param balanced: Try to balance sizes of scaffolds in each set, rather than just putting smallest in test set.\n",
    "    :param seed: Seed for shuffling when doing balanced splitting.\n",
    "    :param logger: A logger.\n",
    "    :return: A tuple containing the train, validation, and test splits of the data.\n",
    "    \"\"\"\n",
    "    assert sum(sizes) == 1\n",
    "\n",
    "\n",
    "    # Split\n",
    "    train_size, val_size, test_size = sizes[0] * len(data), sizes[1] * len(data), sizes[2] * len(data)\n",
    "    train, val, test = [], [], []\n",
    "    train_scaffold_count, val_scaffold_count, test_scaffold_count = 0, 0, 0\n",
    "\n",
    "    # Map from scaffold to index in the data\n",
    "    scaffold_to_indices = scaffold_to_smiles(data.smiles(), use_indices=True)\n",
    "\n",
    "    if balanced:  # Put stuff that's bigger than half the val/test size into train, rest just order randomly\n",
    "        index_sets = list(scaffold_to_indices.values())\n",
    "        big_index_sets = []\n",
    "        small_index_sets = []\n",
    "        for index_set in index_sets:\n",
    "            if len(index_set) > val_size / 2 or len(index_set) > test_size / 2:\n",
    "                big_index_sets.append(index_set)\n",
    "            else:\n",
    "                small_index_sets.append(index_set)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(big_index_sets)\n",
    "        random.shuffle(small_index_sets)\n",
    "        index_sets = big_index_sets + small_index_sets\n",
    "    else:  # Sort from largest to smallest scaffold sets\n",
    "        index_sets = sorted(list(scaffold_to_indices.values()),\n",
    "                            key=lambda index_set: len(index_set),\n",
    "                            reverse=True)\n",
    "\n",
    "    for index_set in index_sets:\n",
    "        if len(train) + len(index_set) <= train_size:\n",
    "            train += index_set\n",
    "            train_scaffold_count += 1\n",
    "        elif len(val) + len(index_set) <= val_size:\n",
    "            val += index_set\n",
    "            val_scaffold_count += 1\n",
    "        else:\n",
    "            test += index_set\t\n",
    "            test_scaffold_count += 1\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.debug(f'Total scaffolds = {len(scaffold_to_indices):,} | '\n",
    "                     f'train scaffolds = {train_scaffold_count:,} | '\n",
    "                     f'val scaffolds = {val_scaffold_count:,} | '\n",
    "                     f'test scaffolds = {test_scaffold_count:,}')\n",
    "\n",
    "    log_scaffold_stats(data, index_sets, logger=logger)\n",
    "\n",
    "    # Map from indices to data\n",
    "    train = [data[i] for i in train]\n",
    "    val = [data[i] for i in val]\n",
    "    test = [data[i] for i in test]\n",
    "\n",
    "    if args.smote==True :\n",
    "    # test for SMOTE\n",
    "        from imblearn.over_sampling import SMOTE \n",
    "        from imblearn.combine import SMOTEENN \n",
    "        import random as Rand\n",
    "        import pandas as pd\n",
    "        args.train_data_size = len(train)\n",
    "\n",
    "    #Smote start\n",
    "        sm = SMOTE(random_state=args.seed, sampling_strategy=args.smote_rate)\n",
    "#        sm = SMOTEENN(random_state=args.seed)\n",
    "        train_smile = []\n",
    "        train_feature = []\n",
    "        train_target = []\n",
    "        for i in range(len(train)):\n",
    "            train_smile.append(train[i].smiles)\n",
    "            train_feature.append(train[i].features)\n",
    "            train_target.append(np.array(train[i].targets, dtype=float))\n",
    "        n_train_features, n_train_targets = sm.fit_resample(train_feature, train_target)\n",
    "        logger.info(f'smote sampling rate is 1:{args.smote_rate}')\n",
    "        logger.info(f'\\n\\nold train_length is {len(train)}, oversampled_train_length is {len(n_train_targets)}\\n\\n')\n",
    "        logger.info(f'result is {n_train_targets[0]}')\n",
    "\n",
    "    #make dummy Smiles\n",
    "        resampled_length = len(n_train_targets)\n",
    "        need_smiles = resampled_length-len(train)\n",
    "        n_train_smiles = train_smile\n",
    "        for i in range(need_smiles):\n",
    "            n_train_smiles.append(Rand.choice(train_smile))\n",
    "\n",
    "        n_train_targets = np.array(n_train_targets)\n",
    "        n_train_smiles = np.array(n_train_smiles, str)\n",
    "        new_train = pd.DataFrame(np.stack([n_train_smiles,n_train_targets]).transpose())\n",
    "        new_train.to_csv(os.path.join(args.save_dir, \"augdata.csv\"), index=False,header=False)\n",
    "\n",
    "    #reload data :( i'm noob\n",
    "        with open(os.path.join(args.save_dir, \"augdata.csv\")) as f:\n",
    "            reader = csv.reader(f)\n",
    "\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                smiles = line[0]\n",
    "\n",
    "                lines.append(line)\n",
    "\n",
    "            aug_train = MoleculeDataset([\n",
    "                MoleculeDatapoint(\n",
    "                    line=line,\n",
    "                    args=args,\n",
    "                    features=n_train_features[i] if n_train_features is not None else None,\n",
    "                    use_compound_names=False\n",
    "                ) for i, line in tqdm(enumerate(lines), total=len(lines), disable=True)\n",
    "            ])\n",
    "\n",
    "\n",
    "        return MoleculeDataset(aug_train), MoleculeDataset(val), MoleculeDataset(test)\n",
    "\n",
    "    else : return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7552ecb-6145-42a9-86c9-37adfbf80e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### split_data() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20c3e9e1-94dd-41b1-9fbb-7136fef837ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_sizes(data: MoleculeDataset) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Determines the proportions of the different classes in the classification dataset.\n",
    "\n",
    "    :param data: A classification dataset\n",
    "    :return: A list of lists of class proportions. Each inner list contains the class proportions\n",
    "    for a task.\n",
    "    \"\"\"\n",
    "    targets = data.targets()\n",
    "\n",
    "    # Filter out Nones\n",
    "    valid_targets = [[] for _ in range(data.num_tasks())]\n",
    "    for i in range(len(targets)):\n",
    "        for task_num in range(len(targets[i])):\n",
    "            if targets[i][task_num] is not None:\n",
    "                valid_targets[task_num].append(targets[i][task_num])\n",
    "\n",
    "    class_sizes = []\n",
    "    for task_targets in valid_targets:\n",
    "        # Make sure we're dealing with a binary classification task\n",
    "        assert set(np.unique(task_targets)) <= {0, 1}\n",
    "\n",
    "        try:\n",
    "            ones = np.count_nonzero(task_targets) / len(task_targets)\n",
    "        except ZeroDivisionError:\n",
    "            ones = float('nan')\n",
    "            print('Warning: class has no targets')\n",
    "        class_sizes.append([1 - ones, ones])\n",
    "\n",
    "    return class_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11c0652e-48c8-44cd-978c-c8703c269ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data: MoleculeDataset,\n",
    "               split_type: str = 'random',\n",
    "               sizes: Tuple[float, float, float] = (0.8, 0.1, 0.1),\n",
    "               seed: int = 0,\n",
    "               args: Namespace = None,\n",
    "               logger: Logger = None) -> Tuple[MoleculeDataset,\n",
    "                                               MoleculeDataset,\n",
    "                                               MoleculeDataset]:\n",
    "    \"\"\"\n",
    "    Splits data into training, validation, and test splits.\n",
    "\n",
    "    :param data: A MoleculeDataset.\n",
    "    :param split_type: Split type.\n",
    "    :param sizes: A length-3 tuple with the proportions of data in the\n",
    "    train, validation, and test sets.\n",
    "    :param seed: The random seed to use before shuffling data.\n",
    "    :param args: Namespace of arguments.\n",
    "    :param logger: A logger.\n",
    "    :return: A tuple containing the train, validation, and test splits of the data.\n",
    "    \"\"\"\n",
    "    assert len(sizes) == 3 and sum(sizes) == 1\n",
    "\n",
    "    if args is not None:\n",
    "        folds_file, val_fold_index, test_fold_index = \\\n",
    "            args.folds_file, args.val_fold_index, args.test_fold_index\n",
    "    else:\n",
    "        folds_file = val_fold_index = test_fold_index = None\n",
    "\n",
    "    if split_type == 'crossval':\n",
    "        index_set = args.crossval_index_sets[args.seed]\n",
    "        data_split = []\n",
    "        for split in range(3):\n",
    "            split_indices = []\n",
    "            for index in index_set[split]:\n",
    "                with open(os.path.join(args.crossval_index_dir, f'{index}.pkl'), 'rb') as rf:\n",
    "                    split_indices.extend(pickle.load(rf))\n",
    "            data_split.append([data[i] for i in split_indices])\n",
    "        train, val, test = tuple(data_split)\n",
    "        return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)\n",
    "\n",
    "    elif split_type == 'index_predetermined':\n",
    "        split_indices = args.crossval_index_sets[args.seed]\n",
    "        assert len(split_indices) == 3\n",
    "        data_split = []\n",
    "        for split in range(3):\n",
    "            data_split.append([data[i] for i in split_indices[split]])\n",
    "        train, val, test = tuple(data_split)\n",
    "        return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)\n",
    "\n",
    "    elif split_type == 'predetermined':\n",
    "        if not val_fold_index:\n",
    "            assert sizes[2] == 0  # test set is created separately so use all of the other data for train and val\n",
    "        assert folds_file is not None\n",
    "        assert test_fold_index is not None\n",
    "\n",
    "        try:\n",
    "            with open(folds_file, 'rb') as f:\n",
    "                all_fold_indices = pickle.load(f)\n",
    "        except UnicodeDecodeError:\n",
    "            with open(folds_file, 'rb') as f:\n",
    "                all_fold_indices = pickle.load(f, encoding='latin1')  # in case we're loading indices from python2\n",
    "        # assert len(data) == sum([len(fold_indices) for fold_indices in all_fold_indices])\n",
    "\n",
    "        log_scaffold_stats(data, all_fold_indices, logger=logger)\n",
    "\n",
    "        folds = [[data[i] for i in fold_indices] for fold_indices in all_fold_indices]\n",
    "\n",
    "        test = folds[test_fold_index]\n",
    "        if val_fold_index is not None:\n",
    "            val = folds[val_fold_index]\n",
    "\n",
    "        train_val = []\n",
    "        for i in range(len(folds)):\n",
    "            if i != test_fold_index and (val_fold_index is None or i != val_fold_index):\n",
    "                train_val.extend(folds[i])\n",
    "\n",
    "        if val_fold_index is not None:\n",
    "            train = train_val\n",
    "        else:\n",
    "            random.seed(seed)\n",
    "            random.shuffle(train_val)\n",
    "            train_size = int(sizes[0] * len(train_val))\n",
    "            train = train_val[:train_size]\n",
    "            val = train_val[train_size:]\n",
    "\n",
    "        return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)\n",
    "\n",
    "    elif split_type == 'scaffold_balanced':\n",
    "        return scaffold_split(data, sizes=sizes, balanced=True, seed=seed, args=args, logger=logger)\n",
    "\n",
    "    elif split_type == 'random':\n",
    "        data.shuffle(seed=seed)\n",
    "\n",
    "        train_size = int(sizes[0] * len(data))\n",
    "        train_val_size = int((sizes[0] + sizes[1]) * len(data))\n",
    "\n",
    "        train = data[:train_size]\n",
    "        val = data[train_size:train_val_size]\n",
    "        test = data[train_val_size:]\n",
    "\n",
    "        if args.smote==True :\n",
    "        # test for SMOTE\n",
    "            from imblearn.over_sampling import SMOTE \n",
    "            from imblearn.combine import SMOTEENN \n",
    "            import random as Rand\n",
    "            import pandas as pd\n",
    "            args.train_data_size = len(train)\n",
    "\n",
    "        #Smote start\n",
    "            sm = SMOTE(random_state=args.seed, sampling_strategy=args.smote_rate)\n",
    "    #        sm = SMOTEENN(random_state=args.seed)\n",
    "            train_smile = []\n",
    "            train_feature = []\n",
    "            train_target = []\n",
    "            for i in range(len(train)):\n",
    "                train_smile.append(train[i].smiles)\n",
    "                train_feature.append(train[i].features)\n",
    "                train_target.append(np.array(train[i].targets, dtype=float))\n",
    "            n_train_features, n_train_targets = sm.fit_resample(train_feature, train_target)\n",
    "            logger.info(f'smote sampling rate is 1:{args.smote_rate}')\n",
    "            logger.info(f'\\n\\nold train_length is {len(train)}, oversampled_train_length is {len(n_train_targets)}\\n\\n')\n",
    "            logger.info(f'result is {n_train_targets[0]}')\n",
    "\n",
    "        #make dummy Smiles\n",
    "            resampled_length = len(n_train_targets)\n",
    "            need_smiles = resampled_length-len(train)\n",
    "            n_train_smiles = train_smile\n",
    "            for i in range(need_smiles):\n",
    "                n_train_smiles.append(Rand.choice(train_smile))\n",
    "\n",
    "            n_train_targets = np.array(n_train_targets)\n",
    "            n_train_smiles = np.array(n_train_smiles, str)\n",
    "            new_train = pd.DataFrame(np.stack([n_train_smiles,n_train_targets]).transpose())\n",
    "            new_train.to_csv(os.path.join(args.save_dir, \"augdata.csv\"), index=False,header=False)\n",
    "\n",
    "        #reload data :( i'm noob\n",
    "            with open(os.path.join(args.save_dir, \"augdata.csv\")) as f:\n",
    "                reader = csv.reader(f)\n",
    "\n",
    "                lines = []\n",
    "                for line in reader:\n",
    "                    smiles = line[0]\n",
    "\n",
    "                    lines.append(line)\n",
    "\n",
    "                aug_train = MoleculeDataset([\n",
    "                    MoleculeDatapoint(\n",
    "                        line=line,\n",
    "                        args=args,\n",
    "                        features=n_train_features[i] if n_train_features is not None else None,\n",
    "                        use_compound_names=False\n",
    "                    ) for i, line in tqdm(enumerate(lines), total=len(lines), disable=True)\n",
    "                ])\n",
    "\n",
    "    #        logger.info(f'aug_train length is : {len(aug_train)}')\n",
    "    #        logger.info(f'aug_train.smiles is {aug_train[0].smiles}')\n",
    "    #        logger.info(f'aug_train.targets is {aug_train[0].targets}')\n",
    "    #        logger.info(f'aug_train.features is {aug_train[0].features}')\n",
    "\n",
    "    #        fn = os.path.join(args.save_dir, \"augfeatures.npz\")\n",
    "    #        np.savez_compressed(fn, features=aug_train)\n",
    "\n",
    "\n",
    "            return MoleculeDataset(aug_train), MoleculeDataset(val), MoleculeDataset(test)\n",
    "\n",
    "        return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'split_type \"{split_type}\" not supported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db7466-a7bf-400f-9b21-1434b33ac957",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### load_data 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2f39a2f-fbac-47f8-843e-803894ac82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, debug, logger):\n",
    "    \"\"\"\n",
    "    load the training data.\n",
    "    :param args:\n",
    "    :param debug:\n",
    "    :param logger:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Get data\n",
    "    debug('Loading data')\n",
    "    args.task_names = get_task_names(args.data_path)\n",
    "    data = get_data(path=args.data_path, args=args, logger=logger)\n",
    "    if data.data[0].features is not None:\n",
    "        args.features_dim = len(data.data[0].features)\n",
    "    else:\n",
    "        args.features_dim = 0\n",
    "    shared_dict = {}\n",
    "    args.num_tasks = data.num_tasks()\n",
    "    args.features_size = data.features_size()\n",
    "    debug(f'Number of tasks = {args.num_tasks}')\n",
    "    # Split data\n",
    "    debug(f'Splitting data with seed {args.seed}')\n",
    "    if args.separate_test_path:\n",
    "        test_data = get_data(path=args.separate_test_path, args=args,\n",
    "                             features_path=args.separate_test_features_path, logger=logger)\n",
    "    if args.separate_val_path:\n",
    "        val_data = get_data(path=args.separate_val_path, args=args,\n",
    "                            features_path=args.separate_val_features_path, logger=logger)\n",
    "    if args.separate_val_path and args.separate_test_path:\n",
    "        train_data = data\n",
    "    elif args.separate_val_path:\n",
    "        train_data, _, test_data = split_data(data=data, split_type=args.split_type,\n",
    "                                              sizes=(0.8, 0.2, 0.0), seed=args.seed, args=args, logger=logger)\n",
    "    elif args.separate_test_path:\n",
    "        train_data, val_data, _ = split_data(data=data, split_type=args.split_type,\n",
    "                                             sizes=(0.8, 0.2, 0.0), seed=args.seed, args=args, logger=logger)\n",
    "    else:\n",
    "        train_data, val_data, test_data = split_data(data=data, split_type=args.split_type,\n",
    "                                                     sizes=args.split_sizes, seed=args.seed, args=args, logger=logger)\n",
    "    if args.features_scaling:\n",
    "        features_scaler = train_data.normalize_features(replace_nan_token=0)\n",
    "        val_data.normalize_features(features_scaler)\n",
    "        test_data.normalize_features(features_scaler)\n",
    "    else:\n",
    "        features_scaler = None\n",
    "\n",
    "    if args.smote == True:\n",
    "        if args.dataset_type == 'classification':\n",
    "            class_sizes = get_class_sizes(data)\n",
    "            debug('Origin Class sizes')\n",
    "            for i, task_class_sizes in enumerate(class_sizes):\n",
    "                debug(f'{args.task_names[i]} '\n",
    "                      f'{\", \".join(f\"{cls}: {int(size*args.train_data_size)}({size * 100:.2f}%)\" for cls, size in enumerate(task_class_sizes))}')\n",
    "        debug(f'Total size = {len(data):,} | '\n",
    "              f'train size = {args.train_data_size:,} | val size = {len(val_data):,} | test size = {len(test_data):,}')\n",
    "        args.train_data_size = len(train_data)\n",
    "    \n",
    "        debug('Smoted Class sizes')\n",
    "        smoted_class_sizes = get_class_sizes(train_data)\n",
    "        for i, task_class_sizes in enumerate(smoted_class_sizes):\n",
    "            debug(f'{args.task_names[i]} '\n",
    "                  f'{\", \".join(f\"{cls}: {int(size*args.train_data_size)}({size * 100:.2f}%)\" for cls, size in enumerate(task_class_sizes))}')\n",
    "        #note : there is some error of number because class_count is class_rate*data_length\n",
    "        debug(f'Total size = {len(test_data)+len(train_data)+len(val_data):,} | '\n",
    "              f'train size = {args.train_data_size:,} | val size = {len(val_data):,} | test size = {len(test_data):,}')\n",
    "    else:\n",
    "        if args.dataset_type == 'classification':\n",
    "            class_sizes = get_class_sizes(data)\n",
    "            debug('Class sizes')\n",
    "            for i, task_class_sizes in enumerate(class_sizes):\n",
    "                debug(f'{args.task_names[i]} '\n",
    "                      f'{\", \".join(f\"{cls}: {size * 100:.2f}%\" for cls, size in enumerate(task_class_sizes))}')\n",
    "        args.train_data_size = len(train_data)\n",
    "        debug(f'Total size = {len(data):,} | '\n",
    "              f'train size = {len(train_data):,} | val size = {len(val_data):,} | test size = {len(test_data):,}')\n",
    "\n",
    "    # Initialize scaler and scale training targets by subtracting mean and dividing standard deviation (regression only)\n",
    "    if args.dataset_type == 'regression':\n",
    "        debug('Fitting scaler')\n",
    "        _, train_targets = train_data.smiles(), train_data.targets()\n",
    "        scaler = StandardScaler().fit(train_targets)\n",
    "        scaled_targets = scaler.transform(train_targets).tolist()\n",
    "        train_data.set_targets(scaled_targets)\n",
    "\n",
    "        val_targets = val_data.targets()\n",
    "        scaled_val_targets = scaler.transform(val_targets).tolist()\n",
    "        val_data.set_targets(scaled_val_targets)\n",
    "    else:\n",
    "        scaler = None\n",
    "    return features_scaler, scaler, shared_dict, test_data, train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d9699-d8b3-4437-8cef-862784258594",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### load_data 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a52fc55-3aba-4fe7-bb65-9ab3352f80e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-4e45bf88bc28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtox21\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "debug, info = logger.debug, logger.info\n",
    "tox21 = load_data(args, debug, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0eeae6b4-8ab3-47f5-a8ec-2724645fe503",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tox21' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-fe0020c0581f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtox21\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# 위에서부터 순서대로 features_scaler, scaler, shared_dict, test_data, train_data, val_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 즉 shared_dict은 아무것도 없다. 이건 사전훈련용인가?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tox21' is not defined"
     ]
    }
   ],
   "source": [
    "tox21\n",
    "# 위에서부터 순서대로 features_scaler, scaler, shared_dict, test_data, train_data, val_data\n",
    "# 즉 shared_dict은 아무것도 없다. 이건 사전훈련용인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268eed89-4a0c-4a74-a50e-fff5b3c73f9c",
   "metadata": {},
   "source": [
    "### 4-3-2. MolCollator() - feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c419067-86ce-4c1f-85c0-ce1270c7c1e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4-3-4-1. mol2graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44e4c0d-eb49-4e18-99eb-73843a8e12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32977ff4-3164-4289-8f0a-cb109e8c3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ATOMIC_NUM = 100\n",
    "\n",
    "\n",
    "ATOM_FEATURES = {\n",
    "    'atomic_num': list(range(MAX_ATOMIC_NUM)),\n",
    "    'degree': [0, 1, 2, 3, 4, 5],\n",
    "    'formal_charge': [-1, -2, 1, 2, 0],\n",
    "    'chiral_tag': [0, 1, 2, 3],\n",
    "    'num_Hs': [0, 1, 2, 3, 4],\n",
    "    'hybridization': [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2\n",
    "    ],\n",
    "}\n",
    "\n",
    "# len(choices) + 1 to include room for uncommon values; + 2 at end for IsAromatic and mass\n",
    "ATOM_FDIM = sum(len(choices) + 1 for choices in ATOM_FEATURES.values()) + 2    # 참고로 이거 133이다... 왜지?\n",
    "BOND_FDIM = 14\n",
    "BOND_FDIM_3D = 15\n",
    "\n",
    "\n",
    "def get_atom_fdim() -> int:\n",
    "    \"\"\"\n",
    "    Gets the dimensionality of atom features.\n",
    "\n",
    "    :param: Arguments.\n",
    "    \"\"\"\n",
    "    return ATOM_FDIM + 18\n",
    "\n",
    "\n",
    "def get_bond_fdim() -> int:\n",
    "    \"\"\"\n",
    "    Gets the dimensionality of bond features.\n",
    "\n",
    "    :param: Arguments.\n",
    "    \"\"\"\n",
    "    return BOND_FDIM\n",
    "\n",
    "\n",
    "def onek_encoding_unk(value: int, choices: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Creates a one-hot encoding.\n",
    "\n",
    "    :param value: The value for which the encoding should be one.\n",
    "    :param choices: A list of possible values.\n",
    "    :return: A one-hot encoding of the value in a list of length len(choices) + 1.\n",
    "    If value is not in the list of choices, then the final element in the encoding is 1.\n",
    "    \"\"\"\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    if min(choices) < 0:\n",
    "        index = value\n",
    "    else:\n",
    "        index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b9cef12-12f4-4228-ac6a-f7342f3eb1f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atomic_num': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99],\n",
       " 'degree': [0, 1, 2, 3, 4, 5],\n",
       " 'formal_charge': [-1, -2, 1, 2, 0],\n",
       " 'chiral_tag': [0, 1, 2, 3],\n",
       " 'num_Hs': [0, 1, 2, 3, 4],\n",
       " 'hybridization': [rdkit.Chem.rdchem.HybridizationType.SP,\n",
       "  rdkit.Chem.rdchem.HybridizationType.SP2,\n",
       "  rdkit.Chem.rdchem.HybridizationType.SP3,\n",
       "  rdkit.Chem.rdchem.HybridizationType.SP3D,\n",
       "  rdkit.Chem.rdchem.HybridizationType.SP3D2]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATOM_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf6d19-cb0b-41ad-b9c3-5219835d36bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0822baa6-c0c8-45f1-9473-ad326940144e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 4-3-4-1-1. MolGraph()\n",
    "- 단일 분자에 대한 그래프 구조 및 feature화 표현에 대한 정보다.\n",
    "- 포함되는건 smiles식, atom의 개수, bond의 개수, 단일 원자/결합을 원자/결합 feature리스트로, a2b는 bond index에 어떠한 atom들이 포함되는지 매핑, b2a는 atom index에 어떠한 bond들이 포함되는지, b2revb는 순결합의 역결합 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e62f4c0-0b6f-4f71-9c70-2404e7fd26e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MolGraph:\n",
    "    \"\"\"\n",
    "    A MolGraph represents the graph structure and featurization of a single molecule.\n",
    "\n",
    "    A MolGraph computes the following attributes:\n",
    "    - smiles: Smiles string.\n",
    "    - n_atoms: The number of atoms in the molecule.\n",
    "    - n_bonds: The number of bonds in the molecule.\n",
    "    - f_atoms: A mapping from an atom index to a list atom features.\n",
    "    - f_bonds: A mapping from a bond index to a list of bond features.\n",
    "    - a2b: A mapping from an atom index to a list of incoming bond indices.\n",
    "    - b2a: A mapping from a bond index to the index of the atom the bond originates from.\n",
    "    - b2revb: A mapping from a bond index to the index of the reverse bond.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smiles: str,  args: Namespace):\n",
    "        \"\"\"\n",
    "        Computes the graph structure and featurization of a molecule.\n",
    "\n",
    "        :param smiles: A smiles string.\n",
    "        :param args: Arguments.\n",
    "        \"\"\"\n",
    "        self.smiles = smiles\n",
    "        self.args = args\n",
    "        self.n_atoms = 0  # number of atoms\n",
    "        self.n_bonds = 0  # number of bonds\n",
    "        self.f_atoms = []  # mapping from atom index to atom features\n",
    "        self.f_bonds = []  # mapping from bond index to concat(in_atom, bond) features\n",
    "        self.a2b = []  # mapping from atom index to incoming bond indices\n",
    "        self.b2a = []  # mapping from bond index to the index of the atom the bond is coming from\n",
    "        self.b2revb = []  # mapping from bond index to the index of the reverse bond\n",
    "\n",
    "        # Convert smiles to molecule\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        self.hydrogen_donor = Chem.MolFromSmarts(\"[$([N;!H0;v3,v4&+1]),$([O,S;H1;+0]),n&H1&+0]\")\n",
    "        self.hydrogen_acceptor = Chem.MolFromSmarts(\n",
    "            \"[$([O,S;H1;v2;!$(*-*=[O,N,P,S])]),$([O,S;H0;v2]),$([O,S;-]),$([N;v3;!$(N-*=[O,N,P,S])]),\"\n",
    "            \"n&H0&+0,$([o,s;+0;!$([o,s]:n);!$([o,s]:c:n)])]\")\n",
    "        self.acidic = Chem.MolFromSmarts(\"[$([C,S](=[O,S,P])-[O;H1,-1])]\")\n",
    "        self.basic = Chem.MolFromSmarts(\n",
    "            \"[#7;+,$([N;H2&+0][$([C,a]);!$([C,a](=O))]),$([N;H1&+0]([$([C,a]);!$([C,a](=O))])[$([C,a]);\"\n",
    "            \"!$([C,a](=O))]),$([N;H0&+0]([C;!$(C(=O))])([C;!$(C(=O))])[C;!$(C(=O))])]\")\n",
    "\n",
    "        self.hydrogen_donor_match = sum(mol.GetSubstructMatches(self.hydrogen_donor), ())\n",
    "        self.hydrogen_acceptor_match = sum(mol.GetSubstructMatches(self.hydrogen_acceptor), ())\n",
    "        self.acidic_match = sum(mol.GetSubstructMatches(self.acidic), ())\n",
    "        self.basic_match = sum(mol.GetSubstructMatches(self.basic), ())\n",
    "        self.ring_info = mol.GetRingInfo()\n",
    "\n",
    "\n",
    "        # fake the number of \"atoms\" if we are collapsing substructures\n",
    "        self.n_atoms = mol.GetNumAtoms()\n",
    "\n",
    "        # Get atom features\n",
    "        for _, atom in enumerate(mol.GetAtoms()):\n",
    "            self.f_atoms.append(self.atom_features(atom))\n",
    "        self.f_atoms = [self.f_atoms[i] for i in range(self.n_atoms)]\n",
    "\n",
    "        for _ in range(self.n_atoms):\n",
    "            self.a2b.append([])\n",
    "\n",
    "        # Get bond features\n",
    "        for a1 in range(self.n_atoms):\n",
    "            for a2 in range(a1 + 1, self.n_atoms):\n",
    "                bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                if bond is None:\n",
    "                    continue\n",
    "\n",
    "                if args.bond_drop_rate > 0:\n",
    "                    if np.random.binomial(1, args.bond_drop_rate):\n",
    "                        continue\n",
    "\n",
    "                f_bond = self.bond_features(bond)\n",
    "\n",
    "                # Always treat the bond as directed.\n",
    "                self.f_bonds.append(self.f_atoms[a1] + f_bond)\n",
    "                self.f_bonds.append(self.f_atoms[a2] + f_bond)\n",
    "\n",
    "                # Update index mappings\n",
    "                b1 = self.n_bonds\n",
    "                b2 = b1 + 1\n",
    "                self.a2b[a2].append(b1)  # b1 = a1 --> a2\n",
    "                self.b2a.append(a1)\n",
    "                self.a2b[a1].append(b2)  # b2 = a2 --> a1\n",
    "                self.b2a.append(a2)\n",
    "                self.b2revb.append(b2)\n",
    "                self.b2revb.append(b1)\n",
    "                self.n_bonds += 2\n",
    "\n",
    "    def atom_features(self, atom: Chem.rdchem.Atom) -> List[Union[bool, int, float]]:\n",
    "        \"\"\"\n",
    "        Builds a feature vector for an atom.\n",
    "\n",
    "        :param atom: An RDKit atom.\n",
    "        :param functional_groups: A k-hot vector indicating the functional groups the atom belongs to.\n",
    "        :return: A list containing the atom features.\n",
    "        \"\"\"\n",
    "        features = onek_encoding_unk(atom.GetAtomicNum() - 1, ATOM_FEATURES['atomic_num']) + \\\n",
    "                   onek_encoding_unk(atom.GetTotalDegree(), ATOM_FEATURES['degree']) + \\\n",
    "                   onek_encoding_unk(atom.GetFormalCharge(), ATOM_FEATURES['formal_charge']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetChiralTag()), ATOM_FEATURES['chiral_tag']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetTotalNumHs()), ATOM_FEATURES['num_Hs']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetHybridization()), ATOM_FEATURES['hybridization']) + \\\n",
    "                   [1 if atom.GetIsAromatic() else 0] + \\\n",
    "                   [atom.GetMass() * 0.01]\n",
    "        atom_idx = atom.GetIdx()\n",
    "        features = features + \\\n",
    "                   onek_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6]) + \\\n",
    "                   [atom_idx in self.hydrogen_acceptor_match] + \\\n",
    "                   [atom_idx in self.hydrogen_donor_match] + \\\n",
    "                   [atom_idx in self.acidic_match] + \\\n",
    "                   [atom_idx in self.basic_match] + \\\n",
    "                   [self.ring_info.IsAtomInRingOfSize(atom_idx, 3),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 4),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 5),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 6),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 7),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 8)]\n",
    "        return features\n",
    "\n",
    "    def bond_features(self, bond: Chem.rdchem.Bond\n",
    "                      ) -> List[Union[bool, int, float]]:\n",
    "        \"\"\"\n",
    "        Builds a feature vector for a bond.\n",
    "\n",
    "        :param bond: A RDKit bond.\n",
    "        :return: A list containing the bond features.\n",
    "        \"\"\"\n",
    "\n",
    "        if bond is None:\n",
    "            fbond = [1] + [0] * (BOND_FDIM - 1)\n",
    "        else:\n",
    "            bt = bond.GetBondType()\n",
    "            fbond = [\n",
    "                0,  # bond is not None\n",
    "                bt == Chem.rdchem.BondType.SINGLE,\n",
    "                bt == Chem.rdchem.BondType.DOUBLE,\n",
    "                bt == Chem.rdchem.BondType.TRIPLE,\n",
    "                bt == Chem.rdchem.BondType.AROMATIC,\n",
    "                (bond.GetIsConjugated() if bt is not None else 0),\n",
    "                (bond.IsInRing() if bt is not None else 0)\n",
    "            ]\n",
    "            fbond += onek_encoding_unk(int(bond.GetStereo()), list(range(6)))\n",
    "        return fbond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e402c-de38-4ac1-92b9-7a289d4e0d63",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 4-3-4-1-2. MolGraph 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bb0724b-2c47-4401-857a-4e405a01aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    bonds = mol.GetNumBonds()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    for ids in range( bonds ):\n",
    "        mol.GetBondWithIdx( ids ).SetProp( 'molBondMapNumber', str( mol.GetBondWithIdx( ids ).GetIdx() ) )\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5211314-df8a-4666-954a-2bad94ed44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'C(O)O'\n",
    "mol = Chem.MolFromSmiles(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e46f9979-9e27-4b9d-a824-602942a6c87c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c366e944fef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmolg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMolGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "molg = MolGraph(smiles, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d3c876c-b5ed-4ce1-8094-0693560d3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = mol.GetNumAtoms()\n",
    "bonds = mol.GetNumBonds()\n",
    "for idx in range( atoms ):\n",
    "    mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "for ids in range( bonds ):\n",
    "    mol.GetBondWithIdx( ids ).SetProp( 'molBondMapNumber', str( mol.GetBondWithIdx( ids ).GetIdx() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f82e3e-1fba-470b-8076-df4dff1c7195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Bond at 0x7f52cc305750>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol.GetBondWithIdx(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5faa064-43bc-42e0-bf1d-b3c633957005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f52cc305c30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol = Chem.MolFromSmiles(smiles)\n",
    "mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46b7ed62-2987-471c-88d7-2035bd7d9a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f52cc305ab0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chem.MolFromSmarts(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09f82ec4-ccdb-4d5a-8dba-68233a3bec7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f52cc305c30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol_with_atom_index(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea95560a-8588-4237-ad19-0421cc8dc23f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'molg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c82f869aec57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmolg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_bonds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'molg' is not defined"
     ]
    }
   ],
   "source": [
    "print(molg.f_bonds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f236b27e-4d16-4270-ab5f-e3ccbd99b3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol.GetNumAtoms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33f47228-05e6-4352-b5d6-3961033ec704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MolGraph at 0x7fa537e55890>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a271abb1-11b6-46da-956c-4c124c636e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrogen_donor = Chem.MolFromSmarts(\"[$([N;!H0;v3,v4&+1]),$([O,S;H1;+0]),n&H1&+0]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "caa09b78-e2a1-4eae-813a-737d6f50933f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mol.GetSubstructMatches(hydrogen_donor), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6a1afe4-8176-4cfe-8219-d333c5644319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C(O)O'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molg.smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59b03984-315a-4b0e-a55b-e8a006992a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molg.n_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11fab14c-4544-491a-856d-448b8c7b7fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molg.n_bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f558970-740b-4b61-80e0-86ba5cd1be83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(molg.f_atoms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47305072-ef03-4a47-8abd-0c1d5a0d43a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-a7c3eb8ffaf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# atom이 2까지만 있기에 더 있으면 오류가 뜸을 보여줌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmolg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_atoms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# atom이 2까지만 있기에 더 있으면 오류가 뜸을 보여줌\n",
    "print(molg.f_atoms[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14eb5154-2c69-45db-9aa4-ea3fd9a334e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(molg.f_bonds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03ac9e92-8d88-48db-96df-0888e52c6c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3], [0], [2]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molg.a2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edad1ad7-fcba-4461-afaa-fe5d0059970b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 2]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molg.b2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "645199b4-0085-4eb7-9a0f-9f9b75a1278d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 3, 2]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molg.b2revb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01427e6b-4494-4efc-a024-3c8a74d2ec17",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 4-3-4-1-3. BatchMolGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb061016-d6a9-4dcb-b3cf-617f3d2872e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMolGraph:\n",
    "    \"\"\"\n",
    "    A BatchMolGraph represents the graph structure and featurization of a batch of molecules.\n",
    "\n",
    "    A BatchMolGraph contains the attributes of a MolGraph plus:\n",
    "    - smiles_batch: A list of smiles strings.\n",
    "    - n_mols: The number of molecules in the batch.\n",
    "    - atom_fdim: The dimensionality of the atom features.\n",
    "    - bond_fdim: The dimensionality of the bond features (technically the combined atom/bond features).\n",
    "    - a_scope: A list of tuples indicating the start and end atom indices for each molecule.\n",
    "    - b_scope: A list of tuples indicating the start and end bond indices for each molecule.\n",
    "    - max_num_bonds: The maximum number of bonds neighboring an atom in this batch.\n",
    "    - b2b: (Optional) A mapping from a bond index to incoming bond indices.\n",
    "    - a2a: (Optional): A mapping from an atom index to neighboring atom indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mol_graphs: List[MolGraph], args: Namespace):\n",
    "        self.smiles_batch = [mol_graph.smiles for mol_graph in mol_graphs]\n",
    "        self.n_mols = len(self.smiles_batch)\n",
    "\n",
    "        self.atom_fdim = get_atom_fdim()\n",
    "        self.bond_fdim = get_bond_fdim() + self.atom_fdim\n",
    "\n",
    "        # Start n_atoms and n_bonds at 1 b/c zero padding\n",
    "        self.n_atoms = 1  # number of atoms (start at 1 b/c need index 0 as padding)\n",
    "        self.n_bonds = 1  # number of bonds (start at 1 b/c need index 0 as padding)\n",
    "        self.a_scope = []  # list of tuples indicating (start_atom_index, num_atoms) for each molecule\n",
    "        self.b_scope = []  # list of tuples indicating (start_bond_index, num_bonds) for each molecule\n",
    "\n",
    "        # All start with zero padding so that indexing with zero padding returns zeros\n",
    "        f_atoms = [[0] * self.atom_fdim]  # atom features\n",
    "        f_bonds = [[0] * self.bond_fdim]  # combined atom/bond features\n",
    "        a2b = [[]]  # mapping from atom index to incoming bond indices\n",
    "        b2a = [0]  # mapping from bond index to the index of the atom the bond is coming from\n",
    "        b2revb = [0]  # mapping from bond index to the index of the reverse bond\n",
    "\n",
    "        for mol_graph in mol_graphs:\n",
    "            f_atoms.extend(mol_graph.f_atoms)\n",
    "            f_bonds.extend(mol_graph.f_bonds)\n",
    "\n",
    "            for a in range(mol_graph.n_atoms):\n",
    "                a2b.append([b + self.n_bonds for b in mol_graph.a2b[a]])\n",
    "\n",
    "            for b in range(mol_graph.n_bonds):\n",
    "                b2a.append(self.n_atoms + mol_graph.b2a[b])\n",
    "                b2revb.append(self.n_bonds + mol_graph.b2revb[b])\n",
    "\n",
    "            self.a_scope.append((self.n_atoms, mol_graph.n_atoms))\n",
    "            self.b_scope.append((self.n_bonds, mol_graph.n_bonds))\n",
    "            self.n_atoms += mol_graph.n_atoms\n",
    "            self.n_bonds += mol_graph.n_bonds\n",
    "\n",
    "        # max with 1 to fix a crash in rare case of all single-heavy-atom mols\n",
    "        self.max_num_bonds = max(1, max(len(in_bonds) for in_bonds in a2b))\n",
    "\n",
    "        self.f_atoms = torch.FloatTensor(f_atoms)\n",
    "        self.f_bonds = torch.FloatTensor(f_bonds)\n",
    "        self.a2b = torch.LongTensor([a2b[a] + [0] * (self.max_num_bonds - len(a2b[a])) for a in range(self.n_atoms)])\n",
    "        self.b2a = torch.LongTensor(b2a)\n",
    "        self.b2revb = torch.LongTensor(b2revb)\n",
    "        self.b2b = None  # try to avoid computing b2b b/c O(n_atoms^3)\n",
    "        self.a2a = self.b2a[self.a2b]  # only needed if using atom messages\n",
    "        self.a_scope = torch.LongTensor(self.a_scope)\n",
    "        self.b_scope = torch.LongTensor(self.b_scope)\n",
    "\n",
    "    def set_new_atom_feature(self, f_atoms):\n",
    "        \"\"\"\n",
    "        Set the new atom feature. Do not update bond feature.\n",
    "        :param f_atoms:\n",
    "        \"\"\"\n",
    "        self.f_atoms = f_atoms\n",
    "\n",
    "    def get_components(self) -> Tuple[torch.FloatTensor, torch.FloatTensor,\n",
    "                                      torch.LongTensor, torch.LongTensor, torch.LongTensor,\n",
    "                                      List[Tuple[int, int]], List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Returns the components of the BatchMolGraph.\n",
    "\n",
    "        :return: A tuple containing PyTorch tensors with the atom features, bond features, and graph structure\n",
    "        and two lists indicating the scope of the atoms and bonds (i.e. which molecules they belong to).\n",
    "        \"\"\"\n",
    "        return self.f_atoms, self.f_bonds, self.a2b, self.b2a, self.b2revb, self.a_scope, self.b_scope, self.a2a\n",
    "\n",
    "    def get_b2b(self) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Computes (if necessary) and returns a mapping from each bond index to all the incoming bond indices.\n",
    "\n",
    "        :return: A PyTorch tensor containing the mapping from each bond index to all the incoming bond indices.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.b2b is None:\n",
    "            b2b = self.a2b[self.b2a]  # num_bonds x max_num_bonds\n",
    "            # b2b includes reverse edge for each bond so need to mask out\n",
    "            revmask = (b2b != self.b2revb.unsqueeze(1).repeat(1, b2b.size(1))).long()  # num_bonds x max_num_bonds\n",
    "            self.b2b = b2b * revmask\n",
    "\n",
    "        return self.b2b\n",
    "\n",
    "    def get_a2a(self) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Computes (if necessary) and returns a mapping from each atom index to all neighboring atom indices.\n",
    "\n",
    "        :return: A PyTorch tensor containing the mapping from each bond index to all the incodming bond indices.\n",
    "        \"\"\"\n",
    "        if self.a2a is None:\n",
    "            # b = a1 --> a2\n",
    "            # a2b maps a2 to all incoming bonds b\n",
    "            # b2a maps each bond b to the atom it comes from a1\n",
    "            # thus b2a[a2b] maps atom a2 to neighboring atoms a1\n",
    "            self.a2a = self.b2a[self.a2b]  # num_atoms x max_num_bonds\n",
    "\n",
    "        return self.a2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b10bd50f-81c0-4c32-9ef6-41659e67dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol2graph(smiles_batch: List[str], shared_dict,\n",
    "              args: Namespace) -> BatchMolGraph:\n",
    "    \"\"\"\n",
    "    Converts a list of SMILES strings to a BatchMolGraph containing the batch of molecular graphs.\n",
    "\n",
    "    :param smiles_batch: A list of SMILES strings.\n",
    "    :param args: Arguments.\n",
    "    :return: A BatchMolGraph containing the combined molecular graph for the molecules\n",
    "    \"\"\"\n",
    "    mol_graphs = []\n",
    "    for smiles in smiles_batch:\n",
    "        if smiles in shared_dict:\n",
    "            mol_graph = shared_dict[smiles]\n",
    "        else:\n",
    "            mol_graph = MolGraph(smiles, args)\n",
    "            if not args.no_cache:\n",
    "                shared_dict[smiles] = mol_graph\n",
    "        mol_graphs.append(mol_graph)\n",
    "\n",
    "    return BatchMolGraph(mol_graphs, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750122d-622f-42b6-9f76-1d1fb12905fd",
   "metadata": {},
   "source": [
    "##### 4-3-4-1-4. MolGraph에 3D 추가해보기\n",
    "- CHEM_RL의 코드는 pahelix/utils/compound_tools에 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d252ee2e-f621-497f-b7dc-4182d627f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdMolTransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5af1d4c-afd1-4d14-9807-1182251f7ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MolGraph_3D:\n",
    "    \"\"\"\n",
    "    A MolGraph represents the graph structure and featurization of a single molecule.\n",
    "\n",
    "    A MolGraph computes the following attributes:\n",
    "    - smiles: Smiles string.\n",
    "    - n_atoms: The number of atoms in the molecule.\n",
    "    - n_bonds: The number of bonds in the molecule.\n",
    "    - f_atoms: A mapping from an atom index to a list atom features.\n",
    "    - f_bonds: A mapping from a bond index to a list of bond features.\n",
    "    - a2b: A mapping from an atom index to a list of incoming bond indices.\n",
    "    - b2a: A mapping from a bond index to the index of the atom the bond originates from.\n",
    "    - b2revb: A mapping from a bond index to the index of the reverse bond.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smiles: str,  args: Namespace):\n",
    "        \"\"\"\n",
    "        Computes the graph structure and featurization of a molecule.\n",
    "\n",
    "        :param smiles: A smiles string.\n",
    "        :param args: Arguments.\n",
    "        \"\"\"\n",
    "        self.smiles = smiles\n",
    "        self.args = args\n",
    "        self.n_atoms = 0  # number of atoms\n",
    "        self.n_bonds = 0  # number of bonds\n",
    "        self.f_atoms = []  # mapping from atom index to atom features\n",
    "        self.f_bonds = []  # mapping from bond index to concat(in_atom, bond) features\n",
    "        self.a2b = []  # mapping from atom index to incoming bond indices\n",
    "        self.b2a = []  # mapping from bond index to the index of the atom the bond is coming from\n",
    "        self.b2revb = []  # mapping from bond index to the index of the reverse bond\n",
    "\n",
    "        # Convert smiles to molecule\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # for 3D conformer\n",
    "        mol_H = AllChem.AddHs(mol)\n",
    "        AllChem.EmbedMolecule(mol_H, randomSeed=args.seed)\n",
    "        self.conf = mol_H.GetConformer()\n",
    "        self.bond_3dlength = []  # 3D 결합에서의 길이\n",
    "        \n",
    "\n",
    "        self.hydrogen_donor = Chem.MolFromSmarts(\"[$([N;!H0;v3,v4&+1]),$([O,S;H1;+0]),n&H1&+0]\")\n",
    "        self.hydrogen_acceptor = Chem.MolFromSmarts(\n",
    "            \"[$([O,S;H1;v2;!$(*-*=[O,N,P,S])]),$([O,S;H0;v2]),$([O,S;-]),$([N;v3;!$(N-*=[O,N,P,S])]),\"\n",
    "            \"n&H0&+0,$([o,s;+0;!$([o,s]:n);!$([o,s]:c:n)])]\")\n",
    "        self.acidic = Chem.MolFromSmarts(\"[$([C,S](=[O,S,P])-[O;H1,-1])]\")\n",
    "        self.basic = Chem.MolFromSmarts(\n",
    "            \"[#7;+,$([N;H2&+0][$([C,a]);!$([C,a](=O))]),$([N;H1&+0]([$([C,a]);!$([C,a](=O))])[$([C,a]);\"\n",
    "            \"!$([C,a](=O))]),$([N;H0&+0]([C;!$(C(=O))])([C;!$(C(=O))])[C;!$(C(=O))])]\")\n",
    "\n",
    "        self.hydrogen_donor_match = sum(mol.GetSubstructMatches(self.hydrogen_donor), ())\n",
    "        self.hydrogen_acceptor_match = sum(mol.GetSubstructMatches(self.hydrogen_acceptor), ())\n",
    "        self.acidic_match = sum(mol.GetSubstructMatches(self.acidic), ())\n",
    "        self.basic_match = sum(mol.GetSubstructMatches(self.basic), ())\n",
    "        self.ring_info = mol.GetRingInfo()\n",
    "\n",
    "\n",
    "        # fake the number of \"atoms\" if we are collapsing substructures\n",
    "        self.n_atoms = mol.GetNumAtoms()\n",
    "\n",
    "        # Get atom features\n",
    "        for _, atom in enumerate(mol.GetAtoms()):\n",
    "            self.f_atoms.append(self.atom_features(atom))\n",
    "        self.f_atoms = [self.f_atoms[i] for i in range(self.n_atoms)]\n",
    "\n",
    "        for _ in range(self.n_atoms):\n",
    "            self.a2b.append([])\n",
    "\n",
    "        # Get bond features\n",
    "        for a1 in range(self.n_atoms):\n",
    "            for a2 in range(a1 + 1, self.n_atoms):\n",
    "                bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                if bond is None:\n",
    "                    bond_3dlength=[0]\n",
    "                    continue\n",
    "                else : bond_3dlength=[rdMolTransforms.GetBondLength(self.conf,a1,a2)]\n",
    "\n",
    "                if args.bond_drop_rate > 0:\n",
    "                    if np.random.binomial(1, args.bond_drop_rate):\n",
    "                        continue\n",
    "\n",
    "                f_bond = self.bond_features(bond)\n",
    "                \n",
    "                # for 3d test\n",
    "                self.bond_3dlength.append(bond_3dlength[0])\n",
    "\n",
    "                # Always treat the bond as directed.\n",
    "                self.f_bonds.append(self.f_atoms[a1] + f_bond + list(bond_3dlength))\n",
    "                self.f_bonds.append(self.f_atoms[a2] + f_bond + list(bond_3dlength))\n",
    "\n",
    "                # Update index mappings\n",
    "                b1 = self.n_bonds\n",
    "                b2 = b1 + 1\n",
    "                self.a2b[a2].append(b1)  # b1 = a1 --> a2\n",
    "                self.b2a.append(a1)\n",
    "                self.a2b[a1].append(b2)  # b2 = a2 --> a1\n",
    "                self.b2a.append(a2)\n",
    "                self.b2revb.append(b2)\n",
    "                self.b2revb.append(b1)\n",
    "                self.n_bonds += 2\n",
    "                \n",
    "\n",
    "    def atom_features(self, atom: Chem.rdchem.Atom) -> List[Union[bool, int, float]]:\n",
    "        \"\"\"\n",
    "        Builds a feature vector for an atom.\n",
    "\n",
    "        :param atom: An RDKit atom.\n",
    "        :param functional_groups: A k-hot vector indicating the functional groups the atom belongs to.\n",
    "        :return: A list containing the atom features.\n",
    "        \"\"\"\n",
    "        features = onek_encoding_unk(atom.GetAtomicNum() - 1, ATOM_FEATURES['atomic_num']) + \\\n",
    "                   onek_encoding_unk(atom.GetTotalDegree(), ATOM_FEATURES['degree']) + \\\n",
    "                   onek_encoding_unk(atom.GetFormalCharge(), ATOM_FEATURES['formal_charge']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetChiralTag()), ATOM_FEATURES['chiral_tag']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetTotalNumHs()), ATOM_FEATURES['num_Hs']) + \\\n",
    "                   onek_encoding_unk(int(atom.GetHybridization()), ATOM_FEATURES['hybridization']) + \\\n",
    "                   [1 if atom.GetIsAromatic() else 0] + \\\n",
    "                   [atom.GetMass() * 0.01]\n",
    "        atom_idx = atom.GetIdx()\n",
    "        features = features + \\\n",
    "                   onek_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6]) + \\\n",
    "                   [atom_idx in self.hydrogen_acceptor_match] + \\\n",
    "                   [atom_idx in self.hydrogen_donor_match] + \\\n",
    "                   [atom_idx in self.acidic_match] + \\\n",
    "                   [atom_idx in self.basic_match] + \\\n",
    "                   [self.ring_info.IsAtomInRingOfSize(atom_idx, 3),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 4),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 5),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 6),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 7),\n",
    "                    self.ring_info.IsAtomInRingOfSize(atom_idx, 8)]\n",
    "        return features\n",
    "\n",
    "    def bond_features(self, bond: Chem.rdchem.Bond\n",
    "                      ) -> List[Union[bool, int, float]]:\n",
    "        \"\"\"\n",
    "        Builds a feature vector for a bond.\n",
    "\n",
    "        :param bond: A RDKit bond.\n",
    "        :return: A list containing the bond features.\n",
    "        \"\"\"\n",
    "\n",
    "        if bond is None:\n",
    "            fbond = [1] + [0] * (BOND_FDIM - 1)\n",
    "        else:\n",
    "            bt = bond.GetBondType()\n",
    "            fbond = [\n",
    "                0,  # bond is not None\n",
    "                bt == Chem.rdchem.BondType.SINGLE,\n",
    "                bt == Chem.rdchem.BondType.DOUBLE,\n",
    "                bt == Chem.rdchem.BondType.TRIPLE,\n",
    "                bt == Chem.rdchem.BondType.AROMATIC,\n",
    "                (bond.GetIsConjugated() if bt is not None else 0),\n",
    "                (bond.IsInRing() if bt is not None else 0)\n",
    "            ]\n",
    "            fbond += onek_encoding_unk(int(bond.GetStereo()), list(range(6)))\n",
    "        return fbond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c852ea2-05ab-4b6f-95b0-4a858518dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol2graph_3D(smiles_batch: List[str], shared_dict,\n",
    "              args: Namespace) -> BatchMolGraph:\n",
    "    \"\"\"\n",
    "    Converts a list of SMILES strings to a BatchMolGraph containing the batch of molecular graphs.\n",
    "\n",
    "    :param smiles_batch: A list of SMILES strings.\n",
    "    :param args: Arguments.\n",
    "    :return: A BatchMolGraph containing the combined molecular graph for the molecules\n",
    "    \"\"\"\n",
    "    mol_graphs = []\n",
    "    for smiles in smiles_batch:\n",
    "        if smiles in shared_dict:\n",
    "            mol_graph = shared_dict[smiles]\n",
    "        else:\n",
    "            mol_graph = MolGraph_3D(smiles, args)\n",
    "            if not args.no_cache:\n",
    "                shared_dict[smiles] = mol_graph\n",
    "        mol_graphs.append(mol_graph)\n",
    "\n",
    "    return BatchMolGraph(mol_graphs, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde4c989-d8fd-40b6-8234-2123a76d0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'C(O)O'\n",
    "molg_3d = MolGraph_3D(smiles, args)\n",
    "mol_H = AllChem.AddHs(mol)\n",
    "AllChem.EmbedMolecule(mol_H, randomSeed=args.seed)\n",
    "conf = mol_H.GetConformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "417498c9-3d30-4c5e-b40b-379a653793bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f52cc31b570>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aef04230-4fa0-4e36-90ea-c2a1a748f4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MolGraph at 0x7fa537e55890>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea6dcba0-0291-4a78-9fda-8ae049160ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.12011, 0, 0, 1, 0, 0, 0, 0, 0, False, False, False, False, False, False, False, False, False, False, 0, True, False, False, False, False, False, 1, 0, 0, 0, 0, 0, 0, 1.370689712999252]\n"
     ]
    }
   ],
   "source": [
    "print(molg_3d.f_bonds[0]) # 이렇게 추가할 경우 나중에 코드에서 BOND_DIM의 길이를 나타내는 변수 수정해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "968427f0-df62-4f59-a81b-eb154a909d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.370689712999252, 1.3847488205134906]\n"
     ]
    }
   ],
   "source": [
    "print(molg_3d.bond_3dlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "16a121f5-1073-4b8c-8160-6628424e171d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11472145, -0.48179606, -0.02051577],\n",
       "       [-1.05627135,  0.49566355,  0.17143937],\n",
       "       [ 1.16222101, -0.00316533, -0.26105657],\n",
       "       [-0.13232037, -1.14276078,  0.88667448],\n",
       "       [-0.37316945, -1.17189252, -0.86473211],\n",
       "       [-0.75949145,  1.40440676, -0.03885919],\n",
       "       [ 1.27375307,  0.89954438,  0.12704979]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates = conf.GetPositions()\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0db7015-febd-407f-89cd-1811357eeb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11472145, -0.48179606, -0.02051577],\n",
       "       [-1.05627135,  0.49566355,  0.17143937],\n",
       "       [ 1.16222101, -0.00316533, -0.26105657],\n",
       "       [-0.13232037, -1.14276078,  0.88667448],\n",
       "       [-0.37316945, -1.17189252, -0.86473211],\n",
       "       [-0.75949145,  1.40440676, -0.03885919],\n",
       "       [ 1.27375307,  0.89954438,  0.12704979]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.GetPositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e59349d-1d9b-4a68-912c-6f92a19b7a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.370689712999252"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((coordinates[0][0]-coordinates[1][0])*(coordinates[0][0]-coordinates[1][0]) + (coordinates[0][1]-coordinates[1][1])*(coordinates[0][1]-coordinates[1][1]) + (coordinates[0][2]-coordinates[1][2])*(coordinates[0][2]-coordinates[1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06ff6599-d1e1-48d2-880e-be764001c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원 코드\n",
    "src_node_i = conf.GetPositions()[0]\n",
    "tar_node_j = conf.GetPositions()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "623ac8f7-c25f-4b8f-a3e5-d5180f75f862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3706897129992517"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([tar_node_j][0] - [src_node_i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0038b83b-2a95-42c3-b3db-c137dbb0ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDKit 제공 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bced6cac-0bbb-41b9-b2d3-a7d6891b9368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.370689712999252"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdMolTransforms.GetBondLength(conf,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85bfb44-ba0f-41c8-a445-ab0ec5ee429d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Molcollator 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae64d25f-15b1-411b-9773-78701dd9f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolCollator(object):\n",
    "    \"\"\"\n",
    "    Collator for pytorch dataloader\n",
    "    :param shared_dict: a shared dict of multiprocess.\n",
    "    :param args: Arguments.\n",
    "    \"\"\"\n",
    "    def __init__(self, shared_dict, args):\n",
    "        self.args = args\n",
    "        self.shared_dict = shared_dict\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles_batch = [d.smiles for d in batch]\n",
    "        features_batch = [d.features for d in batch]\n",
    "        target_batch = [d.targets for d in batch]\n",
    "        batch_mol_graph = mol2graph(smiles_batch, self.shared_dict, self.args)\n",
    "        batch = batch_mol_graph.get_components()\n",
    "        mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])\n",
    "        targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])\n",
    "        return smiles_batch, batch, features_batch, mask, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54992437-a5b2-452a-a95d-e5584a2db24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolCollator_3D(object):\n",
    "    \"\"\"\n",
    "    Collator for pytorch dataloader\n",
    "    :param shared_dict: a shared dict of multiprocess.\n",
    "    :param args: Arguments.\n",
    "    \"\"\"\n",
    "    def __init__(self, shared_dict, args):\n",
    "        self.args = args\n",
    "        self.shared_dict = shared_dict\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles_batch = [d.smiles for d in batch]\n",
    "        features_batch = [d.features for d in batch]\n",
    "        target_batch = [d.targets for d in batch]\n",
    "        batch_mol_graph = mol2graph_3D(smiles_batch, self.shared_dict, self.args)\n",
    "        batch = batch_mol_graph.get_components()\n",
    "        mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])\n",
    "        targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])\n",
    "        return smiles_batch, batch, features_batch, mask, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec8950-ff98-42ac-bc11-ab02cdc88d58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4-3-3. DataLoader()\n",
    "- 이건 torch에 있는 dataloader이다... 근데 collate_fn=MolCollator가 정확하게 어떻게 작동되는지 모르겠다.\n",
    "- collate_fn (callable, optional): merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset.\n",
    "- 맵 스타일의 데이터셋으로부터 배치를 불러들이고, 데이터셋 사이즈를 맞추기 위해? 쓰인다는데,,, 흠,,,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8139b-3d62-4294-91ad-9faeed707beb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### default.collatefn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56b64308-bbcf-4959-b114-39065c5e5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_collate(batch):\n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum(x.numel() for x in batch)\n",
    "            storage = elem.storage()._new_shared(numel)\n",
    "            out = elem.new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
    "\n",
    "            return default_collate([torch.as_tensor(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
    "        return elem_type(*(default_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [default_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError(default_collate_err_msg_format.format(elem_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965f573-a31a-48b5-8e8f-6aa9c1605fc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4-3-3-1 DataLoader 관련 torch 코드들"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bf575b1-aed6-4218-a0df-9eef0bf441d2",
   "metadata": {},
   "source": [
    "        train_data = DataLoader(train_data,\n",
    "                                batch_size=args.batch_size,\n",
    "                                shuffle=shuffle,\n",
    "                                num_workers=10,\n",
    "                                collate_fn=mol_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "238e5ccd-94eb-4671-8deb-05890fb734e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Any, Callable, TypeVar, Generic, Sequence, List, Optional, Sized\n",
    "T_co = TypeVar('T_co', covariant=True)\n",
    "T = TypeVar('T')\n",
    "_worker_init_fn_t = Callable[[int], None]\n",
    "_collate_fn_t = Callable[[List[T]], Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96de695e-1488-4008-902c-d9458c9f1cbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.utils.data._typing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-806881484a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_DataPipeMeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.utils.data._typing'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import _utils\n",
    "from torch.utils.data import IterableDataset, Sampler, SequentialSampler, RandomSampler, BatchSampler, Dataset\n",
    "import multiprocessing as python_multiprocessing\n",
    "import torch\n",
    "import torch.multiprocessing as multiprocessing\n",
    "from torch._utils import ExceptionWrapper\n",
    "from torch._six import string_classes\n",
    "from torch.utils.data._typing import _DataPipeMeta\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import itertools\n",
    "import warnings\n",
    "import queue\n",
    "from typing import Any, Callable, TypeVar, Generic, Sequence, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34681f61-1baa-4bad-816d-4cb0663fdd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DatasetKind(object):\n",
    "    Map = 0\n",
    "    Iterable = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def create_fetcher(kind, dataset, auto_collation, collate_fn, drop_last):\n",
    "        if kind == _DatasetKind.Map:\n",
    "            return _utils.fetch._MapDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)\n",
    "        else:\n",
    "            return _utils.fetch._IterableDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "846ab626-1371-4baf-9ac7-d291afe1bfc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-395c9648f25e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_BaseDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-395c9648f25e>\u001b[0m in \u001b[0;36m_BaseDataLoaderIter\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_BaseDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "class _BaseDataLoaderIter(object):\n",
    "    def __init__(self, loader: DataLoader) -> None:\n",
    "        self._dataset = loader.dataset\n",
    "        self._dataset_kind = loader._dataset_kind\n",
    "        self._IterableDataset_len_called = loader._IterableDataset_len_called\n",
    "        self._auto_collation = loader._auto_collation\n",
    "        self._drop_last = loader.drop_last\n",
    "        self._index_sampler = loader._index_sampler\n",
    "        self._num_workers = loader.num_workers\n",
    "        self._prefetch_factor = loader.prefetch_factor\n",
    "        self._pin_memory = loader.pin_memory and torch.cuda.is_available()\n",
    "        self._timeout = loader.timeout\n",
    "        self._collate_fn = loader.collate_fn\n",
    "        self._sampler_iter = iter(self._index_sampler)\n",
    "        self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n",
    "        self._persistent_workers = loader.persistent_workers\n",
    "        self._num_yielded = 0\n",
    "        self._profile_name = \"enumerate(DataLoader)#{}.__next__\".format(self.__class__.__name__)\n",
    "\n",
    "    def __iter__(self) -> '_BaseDataLoaderIter':\n",
    "        return self\n",
    "\n",
    "    def _reset(self, loader, first_iter=False):\n",
    "        self._sampler_iter = iter(self._index_sampler)\n",
    "        self._num_yielded = 0\n",
    "        self._IterableDataset_len_called = loader._IterableDataset_len_called\n",
    "\n",
    "    def _next_index(self):\n",
    "        return next(self._sampler_iter)  # may raise StopIteration\n",
    "\n",
    "    def _next_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __next__(self) -> Any:\n",
    "        with torch.autograd.profiler.record_function(self._profile_name):\n",
    "            if self._sampler_iter is None:\n",
    "                self._reset()\n",
    "            data = self._next_data()\n",
    "            self._num_yielded += 1\n",
    "            if self._dataset_kind == _DatasetKind.Iterable and \\\n",
    "                    self._IterableDataset_len_called is not None and \\\n",
    "                    self._num_yielded > self._IterableDataset_len_called:\n",
    "                warn_msg = (\"Length of IterableDataset {} was reported to be {} (when accessing len(dataloader)), but {} \"\n",
    "                            \"samples have been fetched. \").format(self._dataset, self._IterableDataset_len_called,\n",
    "                                                                  self._num_yielded)\n",
    "                if self._num_workers > 0:\n",
    "                    warn_msg += (\"For multiprocessing data-loading, this could be caused by not properly configuring the \"\n",
    "                                 \"IterableDataset replica at each worker. Please see \"\n",
    "                                 \"https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\")\n",
    "                warnings.warn(warn_msg)\n",
    "            return data\n",
    "\n",
    "    next = __next__  # Python 2 compatibility\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._index_sampler)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # TODO: add limited pickling support for sharing an iterator\n",
    "        # across multiple threads for HOGWILD.\n",
    "        # Probably the best way to do this is by moving the sample pushing\n",
    "        # to a separate thread and then just sharing the data queue\n",
    "        # but signalling the end is tricky without a non-blocking API\n",
    "        raise NotImplementedError(\"{} cannot be pickled\", self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c095b04-46e7-4eb4-9ed6-e19bd4c30c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_BaseDataLoaderIter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2a71aeb976a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseDataLoaderIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34mr\"\"\"Iterates once over the DataLoader's dataset, as specified by the sampler\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# NOTE [ Data Loader Multiprocessing Shutdown Logic ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_BaseDataLoaderIter' is not defined"
     ]
    }
   ],
   "source": [
    "class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):\n",
    "    r\"\"\"Iterates once over the DataLoader's dataset, as specified by the sampler\"\"\"\n",
    "\n",
    "    # NOTE [ Data Loader Multiprocessing Shutdown Logic ]\n",
    "    #\n",
    "    # Preliminary:\n",
    "    #\n",
    "    # Our data model looks like this (queues are indicated with curly brackets):\n",
    "    #\n",
    "    #                main process                              ||\n",
    "    #                     |                                    ||\n",
    "    #               {index_queue}                              ||\n",
    "    #                     |                                    ||\n",
    "    #              worker processes                            ||     DATA\n",
    "    #                     |                                    ||\n",
    "    #            {worker_result_queue}                         ||     FLOW\n",
    "    #                     |                                    ||\n",
    "    #      pin_memory_thread of main process                   ||   DIRECTION\n",
    "    #                     |                                    ||\n",
    "    #               {data_queue}                               ||\n",
    "    #                     |                                    ||\n",
    "    #                data output                               \\/\n",
    "    #\n",
    "    # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if\n",
    "    #      `pin_memory=False`.\n",
    "    #\n",
    "    #\n",
    "    # Terminating multiprocessing logic requires very careful design. In\n",
    "    # particular, we need to make sure that\n",
    "    #\n",
    "    #   1. The iterator gracefully exits the workers when its last reference is\n",
    "    #      gone or it is depleted.\n",
    "    #\n",
    "    #      In this case, the workers should be gracefully exited because the\n",
    "    #      main process may still need to continue to run, and we want cleaning\n",
    "    #      up code in the workers to be executed (e.g., releasing GPU memory).\n",
    "    #      Naturally, we implement the shutdown logic in `__del__` of\n",
    "    #      DataLoaderIterator.\n",
    "    #\n",
    "    #      We delay the discussion on the logic in this case until later.\n",
    "    #\n",
    "    #   2. The iterator exits the workers when the loader process and/or worker\n",
    "    #      processes exits normally or with error.\n",
    "    #\n",
    "    #      We set all workers and `pin_memory_thread` to have `daemon=True`.\n",
    "    #\n",
    "    #      You may ask, why can't we make the workers non-daemonic, and\n",
    "    #      gracefully exit using the same logic as we have in `__del__` when the\n",
    "    #      iterator gets deleted (see 1 above)?\n",
    "    #\n",
    "    #      First of all, `__del__` is **not** guaranteed to be called when\n",
    "    #      interpreter exits. Even if it is called, by the time it executes,\n",
    "    #      many Python core library resources may alreay be freed, and even\n",
    "    #      simple things like acquiring an internal lock of a queue may hang.\n",
    "    #      Therefore, in this case, we actually need to prevent `__del__` from\n",
    "    #      being executed, and rely on the automatic termination of daemonic\n",
    "    #      children.\n",
    "    #\n",
    "    #      Thus, we register an `atexit` hook that sets a global flag\n",
    "    #      `_utils.python_exit_status`. Since `atexit` hooks are executed in the\n",
    "    #      reverse order of registration, we are guaranteed that this flag is\n",
    "    #      set before library resources we use are freed (which, at least in\n",
    "    #      CPython, is done via an `atexit` handler defined in\n",
    "    #      `multiprocessing/util.py`\n",
    "    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/util.py#L320-L362\n",
    "    #      registered when an object requiring this mechanism is first\n",
    "    #      created, e.g., `mp.Queue`\n",
    "    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/context.py#L100-L103\n",
    "    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/queues.py#L29\n",
    "    #      )\n",
    "    #\n",
    "    #      So in `__del__`, we check if `_utils.python_exit_status` is set or\n",
    "    #      `None` (freed), and perform no-op if so.\n",
    "    #\n",
    "    #      However, simply letting library clean-up codes run can also be bad,\n",
    "    #      because such codes (i.e., `multiprocessing.util._exit_function()`)\n",
    "    #      include join putting threads for `mp.Queue`, which can be blocking.\n",
    "    #      Hence, the main process putting threads are called with\n",
    "    #      `cancel_join_thread` at creation.  See later section\n",
    "    #      [ 3b. A process won't hang when putting into a queue; ]\n",
    "    #      for more details.\n",
    "    #\n",
    "    #      Here are two example cases where library clean-up codes can run\n",
    "    #      before `__del__` is called:\n",
    "    #\n",
    "    #        1. If we hold onto a reference to the iterator, it more often\n",
    "    #           than not tries to do `multiprocessing` library cleaning before\n",
    "    #           clearing the alive referenced objects (https://github.com/pytorch/pytorch/issues/48666)\n",
    "    #           and thus prevents our cleaning-up code to run first.\n",
    "    #\n",
    "    #        2. A similar issue araises when a `DataLoader` is used in a subprocess.\n",
    "    #           When a process ends, it shuts the all its daemonic children\n",
    "    #           down with a SIGTERM (instead of joining them without a timeout).\n",
    "    #           Simiarly for threads, but by a different mechanism. This fact,\n",
    "    #           together with a few implementation details of multiprocessing, forces\n",
    "    #           us to make workers daemonic. All of our problems arise when a\n",
    "    #           DataLoader is used in a subprocess, and are caused by multiprocessing\n",
    "    #           code which looks more or less like this:\n",
    "    #\n",
    "    #               try:\n",
    "    #                   your_function_using_a_dataloader()\n",
    "    #               finally:\n",
    "    #                   multiprocessing.util._exit_function()\n",
    "    #\n",
    "    #           The joining/termination mentioned above happens inside\n",
    "    #           `_exit_function()`. Now, if `your_function_using_a_dataloader()`\n",
    "    #           throws, the stack trace stored in the exception will prevent the\n",
    "    #           frame which uses `DataLoaderIter` to be freed. If the frame has any\n",
    "    #           reference to the `DataLoaderIter` (e.g., in a method of the iter),\n",
    "    #           its  `__del__`, which starts the shutdown procedure, will not be\n",
    "    #           called. That, in turn, means that workers aren't notified. Attempting\n",
    "    #           to join in `_exit_function` will then result in a hang.\n",
    "    #\n",
    "    #           For context, `_exit_function` is also registered as an `atexit` call.\n",
    "    #           So it is unclear to me (@ssnl) why this is needed in a finally block.\n",
    "    #           The code dates back to 2008 and there is no comment on the original\n",
    "    #           PEP 371 or patch https://bugs.python.org/issue3050 (containing both\n",
    "    #           the finally block and the `atexit` registration) that explains this.\n",
    "    #\n",
    "    #\n",
    "    #      Finally, another choice is to just shutdown workers with logic in 1\n",
    "    #      above whenever we see an error in `next`. This isn't ideal because\n",
    "    #        a. It prevents users from using try-catch to resume data loading.\n",
    "    #        b. It doesn't prevent hanging if users have references to the\n",
    "    #           iterator.\n",
    "    #\n",
    "    #   3. All processes exit if any of them die unexpectedly by fatal signals.\n",
    "    #\n",
    "    #      As shown above, the workers are set as daemonic children of the main\n",
    "    #      process. However, automatic cleaning-up of such child processes only\n",
    "    #      happens if the parent process exits gracefully (e.g., not via fatal\n",
    "    #      signals like SIGKILL). So we must ensure that each process will exit\n",
    "    #      even the process that should send/receive data to/from it were\n",
    "    #      killed, i.e.,\n",
    "    #\n",
    "    #        a. A process won't hang when getting from a queue.\n",
    "    #\n",
    "    #           Even with carefully designed data dependencies (i.e., a `put()`\n",
    "    #           always corresponding to a `get()`), hanging on `get()` can still\n",
    "    #           happen when data in queue is corrupted (e.g., due to\n",
    "    #           `cancel_join_thread` or unexpected exit).\n",
    "    #\n",
    "    #           For child exit, we set a timeout whenever we try to get data\n",
    "    #           from `data_queue`, and check the workers' status on each timeout\n",
    "    #           and error.\n",
    "    #           See `_DataLoaderiter._get_batch()` and\n",
    "    #           `_DataLoaderiter._try_get_data()` for details.\n",
    "    #\n",
    "    #           Additionally, for child exit on non-Windows platforms, we also\n",
    "    #           register a SIGCHLD handler (which is supported on Windows) on\n",
    "    #           the main process, which checks if any of the workers fail in the\n",
    "    #           (Python) handler. This is more efficient and faster in detecting\n",
    "    #           worker failures, compared to only using the above mechanism.\n",
    "    #           See `DataLoader.cpp` and `_utils/signal_handling.py` for details.\n",
    "    #\n",
    "    #           For `.get()` calls where the sender(s) is not the workers, we\n",
    "    #           guard them with timeouts, and check the status of the sender\n",
    "    #           when timeout happens:\n",
    "    #             + in the workers, the `_utils.worker.ManagerWatchdog` class\n",
    "    #               checks the status of the main process.\n",
    "    #             + if `pin_memory=True`, when getting from `pin_memory_thread`,\n",
    "    #               check `pin_memory_thread` status periodically until `.get()`\n",
    "    #               returns or see that `pin_memory_thread` died.\n",
    "    #\n",
    "    #        b. A process won't hang when putting into a queue;\n",
    "    #\n",
    "    #           We use `mp.Queue` which has a separate background thread to put\n",
    "    #           objects from an unbounded buffer array. The background thread is\n",
    "    #           daemonic and usually automatically joined when the process\n",
    "    #           *exits*.\n",
    "    #\n",
    "    #           In case that the receiver has ended abruptly while\n",
    "    #           reading from the pipe, the join will hang forever.  The usual\n",
    "    #           solution for this in Python is calling  `q.cancel_join_thread`,\n",
    "    #           which prevents automatically joining it when finalizing\n",
    "    #           (exiting).\n",
    "    #\n",
    "    #           Nonetheless, `cancel_join_thread` must only be called when the\n",
    "    #           queue is **not** going to be read from or write into by another\n",
    "    #           process, because it may hold onto a lock or leave corrupted data\n",
    "    #           in the queue, leading other readers/writers to hang.\n",
    "    #\n",
    "    #           Hence,\n",
    "    #             + For worker processes, we only do so (for their output\n",
    "    #               queues, i.e., `worker_result_queue`) before exiting.\n",
    "    #             + For `pin_memory_thread`, its output queue `data_queue` is a\n",
    "    #               `queue.Queue` that does blocking `put` if the queue is full.\n",
    "    #               So there is no above problem, but as a result, in\n",
    "    #               `_pin_memory_loop`, we do need to  wrap the `put` in a loop\n",
    "    #               that breaks not only upon success, but also when the main\n",
    "    #               process stops reading, i.e., is shutting down.\n",
    "    #             + For loader process, we `cancel_join_thread()` for all\n",
    "    #               `_index_queues` because the whole purpose of workers and\n",
    "    #               `pin_memory_thread` is to serve the loader process.  If\n",
    "    #               loader process is already exiting, we don't really care if\n",
    "    #               the queues are corrupted.\n",
    "    #\n",
    "    #\n",
    "    # Now let's get back to 1:\n",
    "    #   how we gracefully exit the workers when the last reference to the\n",
    "    #   iterator is gone.\n",
    "    #\n",
    "    # To achieve this, we implement the following logic along with the design\n",
    "    # choices mentioned above:\n",
    "    #\n",
    "    # `workers_done_event`:\n",
    "    #   A `multiprocessing.Event` shared among the main process and all worker\n",
    "    #   processes. This is used to signal the workers that the iterator is\n",
    "    #   shutting down. After it is set, they will not send processed data to\n",
    "    #   queues anymore, and only wait for the final `None` before exiting.\n",
    "    #   `done_event` isn't strictly needed. I.e., we can just check for `None`\n",
    "    #   from the input queue, but it allows us to skip wasting resources\n",
    "    #   processing data if we are already shutting down.\n",
    "    #\n",
    "    # `pin_memory_thread_done_event`:\n",
    "    #   A `threading.Event` for a similar purpose to that of\n",
    "    #   `workers_done_event`, but is for the `pin_memory_thread`. The reason\n",
    "    #   that separate events are needed is that `pin_memory_thread` reads from\n",
    "    #   the output queue of the workers. But the workers, upon seeing that\n",
    "    #   `workers_done_event` is set, only wants to see the final `None`, and is\n",
    "    #   not required to flush all data in the output queue (e.g., it may call\n",
    "    #   `cancel_join_thread` on that queue if its `IterableDataset` iterator\n",
    "    #   happens to exhaust coincidentally, which is out of the control of the\n",
    "    #   main process). Thus, since we will exit `pin_memory_thread` before the\n",
    "    #   workers (see below), two separete events are used.\n",
    "    #\n",
    "    # NOTE: In short, the protocol is that the main process will set these\n",
    "    #       `done_event`s and then the corresponding processes/threads a `None`,\n",
    "    #       and that they may exit at any time after receiving the `None`.\n",
    "    #\n",
    "    # NOTE: Using `None` as the final signal is valid, since normal data will\n",
    "    #       always be a 2-tuple with the 1st element being the index of the data\n",
    "    #       transferred (different from dataset index/key), and the 2nd being\n",
    "    #       either the dataset key or the data sample (depending on which part\n",
    "    #       of the data model the queue is at).\n",
    "    #\n",
    "    # [ worker processes ]\n",
    "    #   While loader process is alive:\n",
    "    #     Get from `index_queue`.\n",
    "    #       If get anything else,\n",
    "    #          Check `workers_done_event`.\n",
    "    #            If set, continue to next iteration\n",
    "    #                    i.e., keep getting until see the `None`, then exit.\n",
    "    #            Otherwise, process data:\n",
    "    #                If is fetching from an `IterableDataset` and the iterator\n",
    "    #                    is exhausted, send an `_IterableDatasetStopIteration`\n",
    "    #                    object to signal iteration end. The main process, upon\n",
    "    #                    receiving such an object, will send `None` to this\n",
    "    #                    worker and not use the corresponding `index_queue`\n",
    "    #                    anymore.\n",
    "    #       If timed out,\n",
    "    #          No matter `workers_done_event` is set (still need to see `None`)\n",
    "    #          or not, must continue to next iteration.\n",
    "    #   (outside loop)\n",
    "    #   If `workers_done_event` is set,  (this can be False with `IterableDataset`)\n",
    "    #     `data_queue.cancel_join_thread()`.  (Everything is ending here:\n",
    "    #                                          main process won't read from it;\n",
    "    #                                          other workers will also call\n",
    "    #                                          `cancel_join_thread`.)\n",
    "    #\n",
    "    # [ pin_memory_thread ]\n",
    "    #   # No need to check main thread. If this thread is alive, the main loader\n",
    "    #   # thread must be alive, because this thread is set as daemonic.\n",
    "    #   While `pin_memory_thread_done_event` is not set:\n",
    "    #     Get from `index_queue`.\n",
    "    #       If timed out, continue to get in the next iteration.\n",
    "    #       Otherwise, process data.\n",
    "    #       While `pin_memory_thread_done_event` is not set:\n",
    "    #         Put processed data to `data_queue` (a `queue.Queue` with blocking put)\n",
    "    #         If timed out, continue to put in the next iteration.\n",
    "    #         Otherwise, break, i.e., continuing to the out loop.\n",
    "    #\n",
    "    #   NOTE: we don't check the status of the main thread because\n",
    "    #           1. if the process is killed by fatal signal, `pin_memory_thread`\n",
    "    #              ends.\n",
    "    #           2. in other cases, either the cleaning-up in __del__ or the\n",
    "    #              automatic exit of daemonic thread will take care of it.\n",
    "    #              This won't busy-wait either because `.get(timeout)` does not\n",
    "    #              busy-wait.\n",
    "    #\n",
    "    # [ main process ]\n",
    "    #   In the DataLoader Iter's `__del__`\n",
    "    #     b. Exit `pin_memory_thread`\n",
    "    #          i.   Set `pin_memory_thread_done_event`.\n",
    "    #          ii   Put `None` in `worker_result_queue`.\n",
    "    #          iii. Join the `pin_memory_thread`.\n",
    "    #          iv.  `worker_result_queue.cancel_join_thread()`.\n",
    "    #\n",
    "    #     c. Exit the workers.\n",
    "    #          i.   Set `workers_done_event`.\n",
    "    #          ii.  Put `None` in each worker's `index_queue`.\n",
    "    #          iii. Join the workers.\n",
    "    #          iv.  Call `.cancel_join_thread()` on each worker's `index_queue`.\n",
    "    #\n",
    "    #        NOTE: (c) is better placed after (b) because it may leave corrupted\n",
    "    #              data in `worker_result_queue`, which `pin_memory_thread`\n",
    "    #              reads from, in which case the `pin_memory_thread` can only\n",
    "    #              happen at timeing out, which is slow. Nonetheless, same thing\n",
    "    #              happens if a worker is killed by signal at unfortunate times,\n",
    "    #              but in other cases, we are better off having a non-corrupted\n",
    "    #              `worker_result_queue` for `pin_memory_thread`.\n",
    "    #\n",
    "    #   NOTE: If `pin_memory=False`, there is no `pin_memory_thread` and (b)\n",
    "    #         can be omitted\n",
    "    #\n",
    "    # NB: `done_event`s isn't strictly needed. E.g., we can just check for\n",
    "    #     `None` from `index_queue`, but it allows us to skip wasting resources\n",
    "    #     processing indices already in `index_queue` if we are already shutting\n",
    "    #     down.\n",
    "\n",
    "    def __init__(self, loader):\n",
    "        super(_MultiProcessingDataLoaderIter, self).__init__(loader)\n",
    "\n",
    "        assert self._num_workers > 0\n",
    "        assert self._prefetch_factor > 0\n",
    "\n",
    "        if loader.multiprocessing_context is None:\n",
    "            multiprocessing_context = multiprocessing\n",
    "        else:\n",
    "            multiprocessing_context = loader.multiprocessing_context\n",
    "\n",
    "        self._worker_init_fn = loader.worker_init_fn\n",
    "        self._worker_queue_idx_cycle = itertools.cycle(range(self._num_workers))\n",
    "        # No certainty which module multiprocessing_context is\n",
    "        self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]\n",
    "        self._worker_pids_set = False\n",
    "        self._shutdown = False\n",
    "        self._workers_done_event = multiprocessing_context.Event()\n",
    "\n",
    "        self._index_queues = []\n",
    "        self._workers = []\n",
    "        for i in range(self._num_workers):\n",
    "            # No certainty which module multiprocessing_context is\n",
    "            index_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]\n",
    "            # Need to `cancel_join_thread` here!\n",
    "            # See sections (2) and (3b) above.\n",
    "            index_queue.cancel_join_thread()\n",
    "            w = multiprocessing_context.Process(\n",
    "                target=_utils.worker._worker_loop,\n",
    "                args=(self._dataset_kind, self._dataset, index_queue,\n",
    "                      self._worker_result_queue, self._workers_done_event,\n",
    "                      self._auto_collation, self._collate_fn, self._drop_last,\n",
    "                      self._base_seed, self._worker_init_fn, i, self._num_workers,\n",
    "                      self._persistent_workers))\n",
    "            w.daemon = True\n",
    "            # NB: Process.start() actually take some time as it needs to\n",
    "            #     start a process and pass the arguments over via a pipe.\n",
    "            #     Therefore, we only add a worker to self._workers list after\n",
    "            #     it started, so that we do not call .join() if program dies\n",
    "            #     before it starts, and __del__ tries to join but will get:\n",
    "            #     AssertionError: can only join a started process.\n",
    "            w.start()\n",
    "            self._index_queues.append(index_queue)\n",
    "            self._workers.append(w)\n",
    "\n",
    "        if self._pin_memory:\n",
    "            self._pin_memory_thread_done_event = threading.Event()\n",
    "\n",
    "            # Queue is not type-annotated\n",
    "            self._data_queue = queue.Queue()  # type: ignore[var-annotated]\n",
    "            pin_memory_thread = threading.Thread(\n",
    "                target=_utils.pin_memory._pin_memory_loop,\n",
    "                args=(self._worker_result_queue, self._data_queue,\n",
    "                      torch.cuda.current_device(),\n",
    "                      self._pin_memory_thread_done_event))\n",
    "            pin_memory_thread.daemon = True\n",
    "            pin_memory_thread.start()\n",
    "            # Similar to workers (see comment above), we only register\n",
    "            # pin_memory_thread once it is started.\n",
    "            self._pin_memory_thread = pin_memory_thread\n",
    "        else:\n",
    "            self._data_queue = self._worker_result_queue\n",
    "\n",
    "        # .pid can be None only before process is spawned (not the case, so ignore)\n",
    "        _utils.signal_handling._set_worker_pids(id(self), tuple(w.pid for w in self._workers))  # type: ignore[misc]\n",
    "        _utils.signal_handling._set_SIGCHLD_handler()\n",
    "        self._worker_pids_set = True\n",
    "        self._reset(loader, first_iter=True)\n",
    "\n",
    "    def _reset(self, loader, first_iter=False):\n",
    "        super()._reset(loader, first_iter)\n",
    "        self._send_idx = 0  # idx of the next task to be sent to workers\n",
    "        self._rcvd_idx = 0  # idx of the next task to be returned in __next__\n",
    "        # information about data not yet yielded, i.e., tasks w/ indices in range [rcvd_idx, send_idx).\n",
    "        # map: task idx => - (worker_id,)        if data isn't fetched (outstanding)\n",
    "        #                  \\ (worker_id, data)   if data is already fetched (out-of-order)\n",
    "        self._task_info = {}\n",
    "        self._tasks_outstanding = 0  # always equal to count(v for v in task_info.values() if len(v) == 1)\n",
    "        # A list of booleans representing whether each worker still has work to\n",
    "        # do, i.e., not having exhausted its iterable dataset object. It always\n",
    "        # contains all `True`s if not using an iterable-style dataset\n",
    "        # (i.e., if kind != Iterable).\n",
    "        # Not that this indicates that a worker still has work to do *for this epoch*.\n",
    "        # It does not mean that a worker is dead. In case of `_persistent_workers`,\n",
    "        # the worker will be reset to available in the next epoch.\n",
    "        self._workers_status = [True for i in range(self._num_workers)]\n",
    "        # We resume the prefetching in case it was enabled\n",
    "        if not first_iter:\n",
    "            for idx in range(self._num_workers):\n",
    "                self._index_queues[idx].put(_utils.worker._ResumeIteration())\n",
    "            resume_iteration_cnt = self._num_workers\n",
    "            while resume_iteration_cnt > 0:\n",
    "                return_idx, return_data = self._get_data()\n",
    "                if isinstance(return_idx, _utils.worker._ResumeIteration):\n",
    "                    assert return_data is None\n",
    "                    resume_iteration_cnt -= 1\n",
    "        # prime the prefetch loop\n",
    "        for _ in range(self._prefetch_factor * self._num_workers):\n",
    "            self._try_put_index()\n",
    "\n",
    "    def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n",
    "        # Tries to fetch data from `self._data_queue` once for a given timeout.\n",
    "        # This can also be used as inner loop of fetching without timeout, with\n",
    "        # the sender status as the loop condition.\n",
    "        #\n",
    "        # This raises a `RuntimeError` if any worker died expectedly. This error\n",
    "        # can come from either the SIGCHLD handler in `_utils/signal_handling.py`\n",
    "        # (only for non-Windows platforms), or the manual check below on errors\n",
    "        # and timeouts.\n",
    "        #\n",
    "        # Returns a 2-tuple:\n",
    "        #   (bool: whether successfully get data, any: data if successful else None)\n",
    "        try:\n",
    "            data = self._data_queue.get(timeout=timeout)\n",
    "            return (True, data)\n",
    "        except Exception as e:\n",
    "            # At timeout and error, we manually check whether any worker has\n",
    "            # failed. Note that this is the only mechanism for Windows to detect\n",
    "            # worker failures.\n",
    "            failed_workers = []\n",
    "            for worker_id, w in enumerate(self._workers):\n",
    "                if self._workers_status[worker_id] and not w.is_alive():\n",
    "                    failed_workers.append(w)\n",
    "                    self._mark_worker_as_unavailable(worker_id)\n",
    "            if len(failed_workers) > 0:\n",
    "                pids_str = ', '.join(str(w.pid) for w in failed_workers)\n",
    "                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n",
    "            if isinstance(e, queue.Empty):\n",
    "                return (False, None)\n",
    "            import tempfile\n",
    "            import errno\n",
    "            try:\n",
    "                # Raise an exception if we are this close to the FDs limit.\n",
    "                # Apparently, trying to open only one file is not a sufficient\n",
    "                # test.\n",
    "                # See NOTE [ DataLoader on Linux and open files limit ]\n",
    "                fds_limit_margin = 10\n",
    "                fs = [tempfile.NamedTemporaryFile() for i in range(fds_limit_margin)]\n",
    "            except OSError as e:\n",
    "                if e.errno == errno.EMFILE:\n",
    "                    raise RuntimeError(\n",
    "                        \"Too many open files. Communication with the\"\n",
    "                        \" workers is no longer possible. Please increase the\"\n",
    "                        \" limit using `ulimit -n` in the shell or change the\"\n",
    "                        \" sharing strategy by calling\"\n",
    "                        \" `torch.multiprocessing.set_sharing_strategy('file_system')`\"\n",
    "                        \" at the beginning of your code\") from None\n",
    "            raise\n",
    "\n",
    "# NOTE [ DataLoader on Linux and open files limit ]\n",
    "#\n",
    "# On Linux when DataLoader is used with multiprocessing we pass the data between\n",
    "# the root process and the workers through SHM files. We remove those files from\n",
    "# the filesystem as soon as they are created and keep them alive by\n",
    "# passing around their file descriptors through AF_UNIX sockets. (See\n",
    "# docs/source/multiprocessing.rst and 'Multiprocessing Technical Notes` in\n",
    "# the wiki (https://github.com/pytorch/pytorch/wiki).)\n",
    "#\n",
    "# This sometimes leads us to exceeding the open files limit. When that happens,\n",
    "# and the offending file descriptor is coming over a socket, the `socket` Python\n",
    "# package silently strips the file descriptor from the message, setting only the\n",
    "# `MSG_CTRUNC` flag (which might be a bit misleading since the manpage says that\n",
    "# it _indicates that some control data were discarded due to lack of space in\n",
    "# the buffer for ancillary data_). This might reflect the C implementation of\n",
    "# AF_UNIX sockets.\n",
    "#\n",
    "# This behaviour can be reproduced with the script and instructions at the\n",
    "# bottom of this note.\n",
    "#\n",
    "# When that happens, the standard Python `multiprocessing` (and not\n",
    "# `torch.multiprocessing`) raises a `RuntimeError: received 0 items of ancdata`\n",
    "#\n",
    "# Sometimes, instead of the FD being stripped, you may get an `OSError:\n",
    "# Too many open files`, both in the script below and in DataLoader. However,\n",
    "# this is rare and seems to be nondeterministic.\n",
    "#\n",
    "#\n",
    "#   #!/usr/bin/env python3\n",
    "#   import sys\n",
    "#   import socket\n",
    "#   import os\n",
    "#   import array\n",
    "#   import shutil\n",
    "#   import socket\n",
    "#\n",
    "#\n",
    "#   if len(sys.argv) != 4:\n",
    "#       print(\"Usage: \", sys.argv[0], \" tmp_dirname iteration (send|recv)\")\n",
    "#       sys.exit(1)\n",
    "#\n",
    "#   if __name__ == '__main__':\n",
    "#       dirname = sys.argv[1]\n",
    "#       sock_path = dirname + \"/sock\"\n",
    "#       iterations = int(sys.argv[2])\n",
    "#       def dummy_path(i):\n",
    "#           return dirname + \"/\" + str(i) + \".dummy\"\n",
    "#\n",
    "#\n",
    "#       if sys.argv[3] == 'send':\n",
    "#           while not os.path.exists(sock_path):\n",
    "#               pass\n",
    "#           client = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n",
    "#           client.connect(sock_path)\n",
    "#           for i in range(iterations):\n",
    "#               fd = os.open(dummy_path(i), os.O_WRONLY | os.O_CREAT)\n",
    "#               ancdata = array.array('i', [fd])\n",
    "#               msg = bytes([i % 256])\n",
    "#               print(\"Sending fd \", fd, \" (iteration #\", i, \")\")\n",
    "#               client.sendmsg([msg], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, ancdata)])\n",
    "#\n",
    "#\n",
    "#       else:\n",
    "#           assert sys.argv[3] == 'recv'\n",
    "#\n",
    "#           if os.path.exists(dirname):\n",
    "#               raise Exception(\"Directory exists\")\n",
    "#\n",
    "#           os.mkdir(dirname)\n",
    "#\n",
    "#           print(\"Opening socket...\")\n",
    "#           server = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n",
    "#           server.bind(sock_path)\n",
    "#\n",
    "#           print(\"Listening...\")\n",
    "#           for i in range(iterations):\n",
    "#               a = array.array('i')\n",
    "#               msg, ancdata, flags, addr = server.recvmsg(1, socket.CMSG_SPACE(a.itemsize))\n",
    "#               assert(len(ancdata) == 1)\n",
    "#               cmsg_level, cmsg_type, cmsg_data = ancdata[0]\n",
    "#               a.frombytes(cmsg_data)\n",
    "#               print(\"Received fd \", a[0], \" (iteration #\", i, \")\")\n",
    "#\n",
    "#           shutil.rmtree(dirname)\n",
    "#\n",
    "# Steps to reproduce:\n",
    "#\n",
    "# 1. Run two shells and set lower file descriptor limit in the receiving one:\n",
    "# (shell1) ulimit -n 1020\n",
    "# (shell2) ulimit -n 1022\n",
    "#\n",
    "# 2. Run the script above with the `recv` option in the first shell\n",
    "# (shell1) ./test_socket.py sock_tmp 1017 recv\n",
    "#\n",
    "# 3. Run the script with the `send` option in the second shell:\n",
    "# (shell2) ./test_socket.py sock_tmp 1017 send\n",
    "\n",
    "    def _get_data(self):\n",
    "        # Fetches data from `self._data_queue`.\n",
    "        #\n",
    "        # We check workers' status every `MP_STATUS_CHECK_INTERVAL` seconds,\n",
    "        # which we achieve by running `self._try_get_data(timeout=MP_STATUS_CHECK_INTERVAL)`\n",
    "        # in a loop. This is the only mechanism to detect worker failures for\n",
    "        # Windows. For other platforms, a SIGCHLD handler is also used for\n",
    "        # worker failure detection.\n",
    "        #\n",
    "        # If `pin_memory=True`, we also need check if `pin_memory_thread` had\n",
    "        # died at timeouts.\n",
    "        if self._timeout > 0:\n",
    "            success, data = self._try_get_data(self._timeout)\n",
    "            if success:\n",
    "                return data\n",
    "            else:\n",
    "                raise RuntimeError('DataLoader timed out after {} seconds'.format(self._timeout))\n",
    "        elif self._pin_memory:\n",
    "            while self._pin_memory_thread.is_alive():\n",
    "                success, data = self._try_get_data()\n",
    "                if success:\n",
    "                    return data\n",
    "            else:\n",
    "                # while condition is false, i.e., pin_memory_thread died.\n",
    "                raise RuntimeError('Pin memory thread exited unexpectedly')\n",
    "            # In this case, `self._data_queue` is a `queue.Queue`,. But we don't\n",
    "            # need to call `.task_done()` because we don't use `.join()`.\n",
    "        else:\n",
    "            while True:\n",
    "                success, data = self._try_get_data()\n",
    "                if success:\n",
    "                    return data\n",
    "\n",
    "    def _next_data(self):\n",
    "        while True:\n",
    "            # If the worker responsible for `self._rcvd_idx` has already ended\n",
    "            # and was unable to fulfill this task (due to exhausting an `IterableDataset`),\n",
    "            # we try to advance `self._rcvd_idx` to find the next valid index.\n",
    "            #\n",
    "            # This part needs to run in the loop because both the `self._get_data()`\n",
    "            # call and `_IterableDatasetStopIteration` check below can mark\n",
    "            # extra worker(s) as dead.\n",
    "            while self._rcvd_idx < self._send_idx:\n",
    "                info = self._task_info[self._rcvd_idx]\n",
    "                worker_id = info[0]\n",
    "                if len(info) == 2 or self._workers_status[worker_id]:  # has data or is still active\n",
    "                    break\n",
    "                del self._task_info[self._rcvd_idx]\n",
    "                self._rcvd_idx += 1\n",
    "            else:\n",
    "                # no valid `self._rcvd_idx` is found (i.e., didn't break)\n",
    "                if not self._persistent_workers:\n",
    "                    self._shutdown_workers()\n",
    "                raise StopIteration\n",
    "\n",
    "            # Now `self._rcvd_idx` is the batch index we want to fetch\n",
    "\n",
    "            # Check if the next sample has already been generated\n",
    "            if len(self._task_info[self._rcvd_idx]) == 2:\n",
    "                data = self._task_info.pop(self._rcvd_idx)[1]\n",
    "                return self._process_data(data)\n",
    "\n",
    "            assert not self._shutdown and self._tasks_outstanding > 0\n",
    "            idx, data = self._get_data()\n",
    "            self._tasks_outstanding -= 1\n",
    "            if self._dataset_kind == _DatasetKind.Iterable:\n",
    "                # Check for _IterableDatasetStopIteration\n",
    "                if isinstance(data, _utils.worker._IterableDatasetStopIteration):\n",
    "                    if self._persistent_workers:\n",
    "                        self._workers_status[data.worker_id] = False\n",
    "                    else:\n",
    "                        self._mark_worker_as_unavailable(data.worker_id)\n",
    "                    self._try_put_index()\n",
    "                    continue\n",
    "\n",
    "            if idx != self._rcvd_idx:\n",
    "                # store out-of-order samples\n",
    "                self._task_info[idx] += (data,)\n",
    "            else:\n",
    "                del self._task_info[idx]\n",
    "                return self._process_data(data)\n",
    "\n",
    "    def _try_put_index(self):\n",
    "        assert self._tasks_outstanding < self._prefetch_factor * self._num_workers\n",
    "\n",
    "        try:\n",
    "            index = self._next_index()\n",
    "        except StopIteration:\n",
    "            return\n",
    "        for _ in range(self._num_workers):  # find the next active worker, if any\n",
    "            worker_queue_idx = next(self._worker_queue_idx_cycle)\n",
    "            if self._workers_status[worker_queue_idx]:\n",
    "                break\n",
    "        else:\n",
    "            # not found (i.e., didn't break)\n",
    "            return\n",
    "\n",
    "        self._index_queues[worker_queue_idx].put((self._send_idx, index))\n",
    "        self._task_info[self._send_idx] = (worker_queue_idx,)\n",
    "        self._tasks_outstanding += 1\n",
    "        self._send_idx += 1\n",
    "\n",
    "    def _process_data(self, data):\n",
    "        self._rcvd_idx += 1\n",
    "        self._try_put_index()\n",
    "        if isinstance(data, ExceptionWrapper):\n",
    "            data.reraise()\n",
    "        return data\n",
    "\n",
    "    def _mark_worker_as_unavailable(self, worker_id, shutdown=False):\n",
    "        # Mark a worker as having finished its work e.g., due to\n",
    "        # exhausting an `IterableDataset`. This should be used only when this\n",
    "        # `_MultiProcessingDataLoaderIter` is going to continue running.\n",
    "\n",
    "        assert self._workers_status[worker_id] or (self._persistent_workers and shutdown)\n",
    "\n",
    "        # Signal termination to that specific worker.\n",
    "        q = self._index_queues[worker_id]\n",
    "        # Indicate that no more data will be put on this queue by the current\n",
    "        # process.\n",
    "        q.put(None)\n",
    "\n",
    "        # Note that we don't actually join the worker here, nor do we remove the\n",
    "        # worker's pid from C side struct because (1) joining may be slow, and\n",
    "        # (2) since we don't join, the worker may still raise error, and we\n",
    "        # prefer capturing those, rather than ignoring them, even though they\n",
    "        # are raised after the worker has finished its job.\n",
    "        # Joinning is deferred to `_shutdown_workers`, which it is called when\n",
    "        # all workers finish their jobs (e.g., `IterableDataset` replicas) or\n",
    "        # when this iterator is garbage collected.\n",
    "\n",
    "        self._workers_status[worker_id] = False\n",
    "\n",
    "        assert self._workers_done_event.is_set() == shutdown\n",
    "\n",
    "    def _shutdown_workers(self):\n",
    "        # Called when shutting down this `_MultiProcessingDataLoaderIter`.\n",
    "        # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on\n",
    "        # the logic of this function.\n",
    "        python_exit_status = _utils.python_exit_status\n",
    "        if python_exit_status is True or python_exit_status is None:\n",
    "            # See (2) of the note. If Python is shutting down, do no-op.\n",
    "            return\n",
    "        # Normal exit when last reference is gone / iterator is depleted.\n",
    "        # See (1) and the second half of the note.\n",
    "        if not self._shutdown:\n",
    "            self._shutdown = True\n",
    "            try:\n",
    "                # Normal exit when last reference is gone / iterator is depleted.\n",
    "                # See (1) and the second half of the note.\n",
    "\n",
    "                # Exit `pin_memory_thread` first because exiting workers may leave\n",
    "                # corrupted data in `worker_result_queue` which `pin_memory_thread`\n",
    "                # reads from.\n",
    "                if hasattr(self, '_pin_memory_thread'):\n",
    "                    # Use hasattr in case error happens before we set the attribute.\n",
    "                    self._pin_memory_thread_done_event.set()\n",
    "                    # Send something to pin_memory_thread in case it is waiting\n",
    "                    # so that it can wake up and check `pin_memory_thread_done_event`\n",
    "                    self._worker_result_queue.put((None, None))\n",
    "                    self._pin_memory_thread.join()\n",
    "                    self._worker_result_queue.cancel_join_thread()\n",
    "                    self._worker_result_queue.close()\n",
    "\n",
    "                # Exit workers now.\n",
    "                self._workers_done_event.set()\n",
    "                for worker_id in range(len(self._workers)):\n",
    "                    # Get number of workers from `len(self._workers)` instead of\n",
    "                    # `self._num_workers` in case we error before starting all\n",
    "                    # workers.\n",
    "                    # If we are using workers_status with persistent_workers\n",
    "                    # we have to shut it down because the worker is paused\n",
    "                    if self._persistent_workers or self._workers_status[worker_id]:\n",
    "                        self._mark_worker_as_unavailable(worker_id, shutdown=True)\n",
    "                for w in self._workers:\n",
    "                    # We should be able to join here, but in case anything went\n",
    "                    # wrong, we set a timeout and if the workers fail to join,\n",
    "                    # they are killed in the `finally` block.\n",
    "                    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
    "                for q in self._index_queues:\n",
    "                    q.cancel_join_thread()\n",
    "                    q.close()\n",
    "            finally:\n",
    "                # Even though all this function does is putting into queues that\n",
    "                # we have called `cancel_join_thread` on, weird things can\n",
    "                # happen when a worker is killed by a signal, e.g., hanging in\n",
    "                # `Event.set()`. So we need to guard this with SIGCHLD handler,\n",
    "                # and remove pids from the C side data structure only at the\n",
    "                # end.\n",
    "                #\n",
    "                # FIXME: Unfortunately, for Windows, we are missing a worker\n",
    "                #        error detection mechanism here in this function, as it\n",
    "                #        doesn't provide a SIGCHLD handler.\n",
    "                if self._worker_pids_set:\n",
    "                    _utils.signal_handling._remove_worker_pids(id(self))\n",
    "                    self._worker_pids_set = False\n",
    "                for w in self._workers:\n",
    "                    if w.is_alive():\n",
    "                        # Existing mechanisms try to make the workers exit\n",
    "                        # peacefully, but in case that we unfortunately reach\n",
    "                        # here, which we shouldn't, (e.g., pytorch/pytorch#39570),\n",
    "                        # we kill the worker.\n",
    "                        w.terminate()\n",
    "\n",
    "    def __del__(self):\n",
    "        self._shutdown_workers()\n",
    "class _BaseDataLoaderIter(object):\n",
    "    def __init__(self, loader: DataLoader) -> None:\n",
    "        self._dataset = loader.dataset\n",
    "        self._dataset_kind = loader._dataset_kind\n",
    "        self._IterableDataset_len_called = loader._IterableDataset_len_called\n",
    "        self._auto_collation = loader._auto_collation\n",
    "        self._drop_last = loader.drop_last\n",
    "        self._index_sampler = loader._index_sampler\n",
    "        self._num_workers = loader.num_workers\n",
    "        self._prefetch_factor = loader.prefetch_factor\n",
    "        self._pin_memory = loader.pin_memory and torch.cuda.is_available()\n",
    "        self._timeout = loader.timeout\n",
    "        self._collate_fn = loader.collate_fn\n",
    "        self._sampler_iter = iter(self._index_sampler)\n",
    "        self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n",
    "        self._persistent_workers = loader.persistent_workers\n",
    "        self._num_yielded = 0\n",
    "        self._profile_name = \"enumerate(DataLoader)#{}.__next__\".format(self.__class__.__name__)\n",
    "\n",
    "    def __iter__(self) -> '_BaseDataLoaderIter':\n",
    "        return self\n",
    "\n",
    "    def _reset(self, loader, first_iter=False):\n",
    "        self._sampler_iter = iter(self._index_sampler)\n",
    "        self._num_yielded = 0\n",
    "        self._IterableDataset_len_called = loader._IterableDataset_len_called\n",
    "\n",
    "    def _next_index(self):\n",
    "        return next(self._sampler_iter)  # may raise StopIteration\n",
    "\n",
    "    def _next_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __next__(self) -> Any:\n",
    "        with torch.autograd.profiler.record_function(self._profile_name):\n",
    "            if self._sampler_iter is None:\n",
    "                self._reset()\n",
    "            data = self._next_data()\n",
    "            self._num_yielded += 1\n",
    "            if self._dataset_kind == _DatasetKind.Iterable and \\\n",
    "                    self._IterableDataset_len_called is not None and \\\n",
    "                    self._num_yielded > self._IterableDataset_len_called:\n",
    "                warn_msg = (\"Length of IterableDataset {} was reported to be {} (when accessing len(dataloader)), but {} \"\n",
    "                            \"samples have been fetched. \").format(self._dataset, self._IterableDataset_len_called,\n",
    "                                                                  self._num_yielded)\n",
    "                if self._num_workers > 0:\n",
    "                    warn_msg += (\"For multiprocessing data-loading, this could be caused by not properly configuring the \"\n",
    "                                 \"IterableDataset replica at each worker. Please see \"\n",
    "                                 \"https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\")\n",
    "                warnings.warn(warn_msg)\n",
    "            return data\n",
    "\n",
    "    next = __next__  # Python 2 compatibility\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._index_sampler)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # TODO: add limited pickling support for sharing an iterator\n",
    "        # across multiple threads for HOGWILD.\n",
    "        # Probably the best way to do this is by moving the sample pushing\n",
    "        # to a separate thread and then just sharing the data queue\n",
    "        # but signalling the end is tricky without a non-blocking API\n",
    "        raise NotImplementedError(\"{} cannot be pickled\", self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb61d8c-b911-48ca-9b52-589d69bfdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Generic[T_co]):\n",
    "    r\"\"\"\n",
    "    Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
    "    the given dataset.\n",
    "\n",
    "    The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
    "    iterable-style datasets with single- or multi-process loading, customizing\n",
    "    loading order and optional automatic batching (collation) and memory pinning.\n",
    "\n",
    "    See :py:mod:`torch.utils.data` documentation page for more details.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): dataset from which to load the data.\n",
    "        batch_size (int, optional): how many samples per batch to load\n",
    "            (default: ``1``).\n",
    "        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "            at every epoch (default: ``False``).\n",
    "        sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
    "            samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
    "            implemented. If specified, :attr:`shuffle` must not be specified.\n",
    "        batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
    "            returns a batch of indices at a time. Mutually exclusive with\n",
    "            :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
    "            and :attr:`drop_last`.\n",
    "        num_workers (int, optional): how many subprocesses to use for data\n",
    "            loading. ``0`` means that the data will be loaded in the main process.\n",
    "            (default: ``0``)\n",
    "        collate_fn (callable, optional): merges a list of samples to form a\n",
    "            mini-batch of Tensor(s).  Used when using batched loading from a\n",
    "            map-style dataset.\n",
    "        pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
    "            into CUDA pinned memory before returning them.  If your data elements\n",
    "            are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
    "            see the example below.\n",
    "        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
    "            if the dataset size is not divisible by the batch size. If ``False`` and\n",
    "            the size of dataset is not divisible by the batch size, then the last batch\n",
    "            will be smaller. (default: ``False``)\n",
    "        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
    "            from workers. Should always be non-negative. (default: ``0``)\n",
    "        worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
    "            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
    "            input, after seeding and before data loading. (default: ``None``)\n",
    "        generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
    "            by RandomSampler to generate random indexes and multiprocessing to generate\n",
    "            `base_seed` for workers. (default: ``None``)\n",
    "        prefetch_factor (int, optional, keyword-only arg): Number of samples loaded\n",
    "            in advance by each worker. ``2`` means there will be a total of\n",
    "            2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
    "        persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
    "            the worker processes after a dataset has been consumed once. This allows to\n",
    "            maintain the workers `Dataset` instances alive. (default: ``False``)\n",
    "\n",
    "\n",
    "    .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
    "                 cannot be an unpicklable object, e.g., a lambda function. See\n",
    "                 :ref:`multiprocessing-best-practices` on more details related\n",
    "                 to multiprocessing in PyTorch.\n",
    "\n",
    "    .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
    "                 When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
    "                 it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
    "                 rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
    "                 configurations. This represents the best guess PyTorch can make because PyTorch\n",
    "                 trusts user :attr:`dataset` code in correctly handling multi-process\n",
    "                 loading to avoid duplicate data.\n",
    "\n",
    "                 However, if sharding results in multiple workers having incomplete last batches,\n",
    "                 this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
    "                 be broken into multiple ones and (2) more than one batch worth of samples can be\n",
    "                 dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
    "                 cases in general.\n",
    "\n",
    "                 See `Dataset Types`_ for more details on these two types of datasets and how\n",
    "                 :class:`~torch.utils.data.IterableDataset` interacts with\n",
    "                 `Multi-process data loading`_.\n",
    "\n",
    "    .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
    "                 :ref:`data-loading-randomness` notes for random seed related questions.\n",
    "    \"\"\"\n",
    "    dataset: Dataset[T_co]\n",
    "    batch_size: Optional[int]\n",
    "    num_workers: int\n",
    "    pin_memory: bool\n",
    "    drop_last: bool\n",
    "    timeout: float\n",
    "    sampler: Sampler\n",
    "    prefetch_factor: int\n",
    "    _iterator : Optional['_BaseDataLoaderIter']\n",
    "    __initialized = False\n",
    "\n",
    "    def __init__(self, dataset: Dataset[T_co], batch_size: Optional[int] = 1,\n",
    "                 shuffle: bool = False, sampler: Optional[Sampler] = None,\n",
    "                 batch_sampler: Optional[Sampler[Sequence]] = None,\n",
    "                 num_workers: int = 0, collate_fn: Optional[_collate_fn_t] = None,\n",
    "                 pin_memory: bool = False, drop_last: bool = False,\n",
    "                 timeout: float = 0, worker_init_fn: Optional[_worker_init_fn_t] = None,\n",
    "                 multiprocessing_context=None, generator=None,\n",
    "                 *, prefetch_factor: int = 2,\n",
    "                 persistent_workers: bool = False):\n",
    "        torch._C._log_api_usage_once(\"python.data_loader\")\n",
    "\n",
    "        if num_workers < 0:\n",
    "            raise ValueError('num_workers option should be non-negative; '\n",
    "                             'use num_workers=0 to disable multiprocessing.')\n",
    "\n",
    "        if timeout < 0:\n",
    "            raise ValueError('timeout option should be non-negative')\n",
    "\n",
    "        if num_workers == 0 and prefetch_factor != 2:\n",
    "            raise ValueError('prefetch_factor option could only be specified in multiprocessing.'\n",
    "                             'let num_workers > 0 to enable multiprocessing.')\n",
    "        assert prefetch_factor > 0\n",
    "\n",
    "        if persistent_workers and num_workers == 0:\n",
    "            raise ValueError('persistent_workers option needs num_workers > 0')\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.num_workers = num_workers\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.pin_memory = pin_memory\n",
    "        self.timeout = timeout\n",
    "        self.worker_init_fn = worker_init_fn\n",
    "        self.multiprocessing_context = multiprocessing_context\n",
    "\n",
    "        # Arg-check dataset related before checking samplers because we want to\n",
    "        # tell users that iterable-style datasets are incompatible with custom\n",
    "        # samplers first, so that they don't learn that this combo doesn't work\n",
    "        # after spending time fixing the custom sampler errors.\n",
    "        if isinstance(dataset, IterableDataset):\n",
    "            self._dataset_kind = _DatasetKind.Iterable\n",
    "            # NOTE [ Custom Samplers and IterableDataset ]\n",
    "            #\n",
    "            # `IterableDataset` does not support custom `batch_sampler` or\n",
    "            # `sampler` since the key is irrelevant (unless we support\n",
    "            # generator-style dataset one day...).\n",
    "            #\n",
    "            # For `sampler`, we always create a dummy sampler. This is an\n",
    "            # infinite sampler even when the dataset may have an implemented\n",
    "            # finite `__len__` because in multi-process data loading, naive\n",
    "            # settings will return duplicated data (which may be desired), and\n",
    "            # thus using a sampler with length matching that of dataset will\n",
    "            # cause data lost (you may have duplicates of the first couple\n",
    "            # batches, but never see anything afterwards). Therefore,\n",
    "            # `Iterabledataset` always uses an infinite sampler, an instance of\n",
    "            # `_InfiniteConstantSampler` defined above.\n",
    "            #\n",
    "            # A custom `batch_sampler` essentially only controls the batch size.\n",
    "            # However, it is unclear how useful it would be since an iterable-style\n",
    "            # dataset can handle that within itself. Moreover, it is pointless\n",
    "            # in multi-process data loading as the assignment order of batches\n",
    "            # to workers is an implementation detail so users can not control\n",
    "            # how to batchify each worker's iterable. Thus, we disable this\n",
    "            # option. If this turns out to be useful in future, we can re-enable\n",
    "            # this, and support custom samplers that specify the assignments to\n",
    "            # specific workers.\n",
    "            if shuffle is not False:\n",
    "                raise ValueError(\n",
    "                    \"DataLoader with IterableDataset: expected unspecified \"\n",
    "                    \"shuffle option, but got shuffle={}\".format(shuffle))\n",
    "            elif sampler is not None:\n",
    "                # See NOTE [ Custom Samplers and IterableDataset ]\n",
    "                raise ValueError(\n",
    "                    \"DataLoader with IterableDataset: expected unspecified \"\n",
    "                    \"sampler option, but got sampler={}\".format(sampler))\n",
    "            elif batch_sampler is not None:\n",
    "                # See NOTE [ Custom Samplers and IterableDataset ]\n",
    "                raise ValueError(\n",
    "                    \"DataLoader with IterableDataset: expected unspecified \"\n",
    "                    \"batch_sampler option, but got batch_sampler={}\".format(batch_sampler))\n",
    "        else:\n",
    "            self._dataset_kind = _DatasetKind.Map\n",
    "\n",
    "        if sampler is not None and shuffle:\n",
    "            raise ValueError('sampler option is mutually exclusive with '\n",
    "                             'shuffle')\n",
    "\n",
    "        if batch_sampler is not None:\n",
    "            # auto_collation with custom batch_sampler\n",
    "            if batch_size != 1 or shuffle or sampler is not None or drop_last:\n",
    "                raise ValueError('batch_sampler option is mutually exclusive '\n",
    "                                 'with batch_size, shuffle, sampler, and '\n",
    "                                 'drop_last')\n",
    "            batch_size = None\n",
    "            drop_last = False\n",
    "        elif batch_size is None:\n",
    "            # no auto_collation\n",
    "            if drop_last:\n",
    "                raise ValueError('batch_size=None option disables auto-batching '\n",
    "                                 'and is mutually exclusive with drop_last')\n",
    "\n",
    "        if sampler is None:  # give default samplers\n",
    "            if self._dataset_kind == _DatasetKind.Iterable:\n",
    "                # See NOTE [ Custom Samplers and IterableDataset ]\n",
    "                sampler = _InfiniteConstantSampler()\n",
    "            else:  # map-style\n",
    "                if shuffle:\n",
    "                    sampler = RandomSampler(dataset, generator=generator)\n",
    "                else:\n",
    "                    sampler = SequentialSampler(dataset)\n",
    "\n",
    "        if batch_size is not None and batch_sampler is None:\n",
    "            # auto_collation without custom batch_sampler\n",
    "            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.sampler = sampler\n",
    "        self.batch_sampler = batch_sampler\n",
    "        self.generator = generator\n",
    "\n",
    "        if collate_fn is None:\n",
    "            if self._auto_collation:\n",
    "                collate_fn = _utils.collate.default_collate\n",
    "            else:\n",
    "                collate_fn = _utils.collate.default_convert\n",
    "\n",
    "        self.collate_fn = collate_fn\n",
    "        self.persistent_workers = persistent_workers\n",
    "\n",
    "        self.__initialized = True\n",
    "        self._IterableDataset_len_called = None  # See NOTE [ IterableDataset and __len__ ]\n",
    "\n",
    "        self._iterator = None\n",
    "\n",
    "        self.check_worker_number_rationality()\n",
    "\n",
    "        torch.set_vital('Dataloader', 'enabled', 'True')  # type: ignore[attr-defined]\n",
    "\n",
    "    def _get_iterator(self) -> '_BaseDataLoaderIter':\n",
    "        if self.num_workers == 0:\n",
    "            return _SingleProcessDataLoaderIter(self)\n",
    "        else:\n",
    "            self.check_worker_number_rationality()\n",
    "            return _MultiProcessingDataLoaderIter(self)\n",
    "\n",
    "    @property\n",
    "    def multiprocessing_context(self):\n",
    "        return self.__multiprocessing_context\n",
    "\n",
    "    @multiprocessing_context.setter\n",
    "    def multiprocessing_context(self, multiprocessing_context):\n",
    "        if multiprocessing_context is not None:\n",
    "            if self.num_workers > 0:\n",
    "                if isinstance(multiprocessing_context, string_classes):\n",
    "                    valid_start_methods = multiprocessing.get_all_start_methods()\n",
    "                    if multiprocessing_context not in valid_start_methods:\n",
    "                        raise ValueError(\n",
    "                            ('multiprocessing_context option '\n",
    "                             'should specify a valid start method in {!r}, but got '\n",
    "                             'multiprocessing_context={!r}').format(valid_start_methods, multiprocessing_context))\n",
    "                    # error: Argument 1 to \"get_context\" has incompatible type \"Union[str, bytes]\"; expected \"str\"  [arg-type]\n",
    "                    multiprocessing_context = multiprocessing.get_context(multiprocessing_context)  # type: ignore[arg-type]\n",
    "\n",
    "                if not isinstance(multiprocessing_context, python_multiprocessing.context.BaseContext):\n",
    "                    raise TypeError(('multiprocessing_context option should be a valid context '\n",
    "                                     'object or a string specifying the start method, but got '\n",
    "                                     'multiprocessing_context={}').format(multiprocessing_context))\n",
    "            else:\n",
    "                raise ValueError(('multiprocessing_context can only be used with '\n",
    "                                  'multi-process loading (num_workers > 0), but got '\n",
    "                                  'num_workers={}').format(self.num_workers))\n",
    "\n",
    "        self.__multiprocessing_context = multiprocessing_context\n",
    "\n",
    "    def __setattr__(self, attr, val):\n",
    "        if self.__initialized and attr in (\n",
    "                'batch_size', 'batch_sampler', 'sampler', 'drop_last', 'dataset', 'persistent_workers'):\n",
    "            raise ValueError('{} attribute should not be set after {} is '\n",
    "                             'initialized'.format(attr, self.__class__.__name__))\n",
    "\n",
    "        super(DataLoader, self).__setattr__(attr, val)\n",
    "\n",
    "    # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
    "    # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
    "    def __iter__(self) -> '_BaseDataLoaderIter':\n",
    "        # When using a single worker the returned iterator should be\n",
    "        # created everytime to avoid reseting its state\n",
    "        # However, in the case of a multiple workers iterator\n",
    "        # the iterator is only created once in the lifetime of the\n",
    "        # DataLoader object so that workers can be reused\n",
    "        if self.persistent_workers and self.num_workers > 0:\n",
    "            if self._iterator is None:\n",
    "                self._iterator = self._get_iterator()\n",
    "            else:\n",
    "                self._iterator._reset(self)\n",
    "            return self._iterator\n",
    "        else:\n",
    "            return self._get_iterator()\n",
    "\n",
    "    @property\n",
    "    def _auto_collation(self):\n",
    "        return self.batch_sampler is not None\n",
    "\n",
    "    @property\n",
    "    def _index_sampler(self):\n",
    "        # The actual sampler used for generating indices for `_DatasetFetcher`\n",
    "        # (see _utils/fetch.py) to read data at each time. This would be\n",
    "        # `.batch_sampler` if in auto-collation mode, and `.sampler` otherwise.\n",
    "        # We can't change `.sampler` and `.batch_sampler` attributes for BC\n",
    "        # reasons.\n",
    "        if self._auto_collation:\n",
    "            return self.batch_sampler\n",
    "        else:\n",
    "            return self.sampler\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self._dataset_kind == _DatasetKind.Iterable:\n",
    "            # NOTE [ IterableDataset and __len__ ]\n",
    "            #\n",
    "            # For `IterableDataset`, `__len__` could be inaccurate when one naively\n",
    "            # does multi-processing data loading, since the samples will be duplicated.\n",
    "            # However, no real use case should be actually using that behavior, so\n",
    "            # it should count as a user error. We should generally trust user\n",
    "            # code to do the proper thing (e.g., configure each replica differently\n",
    "            # in `__iter__`), and give us the correct `__len__` if they choose to\n",
    "            # implement it (this will still throw if the dataset does not implement\n",
    "            # a `__len__`).\n",
    "            #\n",
    "            # To provide a further warning, we track if `__len__` was called on the\n",
    "            # `DataLoader`, save the returned value in `self._len_called`, and warn\n",
    "            # if the iterator ends up yielding more than this number of samples.\n",
    "\n",
    "            # Cannot statically verify that dataset is Sized\n",
    "            length = self._IterableDataset_len_called = len(self.dataset)  # type: ignore[assignment, arg-type]\n",
    "            if self.batch_size is not None:  # IterableDataset doesn't allow custom sampler or batch_sampler\n",
    "                from math import ceil\n",
    "                if self.drop_last:\n",
    "                    length = length // self.batch_size\n",
    "                else:\n",
    "                    length = ceil(length / self.batch_size)\n",
    "            return length\n",
    "        else:\n",
    "            return len(self._index_sampler)\n",
    "\n",
    "    def check_worker_number_rationality(self):\n",
    "        # This function check whether the dataloader's worker number is rational based on\n",
    "        # current system's resource. Current rule is that if the number of workers this\n",
    "        # Dataloader will create is bigger than the number of logical cpus that is allowed to\n",
    "        # use, than we will pop up a warning to let user pay attention.\n",
    "        #\n",
    "        # eg. If current system has 2 physical CPUs with 16 cores each. And each core support 2\n",
    "        #     threads, then the total logical cpus here is 2 * 16 * 2 = 64. Let's say current\n",
    "        #     DataLoader process can use half of them which is 32, then the rational max number of\n",
    "        #     worker that initiated from this process is 32.\n",
    "        #     Now, let's say the created DataLoader has num_works = 40, which is bigger than 32.\n",
    "        #     So the warning message is triggered to notify the user to lower the worker number if\n",
    "        #     necessary.\n",
    "        #\n",
    "        #\n",
    "        # [Note] Please note that this function repects `cpuset` only when os.sched_getaffinity is\n",
    "        #        available (available in most of Linux system, but not OSX and Windows).\n",
    "        #        When os.sched_getaffinity is not available, os.cpu_count() is called instead, but\n",
    "        #        it doesn't repect cpuset.\n",
    "        #        We don't take threading into account since each worker process is single threaded\n",
    "        #        at this time.\n",
    "        #\n",
    "        #        We don't set any threading flags (eg. OMP_NUM_THREADS, MKL_NUM_THREADS, etc)\n",
    "        #        other than `torch.set_num_threads` to 1 in the worker process, if the passing\n",
    "        #        in functions use 3rd party modules that rely on those threading flags to determine\n",
    "        #        how many thread to create (eg. numpy, etc), then it is caller's responsibility to\n",
    "        #        set those flags correctly.\n",
    "        def _create_warning_msg(num_worker_suggest, num_worker_created, cpuset_checked):\n",
    "\n",
    "            suggested_max_worker_msg = ((\n",
    "                \"Our suggested max number of worker in current system is {}{}, which is smaller \"\n",
    "                \"than what this DataLoader is going to create.\").format(\n",
    "                    num_worker_suggest,\n",
    "                    (\"\" if cpuset_checked else \" (`cpuset` is not taken into account)\"))\n",
    "            ) if num_worker_suggest is not None else (\n",
    "                \"DataLoader is not able to compute a suggested max number of worker in current system.\")\n",
    "\n",
    "            warn_msg = (\n",
    "                \"This DataLoader will create {} worker processes in total. {} \"\n",
    "                \"Please be aware that excessive worker creation might get DataLoader running slow or even freeze, \"\n",
    "                \"lower the worker number to avoid potential slowness/freeze if necessary.\").format(\n",
    "                    num_worker_created,\n",
    "                    suggested_max_worker_msg)\n",
    "            return warn_msg\n",
    "\n",
    "        if not self.num_workers or self.num_workers == 0:\n",
    "            return\n",
    "\n",
    "        # try to compute a suggested max number of worker based on system's resource\n",
    "        max_num_worker_suggest = None\n",
    "        cpuset_checked = False\n",
    "        if hasattr(os, 'sched_getaffinity'):\n",
    "            try:\n",
    "                max_num_worker_suggest = len(os.sched_getaffinity(0))\n",
    "                cpuset_checked = True\n",
    "            except Exception:\n",
    "                pass\n",
    "        if max_num_worker_suggest is None:\n",
    "            # os.cpu_count() could return Optional[int]\n",
    "            # get cpu count first and check None in order to satify mypy check\n",
    "            cpu_count = os.cpu_count()\n",
    "            if cpu_count is not None:\n",
    "                max_num_worker_suggest = cpu_count\n",
    "\n",
    "        if max_num_worker_suggest is None:\n",
    "            warnings.warn(_create_warning_msg(\n",
    "                max_num_worker_suggest,\n",
    "                self.num_workers,\n",
    "                cpuset_checked))\n",
    "            return\n",
    "\n",
    "        if self.num_workers > max_num_worker_suggest:\n",
    "            warnings.warn(_create_warning_msg(\n",
    "                max_num_worker_suggest,\n",
    "                self.num_workers,\n",
    "                cpuset_checked))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e05c6-29ab-485a-b3d9-2d745bba1de2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataLoader 예시!!!!   !!!중요!!!\n",
    "- loader로 불러들이는 요소는\n",
    "1. SMILES식\n",
    "2. batch : MolGraph. 논문에서 말한 Graph란게 이걸 의미한다\n",
    "3. features_batch : 사전에 추출한 feature map\n",
    "4. mask : 뭐지?\n",
    "5. targets : target 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d945558-1362-4085-8774-ec126bd3ffbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tox21' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3a6102cdcb92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtox21\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tox21' is not defined"
     ]
    }
   ],
   "source": [
    "print(tox21[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80a21071-1c53-4119-9b23-8ba056de8dbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-81040b23dce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_data = DataLoader(tox21[4],\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 collate_fn=MolCollator(shared_dict={}, args=args))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = DataLoader(tox21[4],\n",
    "                                batch_size=1,\n",
    "                                shuffle=True,\n",
    "                                num_workers=10,\n",
    "                                collate_fn=MolCollator(shared_dict={}, args=args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acdb66a2-a425-4095-8c40-113bb4a5c31e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6a115dfddf45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8804b7b-3d72-4fe4-9337-08610960f84b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-c83e069aedd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mwhatthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_data):\n",
    "    if i ==0:\n",
    "        whatthis, batch, features_batch, mask, targets = item\n",
    "    else : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "106c8ef3-a7f2-4d43-a7e2-4079ae62b77c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f7eda1102ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b05b7c-f51d-47f1-bff4-22cd2e19c4f0",
   "metadata": {},
   "source": [
    "#### 배치 내부 구조\n",
    "- self.f_atoms, self.f_bonds, self.a2b, self.b2a, self.b2revb, self.a_scope, self.b_scope, self.a2a"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb96b32a-7779-48f5-ba05-c5ac9ed2a750",
   "metadata": {},
   "source": [
    "    A MolGraph computes the following attributes:\n",
    "    - smiles: Smiles string.\n",
    "    - n_atoms: The number of atoms in the molecule.\n",
    "    - n_bonds: The number of bonds in the molecule.\n",
    "    - f_atoms: A mapping from an atom index to a list atom features.\n",
    "    - f_bonds: A mapping from a bond index to a list of bond features.\n",
    "    - a2b: A mapping from an atom index to a list of incoming bond indices.\n",
    "    - b2a: A mapping from a bond index to the index of the atom the bond originates from.\n",
    "    - b2revb: A mapping from a bond index to the index of the reverse bond.\n",
    "\n",
    "    A BatchMolGraph contains the attributes of a MolGraph plus:\n",
    "    - smiles_batch: A list of smiles strings.\n",
    "    - n_mols: The number of molecules in the batch.\n",
    "    - atom_fdim: The dimensionality of the atom features.\n",
    "    - bond_fdim: The dimensionality of the bond features (technically the combined atom/bond features).\n",
    "    - a_scope: A list of tuples indicating the start and end atom indices for each molecule.\n",
    "    - b_scope: A list of tuples indicating the start and end bond indices for each molecule.\n",
    "    - max_num_bonds: The maximum number of bonds neighboring an atom in this batch.\n",
    "    - b2b: (Optional) A mapping from a bond index to incoming bond indices.\n",
    "    - a2a: (Optional): A mapping from an atom index to neighboring atom indices.\n",
    "    \n",
    "        self.n_atoms = 1  # number of atoms (start at 1 b/c need index 0 as padding)\n",
    "        self.n_bonds = 1  # number of bonds (start at 1 b/c need index 0 as padding)\n",
    "        self.a_scope = []  # list of tuples indicating (start_atom_index, num_atoms) for each molecule\n",
    "        self.b_scope = []  # list of tuples indicating (start_bond_index, num_bonds) for each molecule\n",
    "\n",
    "        # All start with zero padding so that indexing with zero padding returns zeros\n",
    "        f_atoms = [[0] * self.atom_fdim]  # atom features\n",
    "        f_bonds = [[0] * self.bond_fdim]  # combined atom/bond features\n",
    "        a2b = [[]]  # mapping from atom index to incoming bond indices\n",
    "        b2a = [0]  # mapping from bond index to the index of the atom the bond is coming from\n",
    "        b2revb = [0]  # mapping from bond index to the index of the reverse bond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "62b3449a-44cf-4ff3-81a9-8dfbb5549223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O=C(c1ccccc1)C(O)c1ccccc1']\n"
     ]
    }
   ],
   "source": [
    "print(whatthis) #origin is _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "67ec1b87-c0e9-4a09-a138-31932f2fc71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[0][2])\n",
    "# 배치의 처음은 f_atoms다. 조금 이상한거?는 맨처음인 0번은 다 무조건 0이란거... 그 이후부터 원본 f_atoms의 0부터 시작이다 그래서 원래는 10개만 있는 f_atoms인데, 여긴 11이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ae9e34eb-95cc-45e6-8e00-597ecb9ae722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[1][1])\n",
    "# 배치의 2번째는 f_bonds이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8f3b32a-5d48-4046-80b2-74110f788167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 2,  0,  0],\n",
       "        [ 1,  4,  6],\n",
       "        [ 3,  8, 10],\n",
       "        [ 7, 12,  0],\n",
       "        [11, 14,  0],\n",
       "        [13, 16,  0],\n",
       "        [15, 18,  0],\n",
       "        [ 9, 17,  0],\n",
       "        [ 5, 20, 22],\n",
       "        [19,  0,  0],\n",
       "        [21, 24, 26],\n",
       "        [23, 28,  0],\n",
       "        [27, 30,  0],\n",
       "        [29, 32,  0],\n",
       "        [31, 34,  0],\n",
       "        [25, 33,  0]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[2]\n",
    "# 0번째 결합이 몇번 몇번 원자끼리 이어져 있는가를 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "78b0a3d8-7300-4c88-b4de-4135a9ff46e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  2,  3,  2,  9,  3,  4,  3,  8,  4,  5,  5,  6,  6,  7,  7,\n",
       "         8,  9, 10,  9, 11, 11, 12, 11, 16, 12, 13, 13, 14, 14, 15, 15, 16])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bd94b7f2-6af2-461b-81b7-c0089b36aeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  1,  4,  3,  6,  5,  8,  7, 10,  9, 12, 11, 14, 13, 16, 15, 18,\n",
       "        17, 20, 19, 22, 21, 24, 23, 26, 25, 28, 27, 30, 29, 32, 31, 34, 33])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "161f6f98-7526-4743-8abf-52992c3cc4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 16]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[5]\n",
    "# 6번째는 n_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f658c0ba-d25e-4370-b707-a6eb67a167d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 34]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[6]\n",
    "# 7번째는 n_bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ac8be1e-f477-40f2-9a2a-08a9c67baa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 35, 17, 35, 35, 1, 1, 17]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(a) for a in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e80bd5b-52a1-4981-88ae-690f243de14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(a) for a in features_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c7235b04-6cfd-4e1e-8904-081b576dc403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(mask)\n",
    "# mask는 None값이면 0 아니면 1을 넣으란 의미구나. 그 segment의 mask랑 같은 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "278f8a06-d8c2-4282-8923-f1f463db1db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a207962d-5f8a-4169-a561-b35ed494f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_batch=[[None,2,3,4,5,6],[0,0,None,0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c29b6659-ab22-4762-8def-a03e08cbb298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([[x is not None for x in tb] for tb in target_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4ce0b-9b1c-4bdb-b8bc-eaf42b05e6d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **_train, run_train실행코드_** (핵심)\n",
    "- Train ensemble of models  -> 여기서 갑자기 args.hidden_size=1200으로 되어버린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "08ba549c-1d73-4e67-bc17-a85800f3e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa536bf4e60>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-90-2a71aeb976a2>\", line 762, in __del__\n",
      "  File \"<ipython-input-90-2a71aeb976a2>\", line 702, in _shutdown_workers\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'\n"
     ]
    }
   ],
   "source": [
    "from grover.data import MolCollator\n",
    "from grover.data import StandardScaler\n",
    "from grover.util.metrics import get_metric_func\n",
    "from grover.util.nn_utils import initialize_weights, param_count\n",
    "from grover.util.scheduler import NoamLR\n",
    "from grover.util.utils import build_optimizer, build_lr_scheduler, makedirs, load_checkpoint, get_loss_func, \\\n",
    "    save_checkpoint, build_model\n",
    "from grover.util.utils import get_class_sizes, get_data, split_data, get_task_names\n",
    "from task.predict import predict, evaluate, evaluate_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "343fea58-cf90-4637-91f2-0e09716c0c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, data, loss_func, optimizer, scheduler,\n",
    "          shared_dict, args: Namespace, n_iter: int = 0,\n",
    "          logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Trains a model for an epoch.\n",
    "\n",
    "    :param model: Model.\n",
    "    :param data: A MoleculeDataset (or a list of MoleculeDatasets if using moe).\n",
    "    :param loss_func: Loss function.\n",
    "    :param optimizer: An Optimizer.\n",
    "    :param scheduler: A learning rate scheduler.\n",
    "    :param args: Arguments.\n",
    "    :param n_iter: The number of iterations (training examples) trained on so far.\n",
    "    :param logger: A logger for printing intermediate results.\n",
    "    :param writer: A tensorboardX SummaryWriter.\n",
    "    :return: The total number of iterations (training examples) trained on so far.\n",
    "    \"\"\"\n",
    "    # debug = logger.debug if logger is not None else print\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # data.shuffle()\n",
    "\n",
    "    loss_sum, iter_count = 0, 0\n",
    "    cum_loss_sum, cum_iter_count = 0, 0\n",
    "\n",
    "\n",
    "    mol_collator = MolCollator(shared_dict=shared_dict, args=args)\n",
    "\n",
    "    num_workers = 4\n",
    "    if type(data) == DataLoader:\n",
    "        mol_loader = data\n",
    "    else:\n",
    "        mol_loader = DataLoader(data, batch_size=args.batch_size, shuffle=True,\n",
    "                            num_workers=num_workers, collate_fn=mol_collator)\n",
    "\n",
    "    for _, item in enumerate(mol_loader):\n",
    "        _, batch, features_batch, mask, targets = item\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            mask, targets = mask.cuda(), targets.cuda()\n",
    "        class_weights = torch.ones(targets.shape)\n",
    "\n",
    "        if args.cuda:\n",
    "            class_weights = class_weights.cuda()\n",
    "\n",
    "        # Run model\n",
    "        model.zero_grad()\n",
    "        preds = model(batch, features_batch)\n",
    "        loss = loss_func(preds, targets) * class_weights * mask\n",
    "        loss = loss.sum() / mask.sum()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        iter_count += args.batch_size\n",
    "\n",
    "        cum_loss_sum += loss.item()\n",
    "        cum_iter_count += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if isinstance(scheduler, NoamLR):\n",
    "            scheduler.step()\n",
    "\n",
    "        n_iter += args.batch_size\n",
    "\n",
    "        #if (n_iter // args.batch_size) % args.log_frequency == 0:\n",
    "        #    lrs = scheduler.get_lr()\n",
    "        #    loss_avg = loss_sum / iter_count\n",
    "        #    loss_sum, iter_count = 0, 0\n",
    "        #    lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))\n",
    "\n",
    "    return n_iter, cum_loss_sum / cum_iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "773a4d27-8a18-4570-968f-64332b4e4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.hidden_size=1200\n",
    "# 아래에서 갑자기 되므로 추가해둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "88d09f9d-b64e-4079-9db7-9595e1da9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(args: Namespace, time_start, logger: Logger = None) -> List[float]:\n",
    "    \"\"\"\n",
    "    Trains a model and returns test scores on the model checkpoint with the highest validation score.\n",
    "\n",
    "    :param args: Arguments.\n",
    "    :param logger: Logger.\n",
    "    :return: A list of ensemble scores for each task.\n",
    "    \"\"\"\n",
    "    if logger is not None:\n",
    "        debug, info = logger.debug, logger.info\n",
    "    else:\n",
    "        debug = info = print\n",
    "\n",
    "\n",
    "    # pin GPU to local rank.\n",
    "    idx = args.gpu\n",
    "    if args.gpu is not None:\n",
    "        torch.cuda.set_device(idx)\n",
    "\n",
    "    features_scaler, scaler, shared_dict, test_data, train_data, val_data = load_data(args, debug, logger)\n",
    "\n",
    "    metric_func = get_metric_func(metric=args.metric)\n",
    "\n",
    "    # Set up test set evaluation\n",
    "    test_smiles, test_targets = test_data.smiles(), test_data.targets()\n",
    "    sum_test_preds = np.zeros((len(test_smiles), args.num_tasks))\n",
    "\n",
    "    # Train ensemble of models  -> 여기서 갑자기 args.hidden_size=1200으로 되어버린다.\n",
    "    for model_idx in range(args.ensemble_size):\n",
    "        # Tensorboard writer\n",
    "        save_dir = os.path.join(args.save_dir, f'model_{model_idx}')\n",
    "        makedirs(save_dir)\n",
    "\n",
    "        # Load/build model\n",
    "        if args.checkpoint_paths is not None:\n",
    "            if len(args.checkpoint_paths) == 1:\n",
    "                cur_model = 0\n",
    "            else:\n",
    "                cur_model = model_idx\n",
    "            debug(f'Loading model {cur_model} from {args.checkpoint_paths[cur_model]}')\n",
    "            model = load_checkpoint(args.checkpoint_paths[cur_model], current_args=args, logger=logger)\n",
    "        else:\n",
    "            debug(f'Building model {model_idx}')\n",
    "            model = build_model(model_idx=model_idx, args=args)\n",
    "\n",
    "        if args.fine_tune_coff != 1 and args.checkpoint_paths is not None:\n",
    "            debug(\"Fine tune fc layer with different lr\")\n",
    "            initialize_weights(model_idx=model_idx, model=model.ffn, distinct_init=args.distinct_init)\n",
    "\n",
    "        # Get loss and metric functions\n",
    "        loss_func = get_loss_func(args, model)\n",
    "\n",
    "        optimizer = build_optimizer(model, args)\n",
    "\n",
    "        debug(model)\n",
    "        debug(f'Number of parameters = {param_count(model):,}')\n",
    "        if args.cuda:\n",
    "            debug('Moving model to cuda')\n",
    "            model = model.cuda()\n",
    "\n",
    "        # Ensure that model is saved in correct location for evaluation if 0 epochs\n",
    "        save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "\n",
    "        # Learning rate schedulers\n",
    "        scheduler = build_lr_scheduler(optimizer, args)\n",
    "\n",
    "        # Bulid data_loader\n",
    "        shuffle = True\n",
    "        mol_collator = MolCollator(shared_dict={}, args=args)\n",
    "        train_data = DataLoader(train_data,\n",
    "                                batch_size=args.batch_size,\n",
    "                                shuffle=shuffle,\n",
    "                                num_workers=10,\n",
    "                                collate_fn=mol_collator)\n",
    "\n",
    "        # Run training\n",
    "        best_score = float('inf') if args.minimize_score else -float('inf')\n",
    "        best_epoch, n_iter = 0, 0\n",
    "        min_val_loss = float('inf')\n",
    "        for epoch in range(args.epochs):\n",
    "            s_time = time.time()\n",
    "            n_iter, train_loss = train(\n",
    "                epoch=epoch,\n",
    "                model=model,\n",
    "                data=train_data,\n",
    "                loss_func=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                args=args,\n",
    "                n_iter=n_iter,\n",
    "                shared_dict=shared_dict,\n",
    "                logger=logger\n",
    "            )\n",
    "            t_time = time.time() - s_time\n",
    "            s_time = time.time()\n",
    "            val_scores, val_loss = evaluate(\n",
    "                model=model,\n",
    "                data=val_data,\n",
    "                loss_func=loss_func,\n",
    "                num_tasks=args.num_tasks,\n",
    "                metric_func=metric_func,\n",
    "                batch_size=args.batch_size,\n",
    "                dataset_type=args.dataset_type,\n",
    "                scaler=scaler,\n",
    "                shared_dict=shared_dict,\n",
    "                logger=logger,\n",
    "                args=args\n",
    "            )\n",
    "            v_time = time.time() - s_time\n",
    "            # Average validation score\n",
    "            avg_val_score = np.nanmean(val_scores)\n",
    "            # Logged after lr step\n",
    "            if isinstance(scheduler, ExponentialLR):\n",
    "                scheduler.step()\n",
    "\n",
    "            if args.show_individual_scores:\n",
    "                # Individual validation scores\n",
    "                for task_name, val_score in zip(args.task_names, val_scores):\n",
    "                    debug(f'Validation {task_name} {args.metric} = {val_score:.6f}')\n",
    "            print('Epoch: {:04d}'.format(epoch),\n",
    "                  'loss_train: {:.6f}'.format(train_loss),\n",
    "                  'loss_val: {:.6f}'.format(val_loss),\n",
    "                  f'{args.metric}_val: {avg_val_score:.4f}',\n",
    "                  # 'auc_val: {:.4f}'.format(avg_val_score),\n",
    "                  'cur_lr: {:.5f}'.format(scheduler.get_lr()[-1]),\n",
    "                  't_time: {:.4f}s'.format(t_time),\n",
    "                  'v_time: {:.4f}s'.format(v_time))\n",
    "\n",
    "            if args.tensorboard:\n",
    "                writer.add_scalar('loss/train', train_loss, epoch)\n",
    "                writer.add_scalar('loss/val', val_loss, epoch)\n",
    "                writer.add_scalar(f'{args.metric}_val', avg_val_score, epoch)\n",
    "\n",
    "\n",
    "            # Save model checkpoint if improved validation score\n",
    "            if args.select_by_loss:\n",
    "                if val_loss < min_val_loss:\n",
    "                    min_val_loss, best_epoch = val_loss, epoch\n",
    "                    save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "            else:\n",
    "                if args.minimize_score and avg_val_score < best_score or \\\n",
    "                        not args.minimize_score and avg_val_score > best_score:\n",
    "                    best_score, best_epoch = avg_val_score, epoch\n",
    "                    save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)\n",
    "\n",
    "            if epoch - best_epoch > args.early_stop_epoch:\n",
    "                break\n",
    "\n",
    "        ensemble_scores = 0.0\n",
    "\n",
    "        # Evaluate on test set using model with best validation score\n",
    "        if args.select_by_loss:\n",
    "            info(f'Model {model_idx} best val loss = {min_val_loss:.6f} on epoch {best_epoch}')\n",
    "        else:\n",
    "            info(f'Model {model_idx} best validation {args.metric} = {best_score:.6f} on epoch {best_epoch}')\n",
    "        model = load_checkpoint(os.path.join(save_dir, 'model.pt'), cuda=args.cuda, logger=logger)\n",
    "\n",
    "        test_preds, _ = predict(\n",
    "            model=model,\n",
    "            data=test_data,\n",
    "            loss_func=loss_func,\n",
    "            batch_size=args.batch_size,\n",
    "            logger=logger,\n",
    "            shared_dict=shared_dict,\n",
    "            scaler=scaler,\n",
    "            args=args\n",
    "        )\n",
    "\n",
    "        test_scores = evaluate_predictions(\n",
    "            preds=test_preds,\n",
    "            targets=test_targets,\n",
    "            num_tasks=args.num_tasks,\n",
    "            metric_func=metric_func,\n",
    "            dataset_type=args.dataset_type,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        if len(test_preds) != 0:\n",
    "            sum_test_preds += np.array(test_preds, dtype=float)\n",
    "\n",
    "        # Average test score\n",
    "        avg_test_score = np.nanmean(test_scores)\n",
    "        info(f'Model {model_idx} test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "        if args.show_individual_scores:\n",
    "            # Individual test scores\n",
    "            for task_name, test_score in zip(args.task_names, test_scores):\n",
    "                info(f'Model {model_idx} test {task_name} {args.metric} = {test_score:.6f}')\n",
    "\n",
    "        # Evaluate ensemble on test set\n",
    "        avg_test_preds = (sum_test_preds / args.ensemble_size).tolist()\n",
    "\n",
    "        ensemble_scores = evaluate_predictions(\n",
    "            preds=avg_test_preds,\n",
    "            targets=test_targets,\n",
    "            num_tasks=args.num_tasks,\n",
    "            metric_func=metric_func,\n",
    "            dataset_type=args.dataset_type,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        ind = [['preds'] * args.num_tasks + ['targets'] * args.num_tasks, args.task_names * 2]\n",
    "        ind = pd.MultiIndex.from_tuples(list(zip(*ind)))\n",
    "        data = np.concatenate([np.array(avg_test_preds), np.array(test_targets)], 1)\n",
    "        test_result = pd.DataFrame(data, index=test_smiles, columns=ind)\n",
    "        test_result.to_csv(os.path.join(args.save_dir, 'test_result.csv'))\n",
    "\n",
    "        # Average ensemble score\n",
    "        avg_ensemble_test_score = np.nanmean(ensemble_scores)\n",
    "        info(f'Ensemble test {args.metric} = {avg_ensemble_test_score:.6f}')\n",
    "\n",
    "        # Individual ensemble scores\n",
    "        if args.show_individual_scores:\n",
    "            for task_name, ensemble_score in zip(args.task_names, ensemble_scores):\n",
    "                info(f'Ensemble test {task_name} {args.metric} = {ensemble_score:.6f}')\n",
    "\n",
    "    return ensemble_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb591b-930e-49a5-bb8f-1d61495c4df0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## cross_validate() 실행코드\n",
    "- 요약하면, 폴드이름별로 경로를 만들고, train한 결과를 fold별로 종합하라. \n",
    "- 각 폴드별 metric에 따라 평균, 표준편차를 출력하고, 폴드들의 합의 평균, 표준편차를 출력하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a35e62b6-ed29-4811-8e7f-06c6cbc984ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(args: Namespace, logger: Logger = None) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    k-fold cross validation.\n",
    "\n",
    "    :return: A tuple of mean_score and std_score.\n",
    "    \"\"\"\n",
    "    info = logger.info if logger is not None else print\n",
    "\n",
    "    # Initialize relevant variables\n",
    "    init_seed = args.seed\n",
    "    save_dir = args.save_dir\n",
    "    task_names = get_task_names(args.data_path)\n",
    "\n",
    "    # Run training with different random seeds for each fold\n",
    "    all_scores = []\n",
    "    time_start = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "    for fold_num in range(args.num_folds):\n",
    "        info(f'Fold {fold_num}')\n",
    "        args.seed = init_seed + fold_num\n",
    "        args.save_dir = os.path.join(save_dir, f'fold_{fold_num}')\n",
    "        makedirs(args.save_dir)\n",
    "        if args.parser_name == \"finetune\":\n",
    "            model_scores = run_training(args, time_start, logger)\n",
    "        else:\n",
    "            model_scores = run_evaluation(args, logger)\n",
    "        all_scores.append(model_scores)\n",
    "    all_scores = np.array(all_scores)\n",
    "\n",
    "    # Report scores for each fold\n",
    "    info(f'{args.num_folds}-fold cross validation')\n",
    "\n",
    "    for fold_num, scores in enumerate(all_scores):\n",
    "        info(f'Seed {init_seed + fold_num} ==> test {args.metric} = {np.nanmean(scores):.6f}')\n",
    "\n",
    "        if args.show_individual_scores:\n",
    "            for task_name, score in zip(task_names, scores):\n",
    "                info(f'Seed {init_seed + fold_num} ==> test {task_name} {args.metric} = {score:.6f}')\n",
    "\n",
    "    # Report scores across models\n",
    "    avg_scores = np.nanmean(all_scores, axis=1)  # average score for each model across tasks\n",
    "    mean_score, std_score = np.nanmean(avg_scores), np.nanstd(avg_scores)\n",
    "    info(f'overall_{args.split_type}_test_{args.metric}={mean_score:.6f}')\n",
    "    info(f'std={std_score:.6f}')\n",
    "\n",
    "    if args.show_individual_scores:\n",
    "        for task_num, task_name in enumerate(task_names):\n",
    "            info(f'Overall test {task_name} {args.metric} = '\n",
    "                 f'{np.nanmean(all_scores[:, task_num]):.6f} +/- {np.nanstd(all_scores[:, task_num]):.6f}')\n",
    "\n",
    "    return mean_score, std_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6755d3-bb21-46ff-840f-476551f31e9a",
   "metadata": {},
   "source": [
    "## 5. finetune모델 구조(중요하니 별도로 빼둔다)\n",
    "- Embedding(batch)\n",
    "- readout : input은 embedding에서 atom from bond, atom from atom 2종류이며, batch의 6번째인 a_scope가 공통으로 들어간다.\n",
    "- 위에서 나온 결과(1200)에 사전에 만든 features_batch(200)를 concat\n",
    "- 디코더(MLP 여러개층)에 이어붙인 결과를 넣어서 output을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bc8381f-08ea-4777-bc92-e8ba9a848529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4aa0c92a-9e7b-48dc-83aa-b45edece5f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(args: Namespace, model_idx=0):\n",
    "    \"\"\"\n",
    "    Builds a MPNN, which is a message passing neural network + feed-forward layers.\n",
    "\n",
    "    :param args: Arguments.\n",
    "    :return: A MPNN containing the MPN encoder along with final linear layers with parameters initialized.\n",
    "    \"\"\"\n",
    "    if hasattr(args, 'num_tasks'):\n",
    "        args.output_size = args.num_tasks\n",
    "    else:\n",
    "        args.output_size = 1\n",
    "\n",
    "    if args.parser_name == \"fingerprint\":\n",
    "        model = GroverFpGeneration(args)\n",
    "    else:\n",
    "        # finetune and evaluation case.\n",
    "        model = GroverFinetuneTask(args)\n",
    "    initialize_weights(model=model, model_idx=model_idx)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d576c9b-49b6-4c26-8029-40cdaa901307",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5-1. 인코더\n",
    "- GROVEREmbedding : Encoder를 부르고, Encoder결과에 따라 Output를 지정하여 return\n",
    "- GTransEncoder\n",
    "  - 처음에 동일한 Feature를 2개의 변수로 나눠서 선포\n",
    "  - 각각의 MTBlock(기본은 1개, 여러개도 가능) 통과하여 각각의 hiddenstate 생성\n",
    "  - 포인트와이즈 피드포워드로 인접 atom, bond들에 대한 message를 passing시킨다.\n",
    "  - passing이란게 자신을 제외한 atom,bond들의 feature들의 합이다.\n",
    "  - 레이어 정규화한 결과와 인코더에 들어가기 전의 feature를 residual connect 시킨다. 각 관점별 결과를 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbd150-a144-46ae-9e50-73a4c86698dc",
   "metadata": {},
   "source": [
    "#### 5-1-1. MPNEncoder\n",
    "- 먼저 input feature(해당 블락의 feature 뿐만 아니라 - a2a, a2b, b2a, b2arevb 등 다양한 정보를 활용한다.)를 여기서 사용할 feature크기로 Linear(Dense)를 통과시키고, 활성화 함수를 적용시킨다.\n",
    "- 자신을 제외한 node, edge들의 메시지를 임의의 hop만큼 종합하여 Linear(Dense) 1개를 통과시켜서 Message를 구한다.\n",
    "- dyMPN은 truncated normal로 3을 기준으로 +-3에서 하는게 기본이나 0~6사이의 uniform분포에서 hop수를 지정할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "212ad96c-97d4-454a-ba5e-0476b6b28d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e29d7f22-30a7-48c6-80a6-44d84cbf024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNEncoder(nn.Module):\n",
    "    \"\"\"A message passing neural network for encoding a molecule.\"\"\"\n",
    "\n",
    "    def __init__(self, args: Namespace,\n",
    "                 atom_messages: bool,\n",
    "                 init_message_dim: int,\n",
    "                 attached_fea_fdim: int,\n",
    "                 hidden_size: int,\n",
    "                 bias: bool,\n",
    "                 depth: int,\n",
    "                 dropout: float,\n",
    "                 undirected: bool,\n",
    "                 dense: bool,\n",
    "                 aggregate_to_atom: bool,\n",
    "                 attach_fea: bool,\n",
    "                 input_layer=\"fc\",\n",
    "                 dynamic_depth='none'\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initializes the MPNEncoder.\n",
    "        :param args: the arguments.\n",
    "        :param atom_messages: enables atom_messages or not.\n",
    "        :param init_message_dim:  the initial input message dimension.\n",
    "        :param attached_fea_fdim:  the attached feature dimension.\n",
    "        :param hidden_size: the output message dimension during message passing.\n",
    "        :param bias: the bias in the message passing.\n",
    "        :param depth: the message passing depth.\n",
    "        :param dropout: the dropout rate.\n",
    "        :param undirected: the message passing is undirected or not.\n",
    "        :param dense: enables the dense connections.\n",
    "        :param attach_fea: enables the feature attachment during the message passing process.\n",
    "        :param dynamic_depth: enables the dynamic depth. Possible choices: \"none\", \"uniform\" and \"truncnorm\"\n",
    "        \"\"\"\n",
    "        super(MPNEncoder, self).__init__()\n",
    "        self.init_message_dim = init_message_dim\n",
    "        self.attached_fea_fdim = attached_fea_fdim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.depth = depth\n",
    "        self.dropout = dropout\n",
    "        self.input_layer = input_layer\n",
    "        self.layers_per_message = 1\n",
    "        self.undirected = undirected\n",
    "        self.atom_messages = atom_messages\n",
    "        self.dense = dense\n",
    "        self.aggreate_to_atom = aggregate_to_atom\n",
    "        self.attached_fea = attach_fea\n",
    "        self.dynamic_depth = dynamic_depth\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Activation\n",
    "        self.act_func = get_activation_function(args.activation)\n",
    "\n",
    "        # Input\n",
    "        if self.input_layer == \"fc\":\n",
    "            input_dim = self.init_message_dim\n",
    "            self.W_i = nn.Linear(input_dim, self.hidden_size, bias=self.bias)\n",
    "\n",
    "        if self.attached_fea:\n",
    "            w_h_input_size = self.hidden_size + self.attached_fea_fdim\n",
    "        else:\n",
    "            w_h_input_size = self.hidden_size\n",
    "\n",
    "        # Shared weight matrix across depths (default)\n",
    "        self.W_h = nn.Linear(w_h_input_size, self.hidden_size, bias=self.bias)\n",
    "\n",
    "    def forward(self,\n",
    "                init_messages,\n",
    "                init_attached_features,\n",
    "                a2nei,\n",
    "                a2attached,\n",
    "                b2a=None,\n",
    "                b2revb=None,\n",
    "                adjs=None\n",
    "                ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        The forward function.\n",
    "        :param init_messages:  initial massages, can be atom features or bond features.\n",
    "        :param init_attached_features: initial attached_features.\n",
    "        :param a2nei: the relation of item to its neighbors. For the atom message passing, a2nei = a2a. For bond\n",
    "        messages a2nei = a2b\n",
    "        :param a2attached: the relation of item to the attached features during message passing. For the atom message\n",
    "        passing, a2attached = a2b. For the bond message passing a2attached = a2a\n",
    "        :param b2a: remove the reversed bond in bond message passing\n",
    "        :param b2revb: remove the revered atom in bond message passing\n",
    "        :return: if aggreate_to_atom or self.atom_messages, return num_atoms x hidden.\n",
    "        Otherwise, return num_bonds x hidden\n",
    "        \"\"\"\n",
    "\n",
    "        # Input\n",
    "        if self.input_layer == 'fc':\n",
    "            input = self.W_i(init_messages)  # num_bonds x hidden_size # f_bond\n",
    "            message = self.act_func(input)  # num_bonds x hidden_size\n",
    "        elif self.input_layer == 'none':\n",
    "            input = init_messages\n",
    "            message = input\n",
    "\n",
    "        attached_fea = init_attached_features  # f_atom / f_bond\n",
    "\n",
    "        # dynamic depth\n",
    "        # uniform sampling from depth - 1 to depth + 1\n",
    "        # only works in training.\n",
    "        if self.training and self.dynamic_depth != \"none\":\n",
    "            if self.dynamic_depth == \"uniform\":\n",
    "                # uniform sampling\n",
    "                ndepth = numpy.random.randint(self.depth - 3, self.depth + 3)\n",
    "            else:\n",
    "                # truncnorm\n",
    "                mu = self.depth\n",
    "                sigma = 1\n",
    "                lower = mu - 3 * sigma\n",
    "                upper = mu + 3 * sigma\n",
    "                X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "                ndepth = int(X.rvs(1))\n",
    "        else:\n",
    "            ndepth = self.depth\n",
    "\n",
    "        # Message passing\n",
    "        for _ in range(ndepth - 1):\n",
    "            if self.undirected:\n",
    "                # two directions should be the same\n",
    "                message = (message + message[b2revb]) / 2\n",
    "\n",
    "            nei_message = select_neighbor_and_aggregate(message, a2nei)\n",
    "            a_message = nei_message\n",
    "            if self.attached_fea:\n",
    "                attached_nei_fea = select_neighbor_and_aggregate(attached_fea, a2attached)\n",
    "                a_message = torch.cat((nei_message, attached_nei_fea), dim=1)\n",
    "\n",
    "            if not self.atom_messages:\n",
    "                rev_message = message[b2revb]\n",
    "                if self.attached_fea:\n",
    "                    atom_rev_message = attached_fea[b2a[b2revb]]\n",
    "                    rev_message = torch.cat((rev_message, atom_rev_message), dim=1)\n",
    "                # Except reverse bond its-self(w) ! \\sum_{k\\in N(u) \\ w}\n",
    "                message = a_message[b2a] - rev_message  # num_bonds x hidden\n",
    "            else:\n",
    "                message = a_message\n",
    "\n",
    "            message = self.W_h(message)\n",
    "\n",
    "            # BUG here, by default MPNEncoder use the dense connection in the message passing step.\n",
    "            # The correct form should if not self.dense\n",
    "            if self.dense:\n",
    "                message = self.act_func(message)  # num_bonds x hidden_size\n",
    "            else:\n",
    "                message = self.act_func(input + message)\n",
    "            message = self.dropout_layer(message)  # num_bonds x hidden\n",
    "\n",
    "        output = message\n",
    "        print(output)\n",
    "\n",
    "        return output  # num_atoms x hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551280d1-ca56-4e45-80be-ddacc1ac7b73",
   "metadata": {},
   "source": [
    "#### 5-1-2. 멀티헤드어텐션\n",
    "- 위의 MPN을 Q,K,V로 3개에 대해 Head수만큼 만든다(4 또는 8)\n",
    "- 그리고 각각의 Head수만큼 Self-Attention을 점곱하여 계산해낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9f43c46-92a8-40cb-9ece-96b653e952aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product SelfAttention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        :param query:\n",
    "        :param key:\n",
    "        :param value:\n",
    "        :param mask:\n",
    "        :param dropout:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dc142ae-cfb1-4884-861e-44ddcea897c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    The multi-head attention module. Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1, bias=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param h:\n",
    "        :param d_model:\n",
    "        :param dropout:\n",
    "        :param bias:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h  # number of heads\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])  # why 3: query, key, value\n",
    "        self.output_linear = nn.Linear(d_model, d_model, bias)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param query:\n",
    "        :param key:\n",
    "        :param value:\n",
    "        :param mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, _ = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3bfe0f04-520d-43db-b51c-28d75acb17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One head for multi-headed attention.\n",
    "    :return: (query, key, value)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, hidden_size, atom_messages=False):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        :param args: The argument.\n",
    "        :param hidden_size: the dimension of hidden layer in Head.\n",
    "        :param atom_messages: the MPNEncoder type.\n",
    "        \"\"\"\n",
    "        super(Head, self).__init__()\n",
    "        atom_fdim = hidden_size\n",
    "        bond_fdim = hidden_size\n",
    "        hidden_size = hidden_size\n",
    "        self.atom_messages = atom_messages\n",
    "        if self.atom_messages:\n",
    "            init_message_dim = atom_fdim\n",
    "            attached_fea_dim = bond_fdim\n",
    "        else:\n",
    "            init_message_dim = bond_fdim\n",
    "            attached_fea_dim = atom_fdim\n",
    "\n",
    "        # Here we use the message passing network as query, key and value.\n",
    "        self.mpn_q = MPNEncoder(args=args,\n",
    "                                atom_messages=atom_messages,\n",
    "                                init_message_dim=init_message_dim,\n",
    "                                attached_fea_fdim=attached_fea_dim,\n",
    "                                hidden_size=hidden_size,\n",
    "                                bias=args.bias,\n",
    "                                depth=args.depth,\n",
    "                                dropout=args.dropout,\n",
    "                                undirected=args.undirected,\n",
    "                                dense=args.dense,\n",
    "                                aggregate_to_atom=False,\n",
    "                                attach_fea=False,\n",
    "                                input_layer=\"none\",\n",
    "                                dynamic_depth=\"truncnorm\")\n",
    "        self.mpn_k = MPNEncoder(args=args,\n",
    "                                atom_messages=atom_messages,\n",
    "                                init_message_dim=init_message_dim,\n",
    "                                attached_fea_fdim=attached_fea_dim,\n",
    "                                hidden_size=hidden_size,\n",
    "                                bias=args.bias,\n",
    "                                depth=args.depth,\n",
    "                                dropout=args.dropout,\n",
    "                                undirected=args.undirected,\n",
    "                                dense=args.dense,\n",
    "                                aggregate_to_atom=False,\n",
    "                                attach_fea=False,\n",
    "                                input_layer=\"none\",\n",
    "                                dynamic_depth=\"truncnorm\")\n",
    "        self.mpn_v = MPNEncoder(args=args,\n",
    "                                atom_messages=atom_messages,\n",
    "                                init_message_dim=init_message_dim,\n",
    "                                attached_fea_fdim=attached_fea_dim,\n",
    "                                hidden_size=hidden_size,\n",
    "                                bias=args.bias,\n",
    "                                depth=args.depth,\n",
    "                                dropout=args.dropout,\n",
    "                                undirected=args.undirected,\n",
    "                                dense=args.dense,\n",
    "                                aggregate_to_atom=False,\n",
    "                                attach_fea=False,\n",
    "                                input_layer=\"none\",\n",
    "                                dynamic_depth=\"truncnorm\")\n",
    "\n",
    "    def forward(self, f_atoms, f_bonds, a2b, a2a, b2a, b2revb):\n",
    "        \"\"\"\n",
    "        The forward function.\n",
    "        :param f_atoms: the atom features, num_atoms * atom_dim\n",
    "        :param f_bonds: the bond features, num_bonds * bond_dim\n",
    "        :param a2b: mapping from atom index to incoming bond indices.\n",
    "        :param a2a: mapping from atom index to its neighbors. num_atoms * max_num_bonds\n",
    "        :param b2a: mapping from bond index to the index of the atom the bond is coming from.\n",
    "        :param b2revb: mapping from bond index to the index of the reverse bond.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.atom_messages:\n",
    "            init_messages = f_atoms\n",
    "            init_attached_features = f_bonds\n",
    "            a2nei = a2a\n",
    "            a2attached = a2b\n",
    "            b2a = b2a\n",
    "            b2revb = b2revb\n",
    "        else:\n",
    "            init_messages = f_bonds\n",
    "            init_attached_features = f_atoms\n",
    "            a2nei = a2b\n",
    "            a2attached = a2a\n",
    "            b2a = b2a\n",
    "            b2revb = b2revb\n",
    "\n",
    "        q = self.mpn_q(init_messages=init_messages,\n",
    "                       init_attached_features=init_attached_features,\n",
    "                       a2nei=a2nei,\n",
    "                       a2attached=a2attached,\n",
    "                       b2a=b2a,\n",
    "                       b2revb=b2revb)\n",
    "        k = self.mpn_k(init_messages=init_messages,\n",
    "                       init_attached_features=init_attached_features,\n",
    "                       a2nei=a2nei,\n",
    "                       a2attached=a2attached,\n",
    "                       b2a=b2a,\n",
    "                       b2revb=b2revb)\n",
    "        v = self.mpn_v(init_messages=init_messages,\n",
    "                       init_attached_features=init_attached_features,\n",
    "                       a2nei=a2nei,\n",
    "                       a2attached=a2attached,\n",
    "                       b2a=b2a,\n",
    "                       b2revb=b2revb)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06796b8-3a02-4f94-9c1d-b3a1ef996223",
   "metadata": {},
   "source": [
    "#### 5-1-3. MTBlock : \n",
    "- 먼저 input을 내부에서 사용할 Hiddensize로 선형 변환을 시킨다. Dropout, 활성화함수, 레이어정규화를 실시한다\n",
    "- 그 다음 head를 계산하도록 MPN을 통과시켜서 QKV를 생성\n",
    "- 이를 Attention에 넣어준다.\n",
    "- 그 결과를 다시 배치화하여 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40cf17be-04d1-40da-8041-6785c7323bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grover.util.nn_utils import get_activation_function, select_neighbor_and_aggregate\n",
    "from torch.nn import LayerNorm, functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85ea5b84-4c16-4960-aa26-7ffce64ebda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The Multi-headed attention block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 num_attn_head,\n",
    "                 input_dim,\n",
    "                 hidden_size,\n",
    "                 activation=\"ReLU\",\n",
    "                 dropout=0.0,\n",
    "                 bias=True,\n",
    "                 atom_messages=False,\n",
    "                 cuda=True,\n",
    "                 res_connection=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args: the arguments.\n",
    "        :param num_attn_head: the number of attention head.\n",
    "        :param input_dim: the input dimension.\n",
    "        :param hidden_size: the hidden size of the model.\n",
    "        :param activation: the activation function.\n",
    "        :param dropout: the dropout ratio\n",
    "        :param bias: if true: all linear layer contains bias term.\n",
    "        :param atom_messages: the MPNEncoder type\n",
    "        :param cuda: if true, the model run with GPU.\n",
    "        :param res_connection: enables the skip-connection in MTBlock.\n",
    "        \"\"\"\n",
    "        super(MTBlock, self).__init__()\n",
    "        # self.args = args\n",
    "        self.atom_messages = atom_messages\n",
    "        self.hidden_size = hidden_size\n",
    "        self.heads = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.cuda = cuda\n",
    "        self.res_connection = res_connection\n",
    "        self.act_func = get_activation_function(activation)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        # Note: elementwise_affine has to be consistent with the pre-training phase\n",
    "        self.layernorm = nn.LayerNorm(self.hidden_size, elementwise_affine=True)\n",
    "\n",
    "        self.W_i = nn.Linear(self.input_dim, self.hidden_size, bias=bias)\n",
    "        self.attn = MultiHeadedAttention(h=num_attn_head,\n",
    "                                         d_model=self.hidden_size,\n",
    "                                         bias=bias,\n",
    "                                         dropout=dropout)\n",
    "        self.W_o = nn.Linear(self.hidden_size * num_attn_head, self.hidden_size, bias=bias)\n",
    "        self.sublayer = SublayerConnection(self.hidden_size, dropout)\n",
    "        for _ in range(num_attn_head):\n",
    "            self.heads.append(Head(args, hidden_size=hidden_size, atom_messages=atom_messages))\n",
    "\n",
    "    def forward(self, batch, features_batch=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param batch: the graph batch generated by GroverCollator.\n",
    "        :param features_batch: the additional features of molecules. (deprecated)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = batch\n",
    "\n",
    "        if self.atom_messages:\n",
    "            # Only add linear transformation in the input feature.\n",
    "            if f_atoms.shape[1] != self.hidden_size:\n",
    "                f_atoms = self.W_i(f_atoms)\n",
    "                f_atoms = self.dropout_layer(self.layernorm(self.act_func(f_atoms)))\n",
    "\n",
    "        else:  # bond messages\n",
    "            if f_bonds.shape[1] != self.hidden_size:\n",
    "                f_bonds = self.W_i(f_bonds)\n",
    "                f_bonds = self.dropout_layer(self.layernorm(self.act_func(f_bonds)))\n",
    "\n",
    "        queries = []\n",
    "        keys = []\n",
    "        values = []\n",
    "        for head in self.heads:\n",
    "            q, k, v = head(f_atoms, f_bonds, a2b, a2a, b2a, b2revb)\n",
    "            queries.append(q.unsqueeze(1))\n",
    "            keys.append(k.unsqueeze(1))\n",
    "            values.append(v.unsqueeze(1))\n",
    "        queries = torch.cat(queries, dim=1)\n",
    "        keys = torch.cat(keys, dim=1)\n",
    "        values = torch.cat(values, dim=1)\n",
    "\n",
    "        x_out = self.attn(queries, keys, values)  # multi-headed attention\n",
    "        x_out = x_out.view(x_out.shape[0], -1)\n",
    "        x_out = self.W_o(x_out)\n",
    "\n",
    "        x_in = None\n",
    "        # support no residual connection in MTBlock.\n",
    "        if self.res_connection:\n",
    "            if self.atom_messages:\n",
    "                x_in = f_atoms\n",
    "            else:\n",
    "                x_in = f_bonds\n",
    "\n",
    "        if self.atom_messages:\n",
    "            f_atoms = self.sublayer(x_in, x_out)\n",
    "        else:\n",
    "            f_bonds = self.sublayer(x_in, x_out)\n",
    "\n",
    "        batch = f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a\n",
    "        features_batch = features_batch\n",
    "        return batch, features_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87f0a400-f322-4196-9699-976055529c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Implements FFN equation.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, activation=\"PReLU\", dropout=0.1, d_out=None):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        :param d_model: the input dimension.\n",
    "        :param d_ff: the hidden dimension.\n",
    "        :param activation: the activation function.\n",
    "        :param dropout: the dropout rate.\n",
    "        :param d_out: the output dimension, the default value is equal to d_model.\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        if d_out is None:\n",
    "            d_out = d_model\n",
    "        # By default, bias is on.\n",
    "        self.W_1 = nn.Linear(d_model, d_ff)\n",
    "        self.W_2 = nn.Linear(d_ff, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act_func = get_activation_function(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function\n",
    "        :param x: input tensor.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.W_2(self.dropout(self.act_func(self.W_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dbc5c16f-1d17-4558-a479-d01098a3fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        :param size: the input dimension.\n",
    "        :param dropout: the dropout ratio.\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size, elementwise_affine=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, outputs):\n",
    "        \"\"\"Apply residual connection to any sublayer with the same size.\"\"\"\n",
    "        # return x + self.dropout(self.norm(x))\n",
    "        if inputs is None:\n",
    "            return self.dropout(self.norm(outputs))\n",
    "        return inputs + self.dropout(self.norm(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a695d10-747a-46a4-bf17-e231b6588fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_select_nd(source: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Selects the message features from source corresponding to the atom or bond indices in index.\n",
    "\n",
    "    :param source: A tensor of shape (num_bonds, hidden_size) containing message features.\n",
    "    :param index: A tensor of shape (num_atoms/num_bonds, max_num_bonds) containing the atom or bond\n",
    "    indices to select from source.\n",
    "    :return: A tensor of shape (num_atoms/num_bonds, max_num_bonds, hidden_size) containing the message\n",
    "    features corresponding to the atoms/bonds specified in index.\n",
    "    \"\"\"\n",
    "    index_size = index.size()  # (num_atoms/num_bonds, max_num_bonds)\n",
    "    suffix_dim = source.size()[1:]  # (hidden_size,)\n",
    "    final_size = index_size + suffix_dim  # (num_atoms/num_bonds, max_num_bonds, hidden_size)\n",
    "\n",
    "    target = source.index_select(dim=0, index=index.view(-1))  # (num_atoms/num_bonds * max_num_bonds, hidden_size)\n",
    "    target = target.view(final_size)  # (num_atoms/num_bonds, max_num_bonds, hidden_size)\n",
    "\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a32b990d-febe-4046-9aa3-51f5da5635e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_neighbor_and_aggregate(feature, index):\n",
    "    \"\"\"\n",
    "    The basic operation in message passing.\n",
    "    Caution: the index_selec_ND would cause the reproducibility issue when performing the training on CUDA.\n",
    "    See: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    :param feature: the candidate feature for aggregate. (n_nodes, hidden)\n",
    "    :param index: the selected index (neighbor indexes).\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    neighbor = index_select_nd(feature, index)\n",
    "    return neighbor.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e958342-7d73-4f7f-90d7-178c6cd7af34",
   "metadata": {},
   "source": [
    "#### 5-1-4. GTransEncoder\n",
    "- batch : dataloader의 결과물, input\n",
    "- node, edge_blocks : AttentionBlock으로 Linear~LayerNorm까지고 이후에 Aggregate로 갈라진다.\n",
    "- atom_bond_transform = Aggregate부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09649238-54ba-4bbe-ade7-3a094b8d0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTransEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 hidden_size,\n",
    "                 edge_fdim,\n",
    "                 node_fdim,\n",
    "                 dropout=0.0,\n",
    "                 activation=\"ReLU\",\n",
    "                 num_mt_block=1,\n",
    "                 num_attn_head=4,\n",
    "                 atom_emb_output: Union[bool, str] = False,  # options: True, False, None, \"atom\", \"bond\", \"both\"\n",
    "                 bias=False,\n",
    "                 cuda=True,\n",
    "                 res_connection=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args: the arguments.\n",
    "        :param hidden_size: the hidden size of the model.\n",
    "        :param edge_fdim: the dimension of additional feature for edge/bond.\n",
    "        :param node_fdim: the dimension of additional feature for node/atom.\n",
    "        :param dropout: the dropout ratio\n",
    "        :param activation: the activation function\n",
    "        :param num_mt_block: the number of mt block.\n",
    "        :param num_attn_head: the number of attention head.\n",
    "        :param atom_emb_output:  enable the output aggregation after message passing.\n",
    "                                              atom_messages:      True                      False\n",
    "        -False: no aggregating to atom. output size:     (num_atoms, hidden_size)    (num_bonds, hidden_size)\n",
    "        -True:  aggregating to atom.    output size:     (num_atoms, hidden_size)    (num_atoms, hidden_size)\n",
    "        -None:                         same as False\n",
    "        -\"atom\":                       same as True\n",
    "        -\"bond\": aggragating to bond.   output size:     (num_bonds, hidden_size)    (num_bonds, hidden_size)\n",
    "        -\"both\": aggregating to atom&bond. output size:  (num_atoms, hidden_size)    (num_bonds, hidden_size)\n",
    "                                                         (num_bonds, hidden_size)    (num_atoms, hidden_size)\n",
    "        :param bias: enable bias term in all linear layers.\n",
    "        :param cuda: run with cuda.\n",
    "        :param res_connection: enables the skip-connection in MTBlock.\n",
    "        \"\"\"\n",
    "        super(GTransEncoder, self).__init__()\n",
    "\n",
    "        # For the compatibility issue.\n",
    "        if atom_emb_output is False:\n",
    "            atom_emb_output = None\n",
    "        if atom_emb_output is True:\n",
    "            atom_emb_output = 'atom'\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.cuda = cuda\n",
    "        self.bias = bias\n",
    "        self.res_connection = res_connection\n",
    "        self.edge_blocks = nn.ModuleList()\n",
    "        self.node_blocks = nn.ModuleList()\n",
    "\n",
    "        edge_input_dim = edge_fdim\n",
    "        node_input_dim = node_fdim\n",
    "        edge_input_dim_i = edge_input_dim\n",
    "        node_input_dim_i = node_input_dim\n",
    "\n",
    "        for i in range(num_mt_block):\n",
    "            if i != 0:\n",
    "                edge_input_dim_i = self.hidden_size\n",
    "                node_input_dim_i = self.hidden_size\n",
    "            self.edge_blocks.append(MTBlock(args=args,\n",
    "                                            num_attn_head=num_attn_head,\n",
    "                                            input_dim=edge_input_dim_i,\n",
    "                                            hidden_size=self.hidden_size,\n",
    "                                            activation=activation,\n",
    "                                            dropout=dropout,\n",
    "                                            bias=self.bias,\n",
    "                                            atom_messages=False,\n",
    "                                            cuda=cuda))\n",
    "            self.node_blocks.append(MTBlock(args=args,\n",
    "                                            num_attn_head=num_attn_head,\n",
    "                                            input_dim=node_input_dim_i,\n",
    "                                            hidden_size=self.hidden_size,\n",
    "                                            activation=activation,\n",
    "                                            dropout=dropout,\n",
    "                                            bias=self.bias,\n",
    "                                            atom_messages=True,\n",
    "                                            cuda=cuda))\n",
    "\n",
    "        self.atom_emb_output = atom_emb_output\n",
    "\n",
    "        self.ffn_atom_from_atom = PositionwiseFeedForward(self.hidden_size + node_fdim,\n",
    "                                                          self.hidden_size * 4,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          d_out=self.hidden_size)\n",
    "\n",
    "        self.ffn_atom_from_bond = PositionwiseFeedForward(self.hidden_size + node_fdim,\n",
    "                                                          self.hidden_size * 4,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          d_out=self.hidden_size)\n",
    "\n",
    "        self.ffn_bond_from_atom = PositionwiseFeedForward(self.hidden_size + edge_fdim,\n",
    "                                                          self.hidden_size * 4,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          d_out=self.hidden_size)\n",
    "\n",
    "        self.ffn_bond_from_bond = PositionwiseFeedForward(self.hidden_size + edge_fdim,\n",
    "                                                          self.hidden_size * 4,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          d_out=self.hidden_size)\n",
    "\n",
    "        self.atom_from_atom_sublayer = SublayerConnection(size=self.hidden_size, dropout=self.dropout)\n",
    "        self.atom_from_bond_sublayer = SublayerConnection(size=self.hidden_size, dropout=self.dropout)\n",
    "        self.bond_from_atom_sublayer = SublayerConnection(size=self.hidden_size, dropout=self.dropout)\n",
    "        self.bond_from_bond_sublayer = SublayerConnection(size=self.hidden_size, dropout=self.dropout)\n",
    "\n",
    "        self.act_func_node = get_activation_function(self.activation)\n",
    "        self.act_func_edge = get_activation_function(self.activation)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=args.dropout)\n",
    "\n",
    "    def pointwise_feed_forward_to_atom_embedding(self, emb_output, atom_fea, index, ffn_layer):\n",
    "        \"\"\"\n",
    "        The point-wise feed forward and long-range residual connection for atom view.\n",
    "        aggregate to atom.\n",
    "        :param emb_output: the output embedding from the previous multi-head attentions.\n",
    "        :param atom_fea: the atom/node feature embedding.\n",
    "        :param index: the index of neighborhood relations.\n",
    "        :param ffn_layer: the feed forward layer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        aggr_output = select_neighbor_and_aggregate(emb_output, index)\n",
    "        aggr_outputx = torch.cat([atom_fea, aggr_output], dim=1)\n",
    "        return ffn_layer(aggr_outputx), aggr_output\n",
    "\n",
    "    def pointwise_feed_forward_to_bond_embedding(self, emb_output, bond_fea, a2nei, b2revb, ffn_layer):\n",
    "        \"\"\"\n",
    "        The point-wise feed forward and long-range residual connection for bond view.\n",
    "        aggregate to bond.\n",
    "        :param emb_output: the output embedding from the previous multi-head attentions.\n",
    "        :param bond_fea: the bond/edge feature embedding.\n",
    "        :param index: the index of neighborhood relations.\n",
    "        :param ffn_layer: the feed forward layer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        aggr_output = select_neighbor_and_aggregate(emb_output, a2nei)\n",
    "        # remove rev bond / atom --- need for bond view\n",
    "        aggr_output = self.remove_rev_bond_message(emb_output, aggr_output, b2revb)\n",
    "        aggr_outputx = torch.cat([bond_fea, aggr_output], dim=1)\n",
    "        return ffn_layer(aggr_outputx), aggr_output\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_rev_bond_message(orginal_message, aggr_message, b2revb):\n",
    "        \"\"\"\n",
    "\n",
    "        :param orginal_message:\n",
    "        :param aggr_message:\n",
    "        :param b2revb:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rev_message = orginal_message[b2revb]\n",
    "        return aggr_message - rev_message\n",
    "\n",
    "    def atom_bond_transform(self,\n",
    "                            to_atom=True,  # False: to bond\n",
    "                            atomwise_input=None,\n",
    "                            bondwise_input=None,\n",
    "                            original_f_atoms=None,\n",
    "                            original_f_bonds=None,\n",
    "                            a2a=None,\n",
    "                            a2b=None,\n",
    "                            b2a=None,\n",
    "                            b2revb=None\n",
    "                            ):\n",
    "        \"\"\"\n",
    "        Transfer the output of atom/bond multi-head attention to the final atom/bond output.\n",
    "        :param to_atom: if true, the output is atom emebedding, otherwise, the output is bond embedding.\n",
    "        :param atomwise_input: the input embedding of atom/node.\n",
    "        :param bondwise_input: the input embedding of bond/edge.\n",
    "        :param original_f_atoms: the initial atom features.\n",
    "        :param original_f_bonds: the initial bond features.\n",
    "        :param a2a: mapping from atom index to its neighbors. num_atoms * max_num_bonds\n",
    "        :param a2b: mapping from atom index to incoming bond indices.\n",
    "        :param b2a: mapping from bond index to the index of the atom the bond is coming from.\n",
    "        :param b2revb: mapping from bond index to the index of the reverse bond.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if to_atom:\n",
    "            # atom input to atom output\n",
    "            atomwise_input, _ = self.pointwise_feed_forward_to_atom_embedding(atomwise_input, original_f_atoms, a2a,\n",
    "                                                                              self.ffn_atom_from_atom)\n",
    "            atom_in_atom_out = self.atom_from_atom_sublayer(None, atomwise_input)\n",
    "            # bond to atom\n",
    "            bondwise_input, _ = self.pointwise_feed_forward_to_atom_embedding(bondwise_input, original_f_atoms, a2b,\n",
    "                                                                              self.ffn_atom_from_bond)\n",
    "            bond_in_atom_out = self.atom_from_bond_sublayer(None, bondwise_input)\n",
    "            return atom_in_atom_out, bond_in_atom_out\n",
    "        else:  # to bond embeddings\n",
    "\n",
    "            # atom input to bond output\n",
    "            atom_list_for_bond = torch.cat([b2a.unsqueeze(dim=1), a2a[b2a]], dim=1)\n",
    "            atomwise_input, _ = self.pointwise_feed_forward_to_bond_embedding(atomwise_input, original_f_bonds,\n",
    "                                                                              atom_list_for_bond,\n",
    "                                                                              b2a[b2revb], self.ffn_bond_from_atom)\n",
    "            atom_in_bond_out = self.bond_from_atom_sublayer(None, atomwise_input)\n",
    "            # bond input to bond output\n",
    "            bond_list_for_bond = a2b[b2a]\n",
    "            bondwise_input, _ = self.pointwise_feed_forward_to_bond_embedding(bondwise_input, original_f_bonds,\n",
    "                                                                              bond_list_for_bond,\n",
    "                                                                              b2revb, self.ffn_bond_from_bond)\n",
    "            bond_in_bond_out = self.bond_from_bond_sublayer(None, bondwise_input)\n",
    "            return atom_in_bond_out, bond_in_bond_out\n",
    "\n",
    "    def forward(self, batch, features_batch = None):\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = batch\n",
    "        if self.cuda or next(self.parameters()).is_cuda:\n",
    "            f_atoms, f_bonds, a2b, b2a, b2revb = f_atoms.cuda(), f_bonds.cuda(), a2b.cuda(), b2a.cuda(), b2revb.cuda()\n",
    "            a2a = a2a.cuda()\n",
    "\n",
    "        node_batch = f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a\n",
    "        edge_batch = f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a\n",
    "\n",
    "        # opt pointwise_feed_forward\n",
    "        original_f_atoms, original_f_bonds = f_atoms, f_bonds\n",
    "\n",
    "        # Note: features_batch is not used here.\n",
    "        for nb in self.node_blocks:  # atom messages. Multi-headed attention\n",
    "            node_batch, features_batch = nb(node_batch, features_batch)\n",
    "        for eb in self.edge_blocks:  # bond messages. Multi-headed attention\n",
    "            edge_batch, features_batch = eb(edge_batch, features_batch)\n",
    "\n",
    "        atom_output, _, _, _, _, _, _, _ = node_batch  # atom hidden states\n",
    "        _, bond_output, _, _, _, _, _, _ = edge_batch  # bond hidden states\n",
    "\n",
    "        if self.atom_emb_output is None:\n",
    "            # output the embedding from multi-head attention directly.\n",
    "            return atom_output, bond_output\n",
    "\n",
    "        if self.atom_emb_output == 'atom':\n",
    "            return self.atom_bond_transform(to_atom=True,  # False: to bond\n",
    "                                            atomwise_input=atom_output,\n",
    "                                            bondwise_input=bond_output,\n",
    "                                            original_f_atoms=original_f_atoms,\n",
    "                                            original_f_bonds=original_f_bonds,\n",
    "                                            a2a=a2a,\n",
    "                                            a2b=a2b,\n",
    "                                            b2a=b2a,\n",
    "                                            b2revb=b2revb)\n",
    "        elif self.atom_emb_output == 'bond':\n",
    "            return self.atom_bond_transform(to_atom=False,  # False: to bond\n",
    "                                            atomwise_input=atom_output,\n",
    "                                            bondwise_input=bond_output,\n",
    "                                            original_f_atoms=original_f_atoms,\n",
    "                                            original_f_bonds=original_f_bonds,\n",
    "                                            a2a=a2a,\n",
    "                                            a2b=a2b,\n",
    "                                            b2a=b2a,\n",
    "                                            b2revb=b2revb)\n",
    "        else:  # 'both'\n",
    "            atom_embeddings = self.atom_bond_transform(to_atom=True,  # False: to bond\n",
    "                                                       atomwise_input=atom_output,\n",
    "                                                       bondwise_input=bond_output,\n",
    "                                                       original_f_atoms=original_f_atoms,\n",
    "                                                       original_f_bonds=original_f_bonds,\n",
    "                                                       a2a=a2a,\n",
    "                                                       a2b=a2b,\n",
    "                                                       b2a=b2a,\n",
    "                                                       b2revb=b2revb)\n",
    "\n",
    "            bond_embeddings = self.atom_bond_transform(to_atom=False,  # False: to bond\n",
    "                                                       atomwise_input=atom_output,\n",
    "                                                       bondwise_input=bond_output,\n",
    "                                                       original_f_atoms=original_f_atoms,\n",
    "                                                       original_f_bonds=original_f_bonds,\n",
    "                                                       a2a=a2a,\n",
    "                                                       a2b=a2b,\n",
    "                                                       b2a=b2a,\n",
    "                                                       b2revb=b2revb)\n",
    "            # Notice: need to be consistent with output format of DualMPNN encoder\n",
    "            return ((atom_embeddings[0], bond_embeddings[0]),\n",
    "                    (atom_embeddings[1], bond_embeddings[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8d2072a-3186-4a15-9dee-c0d982820ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GROVEREmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    The GROVER Embedding class. It contains the GTransEncoder.\n",
    "    This GTransEncoder can be replaced by any validate encoders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args: Namespace):\n",
    "        \"\"\"\n",
    "        Initialize the GROVEREmbedding class.\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(GROVEREmbedding, self).__init__()\n",
    "        self.embedding_output_type = args.embedding_output_type\n",
    "        edge_dim = get_bond_fdim() + get_atom_fdim()  # fdim에 대한건 4-3-4-1. mol2graph()참조\n",
    "        node_dim = get_atom_fdim()\n",
    "        if not hasattr(args, \"backbone\"):\n",
    "            print(\"No backbone specified in args, use gtrans backbone.\")\n",
    "            args.backbone = \"gtrans\"\n",
    "        if args.backbone == \"gtrans\" or args.backbone == \"dualtrans\":\n",
    "            # dualtrans is the old name.\n",
    "            self.encoders = GTransEncoder(args,\n",
    "                                          hidden_size=args.hidden_size,\n",
    "                                          edge_fdim=edge_dim,\n",
    "                                          node_fdim=node_dim,\n",
    "                                          dropout=args.dropout,\n",
    "                                          activation=args.activation,\n",
    "                                          num_mt_block=args.num_mt_block,\n",
    "                                          num_attn_head=args.num_attn_head,\n",
    "                                          atom_emb_output=self.embedding_output_type,\n",
    "                                          bias=args.bias,\n",
    "                                          cuda=args.cuda)\n",
    "\n",
    "    def forward(self, graph_batch: List) -> Dict:\n",
    "        \"\"\"\n",
    "        The forward function takes graph_batch as input and output a dict. The content of the dict is decided by\n",
    "        self.embedding_output_type.\n",
    "\n",
    "        :param graph_batch: the input graph batch generated by MolCollator.\n",
    "        :return: a dict containing the embedding results.\n",
    "        \"\"\"\n",
    "        output = self.encoders(graph_batch)\n",
    "        if self.embedding_output_type == 'atom':\n",
    "            return {\"atom_from_atom\": output[0], \"atom_from_bond\": output[1],\n",
    "                    \"bond_from_atom\": None, \"bond_from_bond\": None}  # atom_from_atom, atom_from_bond\n",
    "        elif self.embedding_output_type == 'bond':\n",
    "            return {\"atom_from_atom\": None, \"atom_from_bond\": None,\n",
    "                    \"bond_from_atom\": output[0], \"bond_from_bond\": output[1]}  # bond_from_atom, bond_from_bond\n",
    "        elif self.embedding_output_type == \"both\":\n",
    "            return {\"atom_from_atom\": output[0][0], \"bond_from_atom\": output[0][1],\n",
    "                    \"atom_from_bond\": output[1][0], \"bond_from_bond\": output[1][1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1ec48-22ff-4fe2-a3e3-5c18496ff1fc",
   "metadata": {},
   "source": [
    "### 5-2. 인코더 구조별 input / output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bfb8e35-cba5-45ac-a4c2-0bf65080af1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'debug' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-f4c47cffa937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfeatures_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmetric_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metric_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'debug' is not defined"
     ]
    }
   ],
   "source": [
    "idx = args.gpu\n",
    "if args.gpu is not None:\n",
    "    torch.cuda.set_device(idx)\n",
    "\n",
    "features_scaler, scaler, shared_dict, test_data, train_data, val_data = load_data(args, debug, logger)\n",
    "\n",
    "metric_func = get_metric_func(metric=args.metric)\n",
    "\n",
    "# Set up test set evaluation\n",
    "test_smiles, test_targets = test_data.smiles(), test_data.targets()\n",
    "sum_test_preds = np.zeros((len(test_smiles), args.num_tasks))\n",
    "\n",
    "# Train ensemble of models  -> 여기서 갑자기 args.hidden_size=1200으로 되어버린다.\n",
    "for model_idx in range(args.ensemble_size):\n",
    "    # Tensorboard writer\n",
    "    save_dir = os.path.join(args.save_dir, f'model_{model_idx}')\n",
    "    makedirs(save_dir)\n",
    "\n",
    "    # Load/build model\n",
    "    print(f'Building model {model_idx}')\n",
    "    model_motif = build_model(model_idx=model_idx, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64e434fb-f823-4dce-9031-ed22186343db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_motif' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-6edb015d7b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_motif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_motif' is not defined"
     ]
    }
   ],
   "source": [
    "model_motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e8625559-310d-425d-bca0-bb5fb1b5a83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e3ab6-6568-464c-812a-86eaf5e02898",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(tox21[4],\n",
    "                                batch_size=1,\n",
    "                                shuffle=True,\n",
    "                                num_workers=10,\n",
    "                                collate_fn=MolCollator(shared_dict={}, args=args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de2772-61bb-48a9-9a0c-534f433214be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(train_data):\n",
    "    if i ==0:\n",
    "        whatthis, batch, features_batch, mask, targets = item\n",
    "    else : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1badec-374c-450c-afee-ecfe94f2cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = batch\n",
    "f_atoms, f_bonds, a2b, b2a, b2revb = f_atoms.cuda(), f_bonds.cuda(), a2b.cuda(), b2a.cuda(), b2revb.cuda()\n",
    "a2a = a2a.cuda()\n",
    "\n",
    "node_batch = f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a\n",
    "edge_batch = f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7e4d1-409d-4317-a530-609efc280331",
   "metadata": {},
   "source": [
    "#### 5-2-1. Linear 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "99bdcc70-5cc7-4826-8fb2-d001b0a60d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1201, 0.0000, 0.0000,\n",
       "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_bonds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "126f6809-0a35-4398-9e10-da0f7af551f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 165])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_bonds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "26ef65a5-7d19-4f3e-b241-1d230b2a761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11342233, -0.03865054,  0.12465174, ...,  0.05574594,\n",
       "       -0.16496275, -0.01924908], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear_output = model.grover.encoders.edge_blocks[0].W_i(f_bonds)\n",
    "Linear_output[1].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ab294b1f-193c-49d1-a0d6-4815bdbddea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 1200])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0d0d9f53-1d12-40b6-86c2-11060b90081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=165, out_features=1200, bias=False)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grover.encoders.edge_blocks[0].W_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27ced9-ff62-452f-b443-16052364e1a5",
   "metadata": {},
   "source": [
    "#### 5-2-2. Q, K, V를 만드는 MPN레이어\n",
    "- 보니깐 얘가 하는게 먼저 depth만큼 돌리게 한다. depth가 3면 0,1,2라 3번 돌리니깐.\n",
    "- 먼저 바로 옆에 있는 edge들의 feature 값들을 더해서 자기꺼로 추가한다.\n",
    "- 그 다음 역방향 결합의 값을 빼주고, Dense, 활성화, dropout을 적용한다.\n",
    "- 이를 반복한다=update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c702ddd9-b231-4ed3-b017-b0d1ab5726af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNEncoder(\n",
       "  (dropout_layer): Dropout(p=0.0, inplace=False)\n",
       "  (act_func): PReLU(num_parameters=1)\n",
       "  (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grover.encoders.edge_blocks[0].heads[0].mpn_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "257983db-f7b6-44f7-a254-bdcaa9bef765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [-0.0000, -0.0000, 0.1247,  ..., 0.0557, -0.0000, -0.0000],\n",
      "        [0.1467, -0.0000, 0.1062,  ..., 0.1826, -0.0000, -0.0000],\n",
      "        ...,\n",
      "        [0.3121, 0.1703, -0.0000,  ..., 0.1537, -0.0000, -0.0000],\n",
      "        [0.1529, 0.1465, -0.0000,  ..., 0.1278, -0.0000, -0.0000],\n",
      "        [0.5716, 0.1561, -0.0000,  ..., 0.1159, -0.0000, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<PreluBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [-0.0000, -0.0000, 0.1247,  ..., 0.0557, -0.0000, -0.0000],\n",
       "        [0.1467, -0.0000, 0.1062,  ..., 0.1826, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [0.3121, 0.1703, -0.0000,  ..., 0.1537, -0.0000, -0.0000],\n",
       "        [0.1529, 0.1465, -0.0000,  ..., 0.1278, -0.0000, -0.0000],\n",
       "        [0.5716, 0.1561, -0.0000,  ..., 0.1159, -0.0000, -0.0000]],\n",
       "       device='cuda:0', grad_fn=<PreluBackward0>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grover.encoders.edge_blocks[0].heads[0].mpn_q(init_messages=Linear_output,\n",
    "                       init_attached_features=f_atoms,\n",
    "                       a2nei=a2b,\n",
    "                       a2attached=a2a,\n",
    "                       b2a=b2a,\n",
    "                       b2revb=b2revb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ed6f8dab-93ae-4d9e-b212-5c2225d93499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grover.encoders.node_blocks[0].heads[0].mpn_q.atom_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "655b1964-087c-4f89-b41f-64fdffa2c820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grover.encoders.node_blocks[0].heads[0].mpn_q.dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "58222bea-6dab-47aa-b852-b6a317f3fb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grover.encoders.node_blocks[0].heads[0].mpn_q.attached_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "92bc9ec5-64d1-4f40-9f94-dd71989261d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False PReLU False False\n"
     ]
    }
   ],
   "source": [
    "print(args.bias, args.activation, args.undirected, args.dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a36184da-e4f8-49b1-a2f8-9ba4ddeae234",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_messages = Linear_output.cpu()\n",
    "init_attached_features = f_atoms.cpu()\n",
    "act_func = get_activation_function('PReLU')\n",
    "W_h = nn.Linear(1200, 1200, bias=False)\n",
    "self_attached_fea = False\n",
    "self_dense = False\n",
    "self_atom_messages=False\n",
    "# dynamic depth이지만 생략\n",
    "ndepth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "89feebd6-f0e5-4013-9320-34a5b490fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MPN_input = init_messages\n",
    "message = MPN_input\n",
    "\n",
    "attached_fea = init_attached_features  # f_atom / f_bond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d08310c6-309d-4b6d-9914-e697cb3d3c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1134, -0.0387,  0.1247,  ...,  0.0557, -0.1650, -0.0192],\n",
       "        [ 0.0058, -0.0469, -0.0065,  ...,  0.0436, -0.1183, -0.0882],\n",
       "        ...,\n",
       "        [-0.2001,  0.0493, -0.1968,  ...,  0.1034, -0.1318,  0.0650],\n",
       "        [-0.2001,  0.0493, -0.1968,  ...,  0.1034, -0.1318,  0.0650],\n",
       "        [-0.1513,  0.1187, -0.2054,  ..., -0.0436, -0.3216, -0.0532]],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MPN_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bfe7bf0a-c65d-4cae-b6a5-6f60acb9c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0284, -0.0097,  0.1247,  ...,  0.0557, -0.0412, -0.0048],\n",
      "        [ 0.0058, -0.0117, -0.0016,  ...,  0.0436, -0.0296, -0.0221],\n",
      "        ...,\n",
      "        [-0.0500,  0.0493, -0.0492,  ...,  0.1034, -0.0329,  0.0650],\n",
      "        [-0.0500,  0.0493, -0.0492,  ...,  0.1034, -0.0329,  0.0650],\n",
      "        [-0.0378,  0.1187, -0.0513,  ..., -0.0109, -0.0804, -0.0133]],\n",
      "       grad_fn=<PreluBackward0>) torch.Size([31, 1200])\n"
     ]
    }
   ],
   "source": [
    "print(message, message.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8e155b03-0f67-4c05-b6af-4bc26831b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0058, -0.0117, -0.0016,  ...,  0.0436, -0.0296, -0.0221],\n",
      "        [-0.0225, -0.0214,  0.1230,  ...,  0.0994, -0.0708, -0.0269],\n",
      "        ...,\n",
      "        [-0.0879,  0.1680, -0.1005,  ...,  0.0925, -0.1133,  0.0517],\n",
      "        [-0.0757,  0.2374, -0.1027,  ..., -0.0218, -0.1608, -0.0266],\n",
      "        [-0.1176,  0.1032, -0.0751,  ...,  0.1425, -0.1057,  0.0592]],\n",
      "       grad_fn=<SumBackward1>) torch.Size([16, 1200])\n"
     ]
    }
   ],
   "source": [
    "nei_message = select_neighbor_and_aggregate(message, a2b.cpu())\n",
    "print(nei_message, nei_message.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "89b31ca8-d875-4f41-9861-c5cbce2d573f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 2,  0,  0],\n",
      "        [ 1,  4,  0],\n",
      "        [ 3,  6,  0],\n",
      "        [ 5,  8,  0],\n",
      "        [ 7, 10,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [11, 14,  0],\n",
      "        [13, 16, 18],\n",
      "        [15,  0,  0],\n",
      "        [17, 20, 22],\n",
      "        [19, 24,  0],\n",
      "        [23, 26,  0],\n",
      "        [25, 28,  0],\n",
      "        [27, 30,  0],\n",
      "        [21, 29,  0]], device='cuda:0') torch.Size([16, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a2b, a2b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "b563baef-d10d-4dcc-9826-8d1512c89dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0058, -0.0117, -0.0016,  ...,  0.0436, -0.0296, -0.0221],\n",
      "        [-0.0284, -0.0097,  0.1247,  ...,  0.0557, -0.0412, -0.0048],\n",
      "        ...,\n",
      "        [-0.0378,  0.1187, -0.0513,  ..., -0.0109, -0.0804, -0.0133],\n",
      "        [-0.0378,  0.1187, -0.0513,  ..., -0.0109, -0.0804, -0.0133],\n",
      "        [-0.0500,  0.0493, -0.0492,  ...,  0.1034, -0.0329,  0.0650]],\n",
      "       grad_fn=<IndexBackward0>) torch.Size([31, 1200])\n"
     ]
    }
   ],
   "source": [
    "print(message[b2revb], message[b2revb].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d5b58ee9-3e2d-4b00-99d2-a4149c18a55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  1,  4,  3,  6,  5,  8,  7, 10,  9, 12, 11, 14, 13, 16, 15, 18,\n",
       "        17, 20, 19, 22, 21, 24, 23, 26, 25, 28, 27, 30, 29], device='cuda:0')"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2revb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "af2f343b-6ce7-4b92-be2f-72f123151d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_message = message[b2revb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "dca53409-efd8-42af-8fce-d92350468227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0058, -0.0117, -0.0016,  ...,  0.0436, -0.0296, -0.0221],\n",
      "        ...,\n",
      "        [-0.0378,  0.1187, -0.0513,  ..., -0.0109, -0.0804, -0.0133],\n",
      "        [-0.0378,  0.1187, -0.0513,  ..., -0.0109, -0.0804, -0.0133],\n",
      "        [-0.0676,  0.0539, -0.0259,  ...,  0.0392, -0.0727, -0.0058]],\n",
      "       grad_fn=<SubBackward0>) torch.Size([31, 1200])\n"
     ]
    }
   ],
   "source": [
    "message2 = nei_message[b2a] - rev_message\n",
    "print(message2, message2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "de2e6fc2-ff0d-4ecd-ad3d-48454dabca02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  8,\n",
       "        10, 10, 11, 10, 15, 11, 12, 12, 13, 13, 14, 14, 15], device='cuda:0')"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3fdd5f63-33f5-4eb8-8152-210fe68e5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0181,  0.0337,  0.0092,  ...,  0.0160,  0.0267,  0.0200],\n",
      "        ...,\n",
      "        [ 0.0786,  0.0103, -0.0343,  ..., -0.0036,  0.0326,  0.0715],\n",
      "        [ 0.0786,  0.0103, -0.0343,  ..., -0.0036,  0.0326,  0.0715],\n",
      "        [ 0.0587, -0.0200, -0.0072,  ..., -0.0364,  0.0894,  0.0231]],\n",
      "       grad_fn=<MmBackward0>) torch.Size([31, 1200])\n"
     ]
    }
   ],
   "source": [
    "message3 = W_h(message2)\n",
    "print(message3, message3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0aaf2d9e-c84e-49e7-929b-7e6dc011f944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0284, -0.0097,  0.1247,  ...,  0.0557, -0.0412, -0.0048],\n",
      "        [ 0.0239, -0.0033,  0.0027,  ...,  0.0597, -0.0229, -0.0171],\n",
      "        ...,\n",
      "        [-0.0304,  0.0596, -0.0578,  ...,  0.0997, -0.0248,  0.1365],\n",
      "        [-0.0304,  0.0596, -0.0578,  ...,  0.0997, -0.0248,  0.1365],\n",
      "        [-0.0232,  0.0988, -0.0531,  ..., -0.0200, -0.0581, -0.0075]],\n",
      "       grad_fn=<PreluBackward0>) torch.Size([31, 1200])\n"
     ]
    }
   ],
   "source": [
    "message4 = act_func(MPN_input + message3)\n",
    "print(message4, message4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "66b82b65-b391-4374-9450-6bce2f36c3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 1200])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(MPN_input + message3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "eaec0536-dd4c-4c9d-9b4e-52361272b6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0284, -0.0097,  0.1247,  ...,  0.0557, -0.0412, -0.0048],\n",
      "        [ 0.0239, -0.0033,  0.0027,  ...,  0.0597, -0.0229, -0.0171],\n",
      "        ...,\n",
      "        [-0.0304,  0.0596, -0.0578,  ...,  0.0997, -0.0248,  0.1365],\n",
      "        [-0.0304,  0.0596, -0.0578,  ...,  0.0997, -0.0248,  0.1365],\n",
      "        [-0.0232,  0.0988, -0.0531,  ..., -0.0200, -0.0581, -0.0075]],\n",
      "       grad_fn=<PreluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Message passing\n",
    "for _ in range(0):\n",
    "    \"\"\"    \n",
    "    # false므로 생략\n",
    "    if self.undirected:\n",
    "    # two directions should be the same\n",
    "    message = (message + message[b2revb]) / 2\n",
    "    \"\"\"\n",
    "\n",
    "    nei_message = select_neighbor_and_aggregate(message, a2b.cpu())\n",
    "    a_message = nei_message\n",
    "    \"\"\"\n",
    "    # false므로 생략\n",
    "    if self_attached_fea:\n",
    "        attached_nei_fea = select_neighbor_and_aggregate(attached_fea, a2attached)\n",
    "        a_message = torch.cat((nei_message, attached_nei_fea), dim=1)\n",
    "    \"\"\"\n",
    "\n",
    "    # false므로 작동\n",
    "    rev_message = message[b2revb]\n",
    "    # Except reverse bond its-self(w) ! \\sum_{k\\in N(u) \\ w}\n",
    "    message = a_message[b2a] - rev_message  # num_bonds x hidden\n",
    "\n",
    "    message = W_h(message)\n",
    "\n",
    "    # BUG here, by default MPNEncoder use the dense connection in the message passing step.\n",
    "    # The correct form should if not self.dense\n",
    "    message = act_func(MPN_input + message)\n",
    "    # 드롭아웃 안하니 생략\n",
    "    #message = dropout_layer(message)  # num_bonds x hidden\n",
    "\n",
    "output = message\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "75858249-f636-400f-98ed-9110a2b3c860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0284, -0.0097,  0.1247,  ...,  0.0557, -0.0412, -0.0048],\n",
      "        [-0.0089, -0.0255,  0.0158,  ..., -0.0002, -0.0157, -0.0155],\n",
      "        ...,\n",
      "        [-0.0589, -0.0030, -0.0429,  ...,  0.1021, -0.0306,  0.0967],\n",
      "        [-0.0583, -0.0022, -0.0481,  ...,  0.1190, -0.0359,  0.0806],\n",
      "        [-0.0636,  0.0560, -0.0573,  ...,  0.0127, -0.0811, -0.0030]],\n",
      "       grad_fn=<PreluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6bee9d33-b590-47a6-b2a1-14f9918ac331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0284, -0.0097,  0.1247,  ...,  0.0557, -0.0412, -0.0048],\n",
      "        [-0.0131, -0.0139, -0.0179,  ...,  0.0757, -0.0289, -0.0414],\n",
      "        ...,\n",
      "        [-0.0643,  0.0110, -0.0502,  ...,  0.1191, -0.0332, -0.0030],\n",
      "        [-0.0677, -0.0048, -0.0544,  ...,  0.1207, -0.0312, -0.0079],\n",
      "        [-0.0559,  0.1006, -0.0615,  ..., -0.0027, -0.0673, -0.0368]],\n",
      "       device='cuda:0', grad_fn=<PreluBackward0>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0284, -0.0097,  0.1247,  ...,  0.0557, -0.0412, -0.0048],\n",
      "        [ 0.0343,  0.0284,  0.0163,  ..., -0.0008, -0.0226, -0.0379],\n",
      "        ...,\n",
      "        [-0.0625,  0.0366, -0.0342,  ...,  0.0137, -0.0173,  0.0084],\n",
      "        [-0.0617,  0.0205, -0.0292,  ...,  0.0261, -0.0224,  0.0198],\n",
      "        [-0.0486,  0.1516, -0.0352,  ..., -0.0289, -0.0773, -0.0385]],\n",
      "       device='cuda:0', grad_fn=<PreluBackward0>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0284, -0.0097,  0.1247,  ...,  0.0557, -0.0412, -0.0048],\n",
      "        [ 0.0732, -0.0201,  0.0236,  ..., -0.0015, -0.0255, -0.0103],\n",
      "        ...,\n",
      "        [-0.0228,  0.0363, -0.0718,  ...,  0.0964, -0.0087,  0.0733],\n",
      "        [-0.0232,  0.0368, -0.0683,  ...,  0.0855, -0.0083,  0.0722],\n",
      "        [-0.0203,  0.0576, -0.0705,  ..., -0.0255, -0.0755, -0.0214]],\n",
      "       device='cuda:0', grad_fn=<PreluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for head in heads:\n",
    "    q, k, v = head(f_atoms, output, a2b, a2a, b2a, b2revb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4dcf252a-af65-4397-bb05-d06acd157d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 1200])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6167aabb-ac0a-4783-ab2f-539590d56ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 1200])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c7b3bbf7-dc14-4788-96bb-f123dc961354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 1200])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833b4a2-45c1-4593-a74c-31289c17248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_messages = f_bonds\n",
    "init_attached_features = f_atoms\n",
    "a2nei = a2b\n",
    "a2attached = a2a\n",
    "b2a = b2a\n",
    "b2revb = b2revb\n",
    "\n",
    "q = model.grover.encoders.edge_blocks[0].heads[0].mpn_q(init_messages=init_messages,\n",
    "               init_attached_features=init_attached_features,\n",
    "               a2nei=a2nei,\n",
    "               a2attached=a2attached,\n",
    "               b2a=b2a,\n",
    "               b2revb=b2revb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f93ef-82d0-42d6-a223-19225a252608",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_messages = f_bonds\n",
    "init_attached_features = f_atoms\n",
    "a2nei = a2b\n",
    "a2attached = a2a\n",
    "b2a = b2a\n",
    "b2revb = b2revb\n",
    "\n",
    "init_messages=model.grover.encoders.edge_blocks[0].W_i(init_messages)\n",
    "q = model.grover.encoders.edge_blocks[0].heads[0].mpn_q(init_messages=init_messages,\n",
    "               init_attached_features=init_attached_features,\n",
    "               a2nei=a2nei,\n",
    "               a2attached=a2attached,\n",
    "               b2a=b2a,\n",
    "               b2revb=b2revb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dbe529-a911-4df8-a89b-85cd45a7c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.grover(batch, features_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54e608-2c2b-4cbf-a171-3a4450b46bbc",
   "metadata": {},
   "source": [
    "## GroverFinetuneModel코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bab3a182-071d-452d-bfd3-92440a274d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Readout(nn.Module):\n",
    "    \"\"\"The readout function. Convert the node embeddings to the graph embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 rtype: str = \"none\",\n",
    "                 hidden_size: int = 0,\n",
    "                 attn_hidden: int = None,\n",
    "                 attn_out: int = None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        The readout function.\n",
    "        :param rtype: readout type, can be \"mean\" and \"self_attention\".\n",
    "        :param hidden_size: input hidden size\n",
    "        :param attn_hidden: only valid if rtype == \"self_attention\". The attention hidden size.\n",
    "        :param attn_out: only valid if rtype == \"self_attention\". The attention out size.\n",
    "        :param args: legacy use.\n",
    "        \"\"\"\n",
    "        super(Readout, self).__init__()\n",
    "        # Cached zeros\n",
    "        self.cached_zero_vector = nn.Parameter(torch.zeros(hidden_size), requires_grad=False)\n",
    "        self.rtype = \"mean\"\n",
    "\n",
    "        if rtype == \"self_attention\":\n",
    "            self.attn = SelfAttention(hidden=attn_hidden,\n",
    "                                      in_feature=hidden_size,\n",
    "                                      out_feature=attn_out)\n",
    "            self.rtype = \"self_attention\"\n",
    "\n",
    "    def forward(self, embeddings, scope):\n",
    "        \"\"\"\n",
    "        The forward function, given a batch node/edge embedding and a scope list,\n",
    "        produce the graph-level embedding by a scope.\n",
    "        :param embeddings: The embedding matrix, num_atoms or num_bonds \\times hidden_size.\n",
    "        :param scope: a list, in which the element is a list [start, range]. `start` is the index\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Readout\n",
    "        mol_vecs = []\n",
    "        self.attns = []\n",
    "        for _, (a_start, a_size) in enumerate(scope):\n",
    "            if a_size == 0:\n",
    "                mol_vecs.append(self.cached_zero_vector)\n",
    "            else:\n",
    "                cur_hiddens = embeddings.narrow(0, a_start, a_size)\n",
    "                if self.rtype == \"self_attention\":\n",
    "                    cur_hiddens, attn = self.attn(cur_hiddens)\n",
    "                    cur_hiddens = cur_hiddens.flatten()\n",
    "                    # Temporarily disable. Enable it if you want to save attentions.\n",
    "                    # self.attns.append(attn.cpu().detach().numpy())\n",
    "                else:\n",
    "                    cur_hiddens = cur_hiddens.sum(dim=0) / a_size\n",
    "                mol_vecs.append(cur_hiddens)\n",
    "\n",
    "        mol_vecs = torch.stack(mol_vecs, dim=0)  # (num_molecules, hidden_size)\n",
    "        return mol_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39d394c6-f700-4f82-8949-a8ce04125546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroverFinetuneTask(nn.Module):\n",
    "    \"\"\"\n",
    "    The finetune\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(GroverFinetuneTask, self).__init__()\n",
    "\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.iscuda = args.cuda\n",
    "\n",
    "        self.grover = GROVEREmbedding(args)\n",
    "\n",
    "        if args.self_attention:\n",
    "            self.readout = Readout(rtype=\"self_attention\", hidden_size=self.hidden_size,\n",
    "                                   attn_hidden=args.attn_hidden,\n",
    "                                   attn_out=args.attn_out)\n",
    "        else:\n",
    "            self.readout = Readout(rtype=\"mean\", hidden_size=self.hidden_size)\n",
    "\n",
    "        self.mol_atom_from_atom_ffn = self.create_ffn(args)\n",
    "        self.mol_atom_from_bond_ffn = self.create_ffn(args)\n",
    "        #self.ffn = nn.ModuleList()\n",
    "        #self.ffn.append(self.mol_atom_from_atom_ffn)\n",
    "        #self.ffn.append(self.mol_atom_from_bond_ffn)\n",
    "\n",
    "        self.classification = args.dataset_type == 'classification'\n",
    "        if self.classification:\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def create_ffn(self, args: Namespace):\n",
    "        \"\"\"\n",
    "        Creates the feed-forward network for the model.\n",
    "\n",
    "        :param args: Arguments.\n",
    "        \"\"\"\n",
    "        # Note: args.features_dim is set according the real loaded features data\n",
    "        if args.features_only:\n",
    "            first_linear_dim = args.features_size + args.features_dim\n",
    "        else:\n",
    "            if args.self_attention:\n",
    "                first_linear_dim = args.hidden_size * args.attn_out\n",
    "                # TODO: Ad-hoc!\n",
    "                # if args.use_input_features:\n",
    "                first_linear_dim += args.features_dim\n",
    "            else:\n",
    "                first_linear_dim = args.hidden_size + args.features_dim\n",
    "\n",
    "        dropout = nn.Dropout(args.dropout)\n",
    "        activation = get_activation_function(args.activation)\n",
    "        # TODO: ffn_hidden_size\n",
    "        # Create FFN layers\n",
    "        if args.ffn_num_layers == 1:\n",
    "            ffn = [\n",
    "                dropout,\n",
    "                nn.Linear(first_linear_dim, args.output_size)\n",
    "            ]\n",
    "        else:\n",
    "            ffn = [\n",
    "                dropout,\n",
    "                nn.Linear(first_linear_dim, args.ffn_hidden_size)\n",
    "            ]\n",
    "            for _ in range(args.ffn_num_layers - 2):\n",
    "                ffn.extend([\n",
    "                    activation,\n",
    "                    dropout,\n",
    "                    nn.Linear(args.ffn_hidden_size, args.ffn_hidden_size),\n",
    "                ])\n",
    "            ffn.extend([\n",
    "                activation,\n",
    "                dropout,\n",
    "                nn.Linear(args.ffn_hidden_size, args.output_size),\n",
    "            ])\n",
    "\n",
    "        # Create FFN model\n",
    "        return nn.Sequential(*ffn)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_func(args):\n",
    "        def loss_func(preds, targets,\n",
    "                      dt=args.dataset_type,\n",
    "                      dist_coff=args.dist_coff):\n",
    "\n",
    "            if dt == 'classification':\n",
    "                pred_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "            elif dt == 'regression':\n",
    "                pred_loss = nn.MSELoss(reduction='none')\n",
    "            else:\n",
    "                raise ValueError(f'Dataset type \"{args.dataset_type}\" not supported.')\n",
    "\n",
    "            # print(type(preds))\n",
    "            # TODO: Here, should we need to involve the model status? Using len(preds) is just a hack.\n",
    "            if type(preds) is not tuple:\n",
    "                # in eval mode.\n",
    "                return pred_loss(preds, targets)\n",
    "\n",
    "            # in train mode.\n",
    "            dist_loss = nn.MSELoss(reduction='none')\n",
    "            # dist_loss = nn.CosineSimilarity(dim=0)\n",
    "            # print(pred_loss)\n",
    "\n",
    "            dist = dist_loss(preds[0], preds[1])\n",
    "            pred_loss1 = pred_loss(preds[0], targets)\n",
    "            pred_loss2 = pred_loss(preds[1], targets)\n",
    "            return pred_loss1 + pred_loss2 + dist_coff * dist\n",
    "\n",
    "        return loss_func\n",
    "\n",
    "    def forward(self, batch, features_batch):\n",
    "        _, _, _, _, _, a_scope, _, _ = batch\n",
    "\n",
    "        output = self.grover(batch)\n",
    "        # Share readout\n",
    "        mol_atom_from_bond_output = self.readout(output[\"atom_from_bond\"], a_scope)\n",
    "        mol_atom_from_atom_output = self.readout(output[\"atom_from_atom\"], a_scope)\n",
    "\n",
    "        if features_batch[0] is not None:\n",
    "            features_batch = torch.from_numpy(np.stack(features_batch)).float()\n",
    "            if self.iscuda:\n",
    "                features_batch = features_batch.cuda()\n",
    "            features_batch = features_batch.to(output[\"atom_from_atom\"])\n",
    "            if len(features_batch.shape) == 1:\n",
    "                features_batch = features_batch.view([1, features_batch.shape[0]])\n",
    "        else:\n",
    "            features_batch = None\n",
    "\n",
    "\n",
    "        if features_batch is not None:\n",
    "            mol_atom_from_atom_output = torch.cat([mol_atom_from_atom_output, features_batch], 1)\n",
    "            mol_atom_from_bond_output = torch.cat([mol_atom_from_bond_output, features_batch], 1)\n",
    "\n",
    "        if self.training:\n",
    "            atom_ffn_output = self.mol_atom_from_atom_ffn(mol_atom_from_atom_output)\n",
    "            bond_ffn_output = self.mol_atom_from_bond_ffn(mol_atom_from_bond_output)\n",
    "            return atom_ffn_output, bond_ffn_output\n",
    "        else:\n",
    "            atom_ffn_output = self.mol_atom_from_atom_ffn(mol_atom_from_atom_output)\n",
    "            bond_ffn_output = self.mol_atom_from_bond_ffn(mol_atom_from_bond_output)\n",
    "            if self.classification:\n",
    "                atom_ffn_output = self.sigmoid(atom_ffn_output)\n",
    "                bond_ffn_output = self.sigmoid(bond_ffn_output)\n",
    "            output = (atom_ffn_output + bond_ffn_output) / 2\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19560501-efbc-496e-9107-61b1cabf0bc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6a115dfddf45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b797988e-495a-4df0-adb8-f332e84926ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args.hidden_size=1200\n",
    "#grover = GroverFinetuneTask(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7282740-210b-4197-9004-cd6205194795",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **_0. main.py()_** 실행코드!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62288f0e-484f-4fb2-8ed7-b05e6190af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Horovod cannot be imported; multi-GPU training is unsupported\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import RDLogger\n",
    "\n",
    "#add for gridsearch\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "# grover 코드들 import리스트\n",
    "from grover.util.parsing import parse_args, get_newest_train_args\n",
    "from grover.util.utils import create_logger\n",
    "from task.cross_validate import cross_validate, randomsearch, gridsearch, make_confusion_matrix\n",
    "from task.fingerprint import generate_fingerprints\n",
    "from task.predict import make_predictions, write_prediction\n",
    "from task.pretrain import pretrain_model\n",
    "from grover.data.torchvocab import MolVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2608117c-8a68-43de-abab-49dcb701c339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup(seed):\n",
    "    # frozen random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951d648-02ac-4afd-a9fc-46c2fbaed35d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 실행코드들\n",
    "- argument들이 없기에 바로 실행은 안된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e97aa326-01c3-42dc-9903-57fdc5eea04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup random seed\n",
    "setup(seed=42)\n",
    "# Avoid the pylint warning.\n",
    "a = MolVocab\n",
    "# supress rdkit logger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "mol_vocab = MolVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "445ad464-16bb-4335-943d-d12491b15f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grover.data.torchvocab.MolVocab"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a와 mol_vocab은 문자에 대한 사전리스트를 사전에 정의하는 것 같다.\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "452f87e5-5924-408d-bae9-9941e7a20fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grover.data.torchvocab.MolVocab"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29316ec4-d072-4612-a528-f6badcadc1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             {finetune,eval,predict,fingerprint,pretrain} ...\n",
      "ipykernel_launcher.py: error: argument parser_name: invalid choice: '/root/.local/share/jupyter/runtime/kernel-89de9d9d-95e7-445a-8457-d1bf7d1bd7ac.json' (choose from 'finetune', 'eval', 'predict', 'fingerprint', 'pretrain')\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 1787, in parse_known_args\n",
      "    namespace, args = self._parse_known_args(args, namespace)\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 1996, in _parse_known_args\n",
      "    stop_index = consume_positionals(start_index)\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 1952, in consume_positionals\n",
      "    take_action(action, args)\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 1845, in take_action\n",
      "    argument_values = self._get_values(action, argument_strings)\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 2386, in _get_values\n",
      "    self._check_value(action, value[0])\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 2433, in _check_value\n",
      "    raise ArgumentError(action, msg % args)\n",
      "argparse.ArgumentError: argument parser_name: invalid choice: '/root/.local/share/jupyter/runtime/kernel-89de9d9d-95e7-445a-8457-d1bf7d1bd7ac.json' (choose from 'finetune', 'eval', 'predict', 'fingerprint', 'pretrain')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-86-8df83216b7d4>\", line 1, in <module>\n",
      "    args = parse_args()\n",
      "  File \"/root/grover/grover/util/parsing.py\", line 511, in parse_args\n",
      "    args = parser.parse_args()\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 1755, in parse_args\n",
      "    args, argv = self.parse_known_args(args, namespace)\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 1794, in parse_known_args\n",
      "    self.error(str(err))\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 2508, in error\n",
      "    self.exit(2, _('%(prog)s: error: %(message)s\\n') % args)\n",
      "  File \"/opt/conda/lib/python3.7/argparse.py\", line 2495, in exit\n",
      "    _sys.exit(status)\n",
      "SystemExit: 2\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m             \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36m_parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   1995\u001b[0m         \u001b[0;31m# consume any positionals following the last Optional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m         \u001b[0mstop_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsume_positionals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mconsume_positionals\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   1951\u001b[0m                 \u001b[0mstart_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0marg_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m                 \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(action, argument_strings, option_string)\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mseen_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0margument_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(self, action, arg_strings)\u001b[0m\n\u001b[1;32m   2385\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_strings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36m_check_value\u001b[0;34m(self, action, value)\u001b[0m\n\u001b[1;32m   2432\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid choice: %(value)r (choose from %(choices)s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument parser_name: invalid choice: '/root/.local/share/jupyter/runtime/kernel-89de9d9d-95e7-445a-8457-d1bf7d1bd7ac.json' (choose from 'finetune', 'eval', 'predict', 'fingerprint', 'pretrain')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-8df83216b7d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/grover/grover/util/parsing.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1755\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2495\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                            'the full traceback.\\n']\n\u001b[1;32m   2091\u001b[0m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0;32m-> 2092\u001b[0;31m                                                                      value))\n\u001b[0m\u001b[1;32m   2093\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n\u001b[1;32m    632\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0mchained_exception_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                 + out_list)\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b7d7d36-e92e-4580-bd19-9d24b59a541b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-2399877168bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'hidden_size'"
     ]
    }
   ],
   "source": [
    "args.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c83b72f1-0cdd-4cff-97d5-45e160754769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Loading data\n",
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "100%|##########| 7831/7831 [00:01<00:00, 5434.30it/s]\n",
      "Total scaffolds = 2,326 | train scaffolds = 1,535 | val scaffolds = 367 | test scaffolds = 424\n",
      "/root/grover/grover/util/utils.py:697: RuntimeWarning: Mean of empty slice\n",
      "  target_avgs.append(np.nanmean(targets, axis=0))\n",
      "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([0.01736614, 0.0152439 , 0.12600321, 0.02879581, 0.11440329,\n",
      "       0.03917221, 0.01794072, 0.11601307, 0.0257732 , 0.04442771,\n",
      "       0.16479724, 0.03924528]), array([1382, 1312, 1246, 1146, 1215, 1353, 1282, 1224, 1358, 1328, 1159,\n",
      "       1325])), (array([0.01056958, 0.01393939, 0.01355514, 0.01047806, 0.06081946,\n",
      "       0.02207637, 0.02308627, 0.09354414, 0.02058824, 0.04649721,\n",
      "       0.03804348, 0.02112251]), array([1703, 1650, 1623, 1527, 1562, 1676, 1646, 1518, 1700, 1613, 1472,\n",
      "       1657])), (array([nan, nan,  0., nan, nan,  0.,  1., nan, nan,  0.,  1.,  1.]), array([0, 0, 4, 0, 0, 3, 1, 0, 0, 1, 4, 3])), (array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., nan,  0., nan,  0.,  0.]), array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1])), (array([0.        , 0.        , 0.16666667, 0.02857143, 0.02857143,\n",
      "       0.03846154, 0.        , 0.09333333, 0.03896104, 0.01298701,\n",
      "       0.08571429, 0.01298701]), array([77, 74, 72, 70, 70, 78, 73, 75, 77, 77, 70, 77])), (array([0. , 0. , 0. , 1. , 1. , 1. , 0. , 0.5, 0. , 0. , 0.5, 0. ]), array([2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2])), (array([nan, nan, nan, nan, nan, nan, nan,  0., nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0])), (array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., nan,  0., nan, nan,  0.]), array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])), (array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., nan,  0., nan,  0.,  0.]), array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1])), (array([0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
      "       0.33333333, 0.        , 0.5       , 0.        , 0.        ,\n",
      "       1.        , 0.        ]), array([2, 2, 2, 2, 2, 3, 1, 2, 3, 3, 1, 2]))]\n",
      "Class sizes\n",
      "NR-AR 0: 95.75%, 1: 4.25%\n",
      "NR-AR-LBD 0: 96.49%, 1: 3.51%\n",
      "NR-AhR 0: 88.27%, 1: 11.73%\n",
      "NR-Aromatase 0: 94.85%, 1: 5.15%\n",
      "NR-ER 0: 87.20%, 1: 12.80%\n",
      "NR-ER-LBD 0: 94.97%, 1: 5.03%\n",
      "NR-PPAR-gamma 0: 97.12%, 1: 2.88%\n",
      "SR-ARE 0: 83.85%, 1: 16.15%\n",
      "SR-ATAD5 0: 96.27%, 1: 3.73%\n",
      "SR-HSE 0: 94.25%, 1: 5.75%\n",
      "SR-MMP 0: 84.20%, 1: 15.80%\n",
      "SR-p53 0: 93.76%, 1: 6.24%\n",
      "Total size = 7,831 | train size = 6,264 | val size = 783 | test size = 784\n",
      "Loading model 0 from model/zinc10M_0_75.pt\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
      "Pretrained parameter \"av_task_atom.linear.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"av_task_atom.linear.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"av_task_bond.linear.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"av_task_bond.linear.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"bv_task_atom.linear.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"bv_task_atom.linear.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"bv_task_atom.linear_rev.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"bv_task_atom.linear_rev.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"bv_task_bond.linear.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"bv_task_bond.linear.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"bv_task_bond.linear_rev.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"bv_task_bond.linear_rev.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.readout.cached_zero_vector\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.linear_atom_from_atom.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.linear_atom_from_atom.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.linear_atom_from_bond.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.linear_atom_from_bond.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.linear_bond_from_atom.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.linear_bond_from_atom.bias\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.linear_bond_from_bond.weight\" cannot be found in model parameters.\n",
      "Pretrained parameter \"fg_task_all.linear_bond_from_bond.bias\" cannot be found in model parameters.\n",
      "GroverFinetuneTask(\n",
      "  (grover): GROVEREmbedding(\n",
      "    (encoders): GTransEncoder(\n",
      "      (edge_blocks): ModuleList(\n",
      "        (0): MTBlock(\n",
      "          (heads): ModuleList(\n",
      "            (0): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (1): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (3): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (act_func): PReLU(num_parameters=1)\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "          (W_i): Linear(in_features=165, out_features=1200, bias=False)\n",
      "          (attn): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "              (1): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "              (2): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_o): Linear(in_features=4800, out_features=1200, bias=False)\n",
      "          (sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (node_blocks): ModuleList(\n",
      "        (0): MTBlock(\n",
      "          (heads): ModuleList(\n",
      "            (0): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (1): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (3): Head(\n",
      "              (mpn_q): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_k): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "              (mpn_v): MPNEncoder(\n",
      "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "                (act_func): PReLU(num_parameters=1)\n",
      "                (W_h): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (act_func): PReLU(num_parameters=1)\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "          (W_i): Linear(in_features=151, out_features=1200, bias=False)\n",
      "          (attn): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "              (1): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "              (2): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=1200, out_features=1200, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_o): Linear(in_features=4800, out_features=1200, bias=False)\n",
      "          (sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ffn_atom_from_atom): PositionwiseFeedForward(\n",
      "        (W_1): Linear(in_features=1351, out_features=4800, bias=True)\n",
      "        (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (ffn_atom_from_bond): PositionwiseFeedForward(\n",
      "        (W_1): Linear(in_features=1351, out_features=4800, bias=True)\n",
      "        (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (ffn_bond_from_atom): PositionwiseFeedForward(\n",
      "        (W_1): Linear(in_features=1365, out_features=4800, bias=True)\n",
      "        (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (ffn_bond_from_bond): PositionwiseFeedForward(\n",
      "        (W_1): Linear(in_features=1365, out_features=4800, bias=True)\n",
      "        (W_2): Linear(in_features=4800, out_features=1200, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act_func): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (atom_from_atom_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (atom_from_bond_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (bond_from_atom_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (bond_from_bond_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm((1200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (act_func_node): PReLU(num_parameters=1)\n",
      "      (act_func_edge): PReLU(num_parameters=1)\n",
      "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (readout): Readout()\n",
      "  (mol_atom_from_atom_ffn): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=1400, out_features=1200, bias=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=1200, out_features=12, bias=True)\n",
      "  )\n",
      "  (mol_atom_from_bond_ffn): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=1400, out_features=1200, bias=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=1200, out_features=12, bias=True)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Number of parameters = 110,534,458\n",
      "Moving model to cuda\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f52f60377a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1481, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1445, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-a3182a7ebab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'finetune'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pretrain'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pretrain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/task/cross_validate.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"finetune\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mmodel_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mmodel_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/task/train.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(args, time_start, logger)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mshared_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             )\n\u001b[1;32m    205\u001b[0m             \u001b[0mt_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/grover/task/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, data, loss_func, optimizer, scheduler, shared_dict, args, n_iter, logger)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoamLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.parser_name == 'finetune':\n",
    "    logger = create_logger(name='train', save_dir=args.save_dir, quiet=False)\n",
    "    cross_validate(args, logger)\n",
    "elif args.parser_name == 'pretrain':\n",
    "    logger = create_logger(name='pretrain', save_dir=args.save_dir)\n",
    "    pretrain_model(args, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1afa21-4657-40a2-aa1d-e62da6e2fe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
